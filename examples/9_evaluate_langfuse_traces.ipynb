{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to evaluate Langfuse traces Using Flow Judge\n",
    "\n",
    "This notebook demonstrates how to use Flow Judge to evaluate Langfuse traces. \n",
    "It covers setting up Langfuse and Flow Judge, evaluating a single trace,\n",
    "and batch evaluating multiple traces.\n",
    "\n",
    "In this tutorial, we will simulate two different types of usage scenarios:\n",
    "\n",
    "1. **Evaluating Traces in a Production Setup**: We will demonstrate how to integrate Flow Judge with a deployed version of the model to evaluate traces in real-time. This setup is ideal for applications where continuous monitoring and evaluation of LLM system responses are required.\n",
    "\n",
    "2. **Batch Evaluations in an Offline Setup**: We will also cover how to perform batch evaluations of multiple traces in an offline environment. This approach is useful for retrospective analysis, development, experimentation, and evaluation of LLM system performance on historical data, allowing for comprehensive assessments without the need for a live system.\n",
    "\n",
    "\n",
    "By the end of this notebook, you will have a clear understanding of how to implement both real-time and batch evaluation strategies using Flow Judge and Langfuse, tailored to your specific production and analysis needs. \n",
    "\n",
    "## Langfuse\n",
    "Langfuse is an open-source LLM engineering platform that helps teams collaboratively debug, analyze, and iterate on their LLM applications. Integrates with LlamaIndex, Langchain, LiteLLM, and more.\n",
    "\n",
    "## Flow Judge\n",
    "Flow-Judge-v0.1 is an open, small yet powerful language model evaluator designed for custom LLM system evaluation. Flow Judge library supports Tranformers, vLLM, Llamafile and Baseten model types. We have both pre-defined evaluation metrics and ease of creating your own custom metric and rubrics. We also support batched evaluation for efficient processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up Langfuse and Flow Judge\n",
    "\n",
    "In this section, we'll set up our Langfuse credentials and initialize \n",
    "the Flow Judge model for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up credentials \n",
    "import os\n",
    "\n",
    "# Get keys for your project from \n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\"\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langfuse\n",
    "import langfuse\n",
    "\n",
    "# Initialize Langfuse\n",
    "langfuse = langfuse.Langfuse(secret_key=os.environ[\"LANGFUSE_SECRET_KEY\"], public_key=os.environ[\"LANGFUSE_PUBLIC_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:flow_judge.models.huggingface:Downloading the model from Hugging Face Hub using hf-transferfor faster downloads...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae29fbbc9d34a3fbc37342cf8b18081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37055427502b4e25a815924b9e44a620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import flow-judge\n",
    "from flow_judge import EvalInput, FlowJudge, Hf #Baseten\n",
    "from flow_judge.metrics import CustomMetric, RubricItem\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Initialize the model\n",
    "model = Hf()\n",
    "#model = Baseten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will use a custom metric to demonstrate the flexibility to tailor evaluations to your needs. \n",
    "\n",
    "1. To create your own metric you can refer to the tutorial on [how to create a custom metrics](examples/2_custom_evaluation_criteria.ipynb)\n",
    "\n",
    "\n",
    "2. Flow-Judge offers also a set of pre-defined metrics you can use. To see the full list of available metrics you can run\n",
    "    ```python\n",
    "    from flow_judge import list_all_metrics \n",
    "    list_all_metrics() \n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scoring individual traces in production set-up\n",
    "\n",
    "We'll demonstrate how to evaluate a single trace using Flow Judge by creating a sample trace with user query, retrieved context and response.\n",
    " \n",
    "This example demonstrates the evaluation of an LLM system designed to assist with customer support. We will create a custom metric to evaluate issue resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Evaluation sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E-commerce Customer Support Example\n",
    "\n",
    "# Query\n",
    "query = \"\"\"I placed an order last week, but I haven't received any shipping updates. Can you help me track my order?\"\"\"\n",
    "\n",
    "# Context\n",
    "context = \"\"\"\n",
    "\n",
    " Customer Details:\n",
    "- Customer ID: CUST-987654321 \n",
    "\n",
    "# Order Details: \n",
    "- Customer ID: \n",
    "- Order ID: 123456789\n",
    "- Order Date: October 1, 2023\n",
    "- Current Status: In Transit\n",
    "\n",
    "# Shipping Policy:\n",
    "- Standard shipping takes 5-7 business days.\n",
    "- Tracking info is emailed once shipped.\n",
    "\"\"\"\n",
    "\n",
    "# Response\n",
    "response = \"\"\"\n",
    "# Tracking Your Order\n",
    "\n",
    "Your order (ID: 123456789) is currently processing. Standard shipping typically takes 5-7 business days from the order date, October 1, 2023. You can expect your order to arrive between October 6 and October 8, 2023.\n",
    "\n",
    "1. **Check Email**: Look for tracking emails in your inbox or spam once the order ships.\n",
    "2. **Track Online**: Use Order ID 123456789 on the courier's site for updates.\n",
    "3. **Contact Support**: If you don't receive updates by October 8, please contact us for assistance.\n",
    "\n",
    "Thank you for your patience.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Create a custom metric and initialize the judge\n",
    "\n",
    "Now we will create a custom metric on a likert-5 scale to evaluate how effectively the response resolves the customer's issue. To learn more about creating custom metrics please refer to our [tutorial](examples/2_custom_evaluation_criteria.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow_judge.metrics import CustomMetric, RubricItem\n",
    "\n",
    "# Define the criteria for issue resolution\n",
    "evaluation_criteria = \"\"\"How well does the response resolve the customer's issue?\"\"\"\n",
    "\n",
    "# Define the rubric using RubricItem's for a Likert 5-point scale\n",
    "rubric = [\n",
    "    RubricItem(\n",
    "        score=1,\n",
    "        description=\"The response does not address the customer's issue at all. The customer is likely to be very dissatisfied.\"),\n",
    "    RubricItem(\n",
    "        score=2,\n",
    "        description=\"The response addresses the issue poorly, with significant gaps or incorrect information. The customer is likely to be dissatisfied.\"),\n",
    "    RubricItem(\n",
    "        score=3,\n",
    "        description=\"The response partially addresses the issue, but lacks clarity or completeness. The customer may be neutral or slightly dissatisfied.\"),\n",
    "    RubricItem(\n",
    "        score=4,\n",
    "        description=\"The response mostly resolves the issue with clear and mostly accurate information. The customer is likely to be satisfied.\"),\n",
    "    RubricItem(\n",
    "        score=5,\n",
    "        description=\"The response fully resolves the issue with clear and accurate information. The customer is likely to be very satisfied.\"),\n",
    "]\n",
    "\n",
    "# Define the required inputs and output for the metric\n",
    "required_inputs = [\"query\", \"context\"]\n",
    "required_output = \"response\"\n",
    "\n",
    "# Create the custom metric\n",
    "issue_resolution_metric = CustomMetric(\n",
    "    name=\"issue-resolution\",\n",
    "    criteria=evaluation_criteria,\n",
    "    rubric=rubric,\n",
    "    required_inputs=required_inputs,\n",
    "    required_output=required_output\n",
    ")\n",
    "\n",
    "# Initialize the Flow Judge \n",
    "issue_resolution_judge = FlowJudge(metric=issue_resolution_metric, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Creating and updating a Langfuse trace with Flow Judge evaluation\n",
    "\n",
    "Here we will simulate how the process of creating a trace, running your LLM system and evaluating the response would work in production.\n",
    "\n",
    "For actual integration of Langfuse with your application, please refer to their the [quickstart guide](https://langfuse.com/docs/get-started).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langfuse.client.StatefulClient at 0x7fa1fecf9b10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up trace when a new customer issue is created \n",
    "trace = langfuse.trace(name = \"flow-judge-example-customer-issue\")  \n",
    "\n",
    "# Now your LLM system will interact with the customer issue and produce a response  \n",
    "# For example retrieve the relevant context via RAG \n",
    "# context = retrieve_context(customer_issue) \n",
    "# Pass the context to trace as span\n",
    "trace.span(\n",
    "    name = \"retrieval\", input={'query': query}, output={'context': context}\n",
    ")  \n",
    "\n",
    "# Use LLM to generate a response to the customer issue \n",
    "# Response = generate_response(query, context)\n",
    "trace.span(\n",
    "    name = \"generation\", input={\"query\":query, \"context\":context}, output={'response': response}\n",
    ")\n",
    "\n",
    "# Update trace with the inputs and output \n",
    "trace.update(input={\"query\":query, \"context\":context}, output={'response': response})\n",
    "\n",
    "# Evaluate the trace with Flow Judge\n",
    "# Create an EvalInput\n",
    "eval_input = EvalInput(\n",
    "    inputs=[\n",
    "        {\"query\": query},\n",
    "        {\"context\": context},\n",
    "    ],\n",
    "    output={\"response\": response},\n",
    ")\n",
    "\n",
    "# Run the evaluation\n",
    "result = issue_resolution_judge.evaluate(eval_input, save_results=False)\n",
    "\n",
    "# Update trace with the score and feedback \n",
    "trace.score(name=issue_resolution_metric.name, value=result.score, comment=result.feedback)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we evaluated a single trace in a production setup using Flow Judge and Langfuse, focusing on an e-commerce customer support scenario. By defining evaluation criteria and using a Likert-5 scale rubric, we provided feedback on how well the response addressed the customer's issue. After the evaluation, we updated the Langfuse trace with the score and feedback, showcasing the integration of Flow Judge with Langfuse. \n",
    "\n",
    "\n",
    "![Successful Trace in Langfuse](langfuse_images/langfuse_trace.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Score:** 3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Feedback:** The response partially addresses the customer's issue but lacks some crucial information and clarity. While it provides some useful information about the expected delivery window and suggests checking the email for tracking updates, it fails to acknowledge the customer's specific concern about not receiving any shipping updates. The response should have directly addressed this issue by explaining why the customer hasn't received updates yet. Additionally, the suggestion to contact support if no updates are received by October 8 is vague and could be more specific about what kind of updates to expect. The response also doesn't mention the possibility of using the courier's site for tracking, which could be helpful for the customer. Overall, while the response provides some useful information, it lacks the necessary clarity and completeness to fully resolve the customer's issue."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display  \n",
    "\n",
    "# View the results\n",
    "display(Markdown(f\"**Score:** {result.score}\"))\n",
    "display(Markdown(f\"**Feedback:** {result.feedback}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Batch evaluating multiple traces in an offline setup\n",
    "\n",
    "In this section, we'll show how to batch evaluate multiple traces.\n",
    "We'll use a sample dataset, load it into Langfuse, retrieve the traces,\n",
    "and then evaluate them using Flow Judge.  \n",
    "\n",
    "For this example we will be using one-of the built-in metrics for evaluating response faithfulness. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load the data into Langfuse\n",
    "\n",
    ">*NOTE: In practice you would fetch your traces directly from Langfuse and skip this first step.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load your JSON data\n",
    "with open('sample_data/csr_assistant.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Prepare the data for Langfuse\n",
    "traces = []\n",
    "for item in data:\n",
    "    trace = langfuse.trace(name = \"flow-judge-csr-assistant\")\n",
    "    trace.update(\n",
    "        input={\n",
    "            \"query\": item['query'],\n",
    "            \"context\": item['context']\n",
    "            },\n",
    "        output={\"response\": item['response']},\n",
    "    )\n",
    "# Ensure all events have been processed by the Langfuse SDK before attempting to retrieve them in the subsequent step\n",
    "langfuse.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Retrieve traces from Langfuse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Retrieve traces from Langfuse \n",
    "traces = langfuse.fetch_traces(\n",
    "    page=1,\n",
    "    limit=len(data),\n",
    "    name=\"flow-judge-csr-assistant\" \n",
    "    ).data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Evaluate the traces\n",
    "\n",
    "Here we'll demonstrate how to batch evaluate multiple traces using Flow Judge. We'll prepare the inputs and outputs, create EvalInput objects,\n",
    "and then run the batch evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:flow_judge.models.huggingface:Automatically determined batch size: 4\n",
      "Processing batches: 100%|██████████| 2/2 [00:53<00:00, 26.58s/it]\n"
     ]
    }
   ],
   "source": [
    "from flow_judge.metrics import RESPONSE_FAITHFULNESS_5POINT\n",
    "\n",
    "# create a list of inputs and outputs\n",
    "inputs_batch = [\n",
    "    [\n",
    "        {\"query\": trace.input[\"query\"]},\n",
    "        {\"context\": trace.input[\"context\"]}\n",
    "    ]\n",
    "    for trace in traces\n",
    "]\n",
    "outputs_batch = [{\"response\":trace.output[\"response\"]} for trace in traces]\n",
    "\n",
    "# create a list of EvalInputs\n",
    "eval_inputs_batch = [EvalInput(inputs=inputs, output=output) for inputs, output in zip(inputs_batch, outputs_batch)]\n",
    "\n",
    "# Initialize the judge  \n",
    "faithfulness_judge = FlowJudge(metric=RESPONSE_FAITHFULNESS_5POINT, model=model)\n",
    "\n",
    "# Run the batch evaluation\n",
    "results = faithfulness_judge.batch_evaluate(eval_inputs_batch, save_results=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Update evaluation results to Langfuse\n",
    "\n",
    "Finally, we'll update the Langfuse traces with the evaluation results.\n",
    "This allows us to store the Flow Judge scores and feedback in Langfuse\n",
    "for monitoring and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the Langfuse traces with the evaluation results\n",
    "for trace, result in zip(traces, results):\n",
    "    langfuse.score( \n",
    "        trace_id=trace.id,\n",
    "        name=RESPONSE_FAITHFULNESS_5POINT.name,\n",
    "        value=result.score,\n",
    "        comment=result.feedback\n",
    "    )\n",
    "\n",
    "# Flush to ensure all updates are sent to Langfuse\n",
    "langfuse.flush()\n",
    "\n",
    "# Shutdown Langfuse client\n",
    "langfuse.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Batch evaluated traces in Langfuse](langfuse_images/batch_evaluated_traces.png) \n",
    "\n",
    "You can also monitor the evaluation results overtime on the Langfuse dashboard. \n",
    "\n",
    "![Langfuse dashboard](langfuse_images/langfuse_dashboard.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the use of Flow Judge to evaluate Langfuse traces in two key scenarios: individual trace evaluation in a production setup and batch evaluation in an offline environment.\n",
    "\n",
    "In the first section, we focused on an e-commerce customer support scenario, where we created a custom metric to assess the issue resolution of the LLM system's response. By defining clear evaluation criteria and utilizing a Likert-5 scale rubric, we provided feedback on how well the response addressed the customer's issue, updating the Langfuse trace with the score and feedback.\n",
    "\n",
    "In the second section, we explored batch evaluation by loading a dataset into Langfuse, retrieving traces, and evaluating them using predefined faithfulness metric. This approach allows for efficient processing and analysis of multiple traces, enhancing the overall evaluation process.\n",
    "\n",
    "These techniques can be adapted to fit various production environments and datasets, providing valuable insights into LLM system performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flow-eval-tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
