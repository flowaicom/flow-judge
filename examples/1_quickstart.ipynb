{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart\n",
    "\n",
    "This tutorial demonstrates how to use the `flow-judge` library to perform language model-based evaluations using Flow-Judge-v0.1 models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running an evaluation\n",
    "\n",
    "Running an evaluation is as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting llamafile server...\n",
      "INFO:root:Downloading llamafile to /home/bernardo/.cache/flow-judge/flow-judge.llamafile\n",
      "Downloading: 100%|██████████| 2.45G/2.45G [00:41<00:00, 63.8MiB/s]\n",
      "INFO:root:Llamafile path: /home/bernardo/.cache/flow-judge/flow-judge.llamafile\n",
      "INFO:root:Starting llamafile server with command: sh -c '/home/bernardo/.cache/flow-judge/flow-judge.llamafile --server --host 127.0.0.1 --port 8085 -c 8192 -ngl 34 --temp 0.1 -n 2000 --threads 24 --nobrowser -b 32 --parallel 1 --cont-batching'\n",
      "INFO:root:Subprocess started with PID: 15533\n",
      "INFO:openai._base_client:Retrying request to /models in 0.454612 seconds\n",
      "INFO:root:import_cuda_impl: initializing gpu module...\n",
      "INFO:root:extracting /zip/llama.cpp/ggml.h to /home/bernardo/.llamafile/v/0.8.13/ggml.h\n",
      "INFO:root:extracting /zip/llamafile/compcap.cu to /home/bernardo/.llamafile/v/0.8.13/compcap.cu\n",
      "INFO:root:extracting /zip/llamafile/llamafile.h to /home/bernardo/.llamafile/v/0.8.13/llamafile.h\n",
      "INFO:root:extracting /zip/llamafile/tinyblas.h to /home/bernardo/.llamafile/v/0.8.13/tinyblas.h\n",
      "INFO:root:extracting /zip/llamafile/tinyblas.cu to /home/bernardo/.llamafile/v/0.8.13/tinyblas.cu\n",
      "INFO:root:extracting /zip/llama.cpp/ggml-impl.h to /home/bernardo/.llamafile/v/0.8.13/ggml-impl.h\n",
      "INFO:root:extracting /zip/llama.cpp/ggml-cuda.h to /home/bernardo/.llamafile/v/0.8.13/ggml-cuda.h\n",
      "INFO:root:extracting /zip/llama.cpp/ggml-alloc.h to /home/bernardo/.llamafile/v/0.8.13/ggml-alloc.h\n",
      "INFO:root:extracting /zip/llama.cpp/ggml-common.h to /home/bernardo/.llamafile/v/0.8.13/ggml-common.h\n",
      "INFO:root:extracting /zip/llama.cpp/ggml-backend.h to /home/bernardo/.llamafile/v/0.8.13/ggml-backend.h\n",
      "INFO:root:extracting /zip/llama.cpp/ggml-backend-impl.h to /home/bernardo/.llamafile/v/0.8.13/ggml-backend-impl.h\n",
      "INFO:root:extracting /zip/llama.cpp/ggml-cuda.cu to /home/bernardo/.llamafile/v/0.8.13/ggml-cuda.cu\n",
      "INFO:root:get_rocm_bin_path: note: hipcc not found on $PATH\n",
      "INFO:root:get_rocm_bin_path: note: $HIP_PATH/bin/hipcc does not exist\n",
      "INFO:root:get_rocm_bin_path: note: /opt/rocm/bin/hipcc does not exist\n",
      "INFO:root:extracting /zip/ggml-rocm.so to /home/bernardo/.llamafile/v/0.8.13/ggml-rocm.so\n",
      "INFO:openai._base_client:Retrying request to /models in 0.856212 seconds\n",
      "INFO:root:link_cuda_dso: note: dynamically linking /home/bernardo/.llamafile/v/0.8.13/ggml-rocm.so\n",
      "INFO:root:link_cuda_dso: warning: libamdhip64.so.6: cannot open shared object file: No such file or directory: failed to load library\n",
      "INFO:root:import_cuda_impl: won't compile AMD GPU support because $HIP_PATH/bin/clang++ is missing\n",
      "INFO:root:extracting /zip/ggml-rocm.so to /home/bernardo/.llamafile/v/0.8.13/ggml-rocm.so\n",
      "INFO:root:link_cuda_dso: note: dynamically linking /home/bernardo/.llamafile/v/0.8.13/ggml-rocm.so\n",
      "INFO:root:link_cuda_dso: warning: libamdhip64.so.6: cannot open shared object file: No such file or directory: failed to load library\n",
      "INFO:root:extracting /zip/ggml-cuda.so to /home/bernardo/.llamafile/v/0.8.13/ggml-cuda.so\n",
      "INFO:root:link_cuda_dso: note: dynamically linking /home/bernardo/.llamafile/v/0.8.13/ggml-cuda.so\n",
      "INFO:root:ggml_cuda_link: welcome to CUDA SDK with tinyBLAS\n",
      "INFO:root:link_cuda_dso: GPU support loaded\n",
      "INFO:root:{\"build\":1500,\"commit\":\"a30b324\",\"function\":\"server_cli\",\"level\":\"INFO\",\"line\":2841,\"msg\":\"build info\",\"tid\":\"11681088\",\"timestamp\":1728369857}\n",
      "INFO:root:{\"function\":\"server_cli\",\"level\":\"INFO\",\"line\":2844,\"msg\":\"system info\",\"n_threads\":24,\"n_threads_batch\":-1,\"system_info\":\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \",\"tid\":\"11681088\",\"timestamp\":1728369857,\"total_threads\":24}\n",
      "INFO:root:llama_model_loader: loaded meta data with 33 key-value pairs and 197 tensors from flow-judge-v0.1-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "INFO:root:llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "INFO:root:llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
      "INFO:root:llama_model_loader: - kv   1:                               general.type str              = model\n",
      "INFO:root:llama_model_loader: - kv   2:                               general.name str              = Phi 3.5 Mini Instruct\n",
      "INFO:root:llama_model_loader: - kv   3:                       general.organization str              = Microsoft\n",
      "INFO:root:llama_model_loader: - kv   4:                           general.finetune str              = instruct\n",
      "INFO:root:llama_model_loader: - kv   5:                           general.basename str              = Phi-3.5\n",
      "INFO:root:llama_model_loader: - kv   6:                         general.size_label str              = mini\n",
      "INFO:root:llama_model_loader: - kv   7:                        phi3.context_length u32              = 131072\n",
      "INFO:root:llama_model_loader: - kv   8:  phi3.rope.scaling.original_context_length u32              = 4096\n",
      "INFO:root:llama_model_loader: - kv   9:                      phi3.embedding_length u32              = 3072\n",
      "INFO:root:llama_model_loader: - kv  10:                   phi3.feed_forward_length u32              = 8192\n",
      "INFO:root:llama_model_loader: - kv  11:                           phi3.block_count u32              = 32\n",
      "INFO:root:llama_model_loader: - kv  12:                  phi3.attention.head_count u32              = 32\n",
      "INFO:root:llama_model_loader: - kv  13:               phi3.attention.head_count_kv u32              = 32\n",
      "INFO:root:llama_model_loader: - kv  14:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "INFO:root:llama_model_loader: - kv  15:                  phi3.rope.dimension_count u32              = 96\n",
      "INFO:root:llama_model_loader: - kv  16:                        phi3.rope.freq_base f32              = 10000.000000\n",
      "INFO:root:llama_model_loader: - kv  17:                          general.file_type u32              = 15\n",
      "INFO:root:llama_model_loader: - kv  18:              phi3.attention.sliding_window u32              = 262144\n",
      "INFO:root:llama_model_loader: - kv  19:              phi3.rope.scaling.attn_factor f32              = 1.190238\n",
      "INFO:root:llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "INFO:root:llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\n",
      "INFO:root:llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "INFO:root:llama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...\n",
      "INFO:root:llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "INFO:root:llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "INFO:root:llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "INFO:root:llama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "INFO:root:llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 32000\n",
      "INFO:root:llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = false\n",
      "INFO:root:llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
      "INFO:root:llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "INFO:root:llama_model_loader: - kv  32:               general.quantization_version u32              = 2\n",
      "INFO:root:llama_model_loader: - type  f32:   67 tensors\n",
      "INFO:root:llama_model_loader: - type q4_K:   81 tensors\n",
      "INFO:root:llama_model_loader: - type q5_K:   32 tensors\n",
      "INFO:root:llama_model_loader: - type q6_K:   17 tensors\n",
      "INFO:root:llm_load_vocab: special tokens cache size = 14\n",
      "INFO:root:llm_load_vocab: token to piece cache size = 0.1685 MB\n",
      "INFO:root:llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "INFO:root:llm_load_print_meta: arch             = phi3\n",
      "INFO:root:llm_load_print_meta: vocab type       = SPM\n",
      "INFO:root:llm_load_print_meta: n_vocab          = 32064\n",
      "INFO:root:llm_load_print_meta: n_merges         = 0\n",
      "INFO:root:llm_load_print_meta: vocab_only       = 0\n",
      "INFO:root:llm_load_print_meta: n_ctx_train      = 131072\n",
      "INFO:root:llm_load_print_meta: n_embd           = 3072\n",
      "INFO:root:llm_load_print_meta: n_layer          = 32\n",
      "INFO:root:llm_load_print_meta: n_head           = 32\n",
      "INFO:root:llm_load_print_meta: n_head_kv        = 32\n",
      "INFO:root:llm_load_print_meta: n_rot            = 96\n",
      "INFO:root:llm_load_print_meta: n_swa            = 262144\n",
      "INFO:root:llm_load_print_meta: n_embd_head_k    = 96\n",
      "INFO:root:llm_load_print_meta: n_embd_head_v    = 96\n",
      "INFO:root:llm_load_print_meta: n_gqa            = 1\n",
      "INFO:root:llm_load_print_meta: n_embd_k_gqa     = 3072\n",
      "INFO:root:llm_load_print_meta: n_embd_v_gqa     = 3072\n",
      "INFO:root:llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "INFO:root:llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "INFO:root:llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "INFO:root:llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "INFO:root:llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "INFO:root:llm_load_print_meta: n_ff             = 8192\n",
      "INFO:root:llm_load_print_meta: n_expert         = 0\n",
      "INFO:root:llm_load_print_meta: n_expert_used    = 0\n",
      "INFO:root:llm_load_print_meta: causal attn      = 1\n",
      "INFO:root:llm_load_print_meta: pooling type     = 0\n",
      "INFO:root:llm_load_print_meta: rope type        = 2\n",
      "INFO:root:llm_load_print_meta: rope scaling     = linear\n",
      "INFO:root:llm_load_print_meta: freq_base_train  = 10000.0\n",
      "INFO:root:llm_load_print_meta: freq_scale_train = 1\n",
      "INFO:root:llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "INFO:root:llm_load_print_meta: rope_finetuned   = unknown\n",
      "INFO:root:llm_load_print_meta: ssm_d_conv       = 0\n",
      "INFO:root:llm_load_print_meta: ssm_d_inner      = 0\n",
      "INFO:root:llm_load_print_meta: ssm_d_state      = 0\n",
      "INFO:root:llm_load_print_meta: ssm_dt_rank      = 0\n",
      "INFO:root:llm_load_print_meta: model type       = 3B\n",
      "INFO:root:llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "INFO:root:llm_load_print_meta: model params     = 3.82 B\n",
      "INFO:root:llm_load_print_meta: model size       = 2.23 GiB (5.01 BPW)\n",
      "INFO:root:llm_load_print_meta: general.name     = Phi 3.5 Mini Instruct\n",
      "INFO:root:llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "INFO:root:llm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\n",
      "INFO:root:llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "INFO:root:llm_load_print_meta: PAD token        = 32000 '<|endoftext|>'\n",
      "INFO:root:llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "INFO:root:llm_load_print_meta: EOT token        = 32007 '<|end|>'\n",
      "INFO:root:llm_load_print_meta: max token length = 48\n",
      "INFO:root:ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "INFO:root:ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "INFO:root:ggml_cuda_init: found 1 CUDA devices:\n",
      "INFO:root:Device 0: NVIDIA GeForce RTX 3090 Ti, compute capability 8.6, VMM: yes\n",
      "INFO:root:llm_load_tensors: ggml ctx size =    0.24 MiB\n",
      "INFO:root:llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "INFO:root:llm_load_tensors: offloaded 32/33 layers to GPU\n",
      "INFO:root:llm_load_tensors:        CPU buffer size =  2281.66 MiB\n",
      "INFO:root:llm_load_tensors:      CUDA0 buffer size =  2151.77 MiB\n",
      "INFO:root:............................................................................................\n",
      "INFO:root:llama_new_context_with_model: n_ctx      = 8192\n",
      "INFO:root:llama_new_context_with_model: n_batch    = 32\n",
      "INFO:root:llama_new_context_with_model: n_ubatch   = 32\n",
      "INFO:root:llama_new_context_with_model: flash_attn = 0\n",
      "INFO:root:llama_new_context_with_model: freq_base  = 10000.0\n",
      "INFO:root:llama_new_context_with_model: freq_scale = 1\n",
      "INFO:root:llama_kv_cache_init:      CUDA0 KV buffer size =  3072.00 MiB\n",
      "INFO:root:llama_new_context_with_model: KV self size  = 3072.00 MiB, K (f16): 1536.00 MiB, V (f16): 1536.00 MiB\n",
      "INFO:root:llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
      "INFO:root:llama_new_context_with_model:      CUDA0 compute buffer size =    81.72 MiB\n",
      "INFO:root:llama_new_context_with_model:  CUDA_Host compute buffer size =     1.38 MiB\n",
      "INFO:root:llama_new_context_with_model: graph nodes  = 1286\n",
      "INFO:root:llama_new_context_with_model: graph splits = 4\n",
      "INFO:root:{\"function\":\"initialize\",\"level\":\"INFO\",\"line\":491,\"msg\":\"initializing slots\",\"n_slots\":1,\"tid\":\"11681088\",\"timestamp\":1728369858}\n",
      "INFO:root:\n",
      "INFO:root:{\"function\":\"initialize\",\"level\":\"INFO\",\"line\":500,\"msg\":\"new slot\",\"n_ctx_slot\":8192,\"slot_id\":0,\"tid\":\"11681088\",\"timestamp\":1728369858}\n",
      "INFO:root:llama server listening at http://127.0.0.1:8085\n",
      "INFO:root:{\"function\":\"server_cli\",\"level\":\"INFO\",\"line\":3062,\"msg\":\"model loaded\",\"tid\":\"11681088\",\"timestamp\":1728369858}\n",
      "INFO:root:{\"function\":\"server_cli\",\"hostname\":\"127.0.0.1\",\"level\":\"INFO\",\"line\":3185,\"msg\":\"HTTP server listening\",\"port\":\"8085\",\"tid\":\"11681088\",\"timestamp\":1728369858}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1661,\"msg\":\"all slots are idle and system prompt is empty, clear the KV cache\",\"tid\":\"11681088\",\"timestamp\":1728369858}\n",
      "INFO:root:\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:8085/v1/models \"HTTP/1.1 200 OK\"\n",
      "INFO:root:{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"GET\",\"msg\":\"request\",\"params\":{},\"path\":\"/v1/models\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":46822,\"status\":200,\"tid\":\"139087058437344\",\"timestamp\":1728369858}\n",
      "INFO:root:Llamafile server started successfully\n",
      "INFO:root:{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"GET\",\"msg\":\"request\",\"params\":{},\"path\":\"/v1/models\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":46822,\"status\":200,\"tid\":\"139087058437344\",\"timestamp\":1728369858}\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:8085/v1/models \"HTTP/1.1 200 OK\"\n",
      "INFO:root:{\"function\":\"launch_slot_with_data\",\"level\":\"INFO\",\"line\":886,\"msg\":\"slot is processing task\",\"slot_id\":0,\"task_id\":0,\"tid\":\"11681088\",\"timestamp\":1728369858}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1912,\"msg\":\"kv cache rm [p0, end)\",\"p0\":0,\"slot_id\":0,\"task_id\":0,\"tid\":\"11681088\",\"timestamp\":1728369858}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":313,\"msg\":\"prompt eval time     =    1101.92 ms /  2145 tokens (    0.51 ms per token,  1946.61 tokens per second)\",\"n_tokens_second\":1946.607593856888,\"num_prompt_tokens_processed\":2145,\"slot_id\":0,\"t_prompt_processing\":1101.917,\"t_token\":0.5137142191142191,\"task_id\":0,\"tid\":\"11681088\",\"timestamp\":1728369862}\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8085/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":327,\"msg\":\"generation eval time =    3096.90 ms /   212 runs   (   14.61 ms per token,    68.46 tokens per second)\",\"n_decoded\":212,\"n_tokens_second\":68.45546390847116,\"slot_id\":0,\"t_token\":14.608037735849056,\"t_token_generation\":3096.904,\"task_id\":0,\"tid\":\"11681088\",\"timestamp\":1728369862}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":337,\"msg\":\"          total time =    4198.82 ms\",\"slot_id\":0,\"t_prompt_processing\":1101.917,\"t_token_generation\":3096.904,\"t_total\":4198.821,\"task_id\":0,\"tid\":\"11681088\",\"timestamp\":1728369862}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1723,\"msg\":\"slot released\",\"n_cache_tokens\":2357,\"n_ctx\":8192,\"n_past\":2356,\"n_system_tokens\":0,\"slot_id\":0,\"task_id\":0,\"tid\":\"11681088\",\"timestamp\":1728369862,\"truncated\":false}\n",
      "INFO:root:{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"POST\",\"msg\":\"request\",\"params\":{},\"path\":\"/v1/chat/completions\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":46822,\"status\":200,\"tid\":\"139087058437344\",\"timestamp\":1728369862}\n"
     ]
    }
   ],
   "source": [
    "from flow_judge import Vllm, Llamafile, Hf, EvalInput, FlowJudge\n",
    "\n",
    "from flow_judge.metrics import RESPONSE_FAITHFULNESS_5POINT\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "# If you are running on an Ampere GPU or newer, create a model using VLLM\n",
    "# model = Vllm()\n",
    "\n",
    "# If you have other applications open taking up VRAM, you can use less VRAM by setting gpu_memory_utilization to a lower value.\n",
    "# model = Vllm(gpu_memory_utilization=0.70)\n",
    "\n",
    "# Or if not running on Ampere GPU or newer, create a model using no flash attn and Hugging Face Transformers\n",
    "# model = Hf(flash_attn=False)\n",
    "\n",
    "# Or create a model using Llamafile if not running an Nvidia GPU & running a Silicon MacOS for example\n",
    "model = Llamafile()\n",
    "\n",
    "\n",
    "# Initialize the judge\n",
    "faithfulness_judge = FlowJudge(\n",
    "    metric=RESPONSE_FAITHFULNESS_5POINT,\n",
    "    model=model\n",
    ")\n",
    "\n",
    "# Sample to evaluate\n",
    "query = \"\"\"Please read the technical issue that the user is facing and help me create a detailed solution based on the context provided.\"\"\"\n",
    "context = \"\"\"# Customer Issue:\n",
    "I'm having trouble when uploading a git lfs tracked file to my repo: (base)  bernardo@bernardo-desktop  ~/repos/lm-evaluation-harness  ↱ Flow-Judge-v0.1_evals  git push\n",
    "batch response: This repository is over its data quota. Account responsible for LFS bandwidth should purchase more data packs to restore access.\n",
    "\n",
    "# Documentation:\n",
    "Configuring Git Large File Storage\n",
    "Once Git LFS is installed, you need to associate it with a large file in your repository.\n",
    "\n",
    "Platform navigation\n",
    "Mac\n",
    "Windows\n",
    "Linux\n",
    "If there are existing files in your repository that you'd like to use GitHub with, you need to first remove them from the repository and then add them to Git LFS locally. For more information, see \"Moving a file in your repository to Git Large File Storage.\"\n",
    "\n",
    "If there are referenced Git LFS files that did not upload successfully, you will receive an error message. For more information, see \"Resolving Git Large File Storage upload failures.\"\n",
    "\n",
    "Open Terminal.\n",
    "\n",
    "Change your current working directory to an existing repository you'd like to use with Git LFS.\n",
    "\n",
    "To associate a file type in your repository with Git LFS, enter git lfs track followed by the name of the file extension you want to automatically upload to Git LFS.\n",
    "\n",
    "For example, to associate a .psd file, enter the following command:\n",
    "\n",
    "$ git lfs track \"*.psd\"\n",
    "> Tracking \"*.psd\"\n",
    "Every file type you want to associate with Git LFS will need to be added with git lfs track. This command amends your repository's .gitattributes file and associates large files with Git LFS.\n",
    "\n",
    "Note: We strongly suggest that you commit your local .gitattributes file into your repository.\n",
    "\n",
    "Relying on a global .gitattributes file associated with Git LFS may cause conflicts when contributing to other Git projects.\n",
    "Including the .gitattributes file in the repository allows people creating forks or fresh clones to more easily collaborate using Git LFS.\n",
    "Including the .gitattributes file in the repository allows Git LFS objects to optionally be included in ZIP file and tarball archives.\n",
    "Add a file to the repository matching the extension you've associated:\n",
    "\n",
    "git add path/to/file.psd\n",
    "Commit the file and push it to GitHub:\n",
    "\n",
    "git commit -m \"add file.psd\"\n",
    "git push\n",
    "You should see some diagnostic information about your file upload:\n",
    "\n",
    "> Sending file.psd\n",
    "> 44.74 MB / 81.04 MB  55.21 % 14s\n",
    "> 64.74 MB / 81.04 MB  79.21 % 3s\"\"\"\n",
    "response = \"\"\"# Resolving Git LFS Data Quota Issue\n",
    "\n",
    "## Problem\n",
    "The user is encountering an error when trying to push Git LFS tracked files:\n",
    "```\n",
    "This repository is over its data quota. Account responsible for LFS bandwidth should purchase more data packs to restore access.\n",
    "```\n",
    "\n",
    "## Solution\n",
    "\n",
    "1. **Understand the issue**\n",
    "   - The error message indicates that the repository has exceeded its data quota for Git LFS (Large File Storage).\n",
    "   - This is likely due to the size or number of large files being tracked by Git LFS.\n",
    "\n",
    "2. **Check current LFS usage**\n",
    "   - Run the following command to see the current LFS objects:\n",
    "     ```\n",
    "     git lfs ls-files\n",
    "     ```\n",
    "   - This will show you which files are being tracked by Git LFS and their sizes.\n",
    "\n",
    "3. **Review Git LFS pricing and quotas**\n",
    "   - Visit GitHub's pricing page to understand the current LFS storage and bandwidth limits for your account type.\n",
    "   - Free accounts typically have lower limits compared to paid plans.\n",
    "\n",
    "4. **Options to resolve the issue**\n",
    "\n",
    "   a. **Purchase more data packs**\n",
    "      - If you need to keep all current LFS objects and continue using LFS extensively:\n",
    "        1. Go to your GitHub account settings.\n",
    "        2. Navigate to the Billing section.\n",
    "        3. Look for options to purchase additional data packs for Git LFS.\n",
    "\n",
    "   b. **Optimize repository**\n",
    "      - If purchasing more data is not an option, consider optimizing your repository:\n",
    "        1. Remove unnecessary large files from LFS tracking.\n",
    "        2. Use `.gitignore` to exclude large files that don't need version control.\n",
    "        3. Consider using alternative storage solutions for very large assets.\n",
    "\n",
    "   c. **Clean up LFS cache**\n",
    "      - Sometimes, cleaning up the LFS cache can help:\n",
    "        ```\n",
    "        git lfs prune\n",
    "        ```\n",
    "\n",
    "5. **Reconfigure Git LFS tracking**\n",
    "   - Review your `.gitattributes` file to ensure only necessary file types are tracked:\n",
    "     ```\n",
    "     git lfs track\n",
    "     ```\n",
    "   - Modify tracking as needed:\n",
    "     ```\n",
    "     git lfs track \"*.psd\"  # Track PSD files\n",
    "     git lfs untrack \"*.zip\"  # Stop tracking ZIP files\n",
    "     ```\n",
    "\n",
    "6. **Commit changes and retry push**\n",
    "   - After making necessary changes:\n",
    "     ```\n",
    "     git add .gitattributes\n",
    "     git commit -m \"Update Git LFS tracking\"\n",
    "     git push\n",
    "     ```\n",
    "\n",
    "7. **If issues persist**\n",
    "   - Contact GitHub support for further assistance.\n",
    "   - They may be able to provide more specific guidance based on your account and repository details.\n",
    "\n",
    "Remember to regularly monitor your Git LFS usage to avoid hitting quotas in the future. Consider setting up alerts or regularly checking your GitHub account's storage usage statistics.\"\"\"\n",
    "\n",
    "# Create an EvalInput\n",
    "# We want to evaluate the response to the customer issue based on the context and the user instructions\n",
    "eval_input = EvalInput(\n",
    "    inputs=[\n",
    "        {\"query\": query},\n",
    "        {\"context\": context},\n",
    "    ],\n",
    "    output={\"response\": response},\n",
    ")\n",
    "\n",
    "# Run the evaluation\n",
    "result = faithfulness_judge.evaluate(eval_input, save_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "__Feedback:__\n",
       "The response provided is highly consistent with the given context and does not contain any hallucinated or fabricated information. It directly addresses the customer's issue of exceeding the Git LFS data quota and offers a detailed solution based on the information provided in the context. The solution includes checking current LFS usage, reviewing Git LFS pricing and quotas, and various options to resolve the issue, such as purchasing more data packs or optimizing the repository. The steps outlined are all supported by the context, including commands like `git lfs ls-files`, `git lfs prune`, and `git lfs track`. The response also suggests reviewing the `.gitattributes` file and committing changes, which are all mentioned in the context. There are no significant deviations or fabrications from the provided information.\n",
       "\n",
       "Therefore, the response is completely consistent with and faithful to the provided context.\n",
       "\n",
       "__Score:__\n",
       "5"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the result\n",
    "display(Markdown(f\"__Feedback:__\\n{result.feedback}\\n\\n__Score:__\\n{result.score}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "`flow-judge` support different model configurations. This refers to the library use for running inference with the models. We currently support:\n",
    "- vLLM sync & async (default engine, mode sync)\n",
    "- Hugging Face\n",
    "- Llamafile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "A judge is initialized with a metric and a model.\n",
    "\n",
    "We include some common metrics in the library, such as:\n",
    "- RESPONSE_FAITHFULNESS_3POINT\n",
    "- RESPONSE_FAITHFULNESS_5POINT\n",
    "- RESPONSE_COMPREHENSIVENESS_3POINT\n",
    "- RESPONSE_COMPREHENSIVENESS_5POINT\n",
    "\n",
    "But you can also implement your own metrics and use them with the judge.\n",
    "\n",
    "Note that metrics have required inputs and outputs as you can see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: Response Faithfulness (5-point Likert)\n",
      "Required inputs: query, context\n",
      "Required output: response\n"
     ]
    }
   ],
   "source": [
    "RESPONSE_FAITHFULNESS_5POINT.print_required_keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`flow-judge` checks under the hood if the keys match. This is important to ensure the right prompt is being formatted.\n",
    "\n",
    "When you define a custom metric, you should specify the required keys as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running batched evaluations\n",
    "\n",
    "The `FlowJudge` class also supports batch evaluation. This is useful when you want to evaluate multiple samples at once in Evaluation-Driven Development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:{\"function\":\"launch_slot_with_data\",\"level\":\"INFO\",\"line\":886,\"msg\":\"slot is processing task\",\"slot_id\":0,\"task_id\":214,\"tid\":\"11681088\",\"timestamp\":1728369888}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1912,\"msg\":\"kv cache rm [p0, end)\",\"p0\":0,\"slot_id\":0,\"task_id\":214,\"tid\":\"11681088\",\"timestamp\":1728369888}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":313,\"msg\":\"prompt eval time     =     586.47 ms /  1257 tokens (    0.47 ms per token,  2143.33 tokens per second)\",\"n_tokens_second\":2143.3321397513937,\"num_prompt_tokens_processed\":1257,\"slot_id\":0,\"t_prompt_processing\":586.47,\"t_token\":0.46656324582338904,\"task_id\":214,\"tid\":\"11681088\",\"timestamp\":1728369893}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":327,\"msg\":\"generation eval time =    4293.86 ms /   364 runs   (   11.80 ms per token,    84.77 tokens per second)\",\"n_decoded\":364,\"n_tokens_second\":84.77217013495078,\"slot_id\":0,\"t_token\":11.796324175824177,\"t_token_generation\":4293.862,\"task_id\":214,\"tid\":\"11681088\",\"timestamp\":1728369893}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":337,\"msg\":\"          total time =    4880.33 ms\",\"slot_id\":0,\"t_prompt_processing\":586.47,\"t_token_generation\":4293.862,\"t_total\":4880.332,\"task_id\":214,\"tid\":\"11681088\",\"timestamp\":1728369893}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1723,\"msg\":\"slot released\",\"n_cache_tokens\":1621,\"n_ctx\":8192,\"n_past\":1620,\"n_system_tokens\":0,\"slot_id\":0,\"task_id\":214,\"tid\":\"11681088\",\"timestamp\":1728369893,\"truncated\":false}\n",
      "INFO:root:{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"POST\",\"msg\":\"request\",\"params\":{},\"path\":\"/v1/chat/completions\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":35206,\"status\":200,\"tid\":\"139087058442240\",\"timestamp\":1728369893}\n",
      "INFO:root:{\"function\":\"launch_slot_with_data\",\"level\":\"INFO\",\"line\":886,\"msg\":\"slot is processing task\",\"slot_id\":0,\"task_id\":580,\"tid\":\"11681088\",\"timestamp\":1728369893}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1912,\"msg\":\"kv cache rm [p0, end)\",\"p0\":0,\"slot_id\":0,\"task_id\":580,\"tid\":\"11681088\",\"timestamp\":1728369893}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":313,\"msg\":\"prompt eval time     =     533.75 ms /  1228 tokens (    0.43 ms per token,  2300.69 tokens per second)\",\"n_tokens_second\":2300.6939552451327,\"num_prompt_tokens_processed\":1228,\"slot_id\":0,\"t_prompt_processing\":533.752,\"t_token\":0.4346514657980456,\"task_id\":580,\"tid\":\"11681088\",\"timestamp\":1728369898}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":327,\"msg\":\"generation eval time =    4554.39 ms /   388 runs   (   11.74 ms per token,    85.19 tokens per second)\",\"n_decoded\":388,\"n_tokens_second\":85.19249111626755,\"slot_id\":0,\"t_token\":11.738123711340206,\"t_token_generation\":4554.392,\"task_id\":580,\"tid\":\"11681088\",\"timestamp\":1728369898}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":337,\"msg\":\"          total time =    5088.14 ms\",\"slot_id\":0,\"t_prompt_processing\":533.752,\"t_token_generation\":4554.392,\"t_total\":5088.144,\"task_id\":580,\"tid\":\"11681088\",\"timestamp\":1728369898}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1723,\"msg\":\"slot released\",\"n_cache_tokens\":1616,\"n_ctx\":8192,\"n_past\":1615,\"n_system_tokens\":0,\"slot_id\":0,\"task_id\":580,\"tid\":\"11681088\",\"timestamp\":1728369898,\"truncated\":false}\n",
      "INFO:root:{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"POST\",\"msg\":\"request\",\"params\":{},\"path\":\"/v1/chat/completions\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":35206,\"status\":200,\"tid\":\"139087058442240\",\"timestamp\":1728369898}\n",
      "INFO:root:{\"function\":\"launch_slot_with_data\",\"level\":\"INFO\",\"line\":886,\"msg\":\"slot is processing task\",\"slot_id\":0,\"task_id\":970,\"tid\":\"11681088\",\"timestamp\":1728369898}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1912,\"msg\":\"kv cache rm [p0, end)\",\"p0\":0,\"slot_id\":0,\"task_id\":970,\"tid\":\"11681088\",\"timestamp\":1728369898}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":313,\"msg\":\"prompt eval time     =     642.88 ms /  1413 tokens (    0.45 ms per token,  2197.93 tokens per second)\",\"n_tokens_second\":2197.932108319321,\"num_prompt_tokens_processed\":1413,\"slot_id\":0,\"t_prompt_processing\":642.877,\"t_token\":0.45497310686482656,\"task_id\":970,\"tid\":\"11681088\",\"timestamp\":1728369901}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":327,\"msg\":\"generation eval time =    2313.06 ms /   193 runs   (   11.98 ms per token,    83.44 tokens per second)\",\"n_decoded\":193,\"n_tokens_second\":83.43921755630309,\"slot_id\":0,\"t_token\":11.984772020725389,\"t_token_generation\":2313.061,\"task_id\":970,\"tid\":\"11681088\",\"timestamp\":1728369901}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":337,\"msg\":\"          total time =    2955.94 ms\",\"slot_id\":0,\"t_prompt_processing\":642.877,\"t_token_generation\":2313.061,\"t_total\":2955.938,\"task_id\":970,\"tid\":\"11681088\",\"timestamp\":1728369901}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1723,\"msg\":\"slot released\",\"n_cache_tokens\":1606,\"n_ctx\":8192,\"n_past\":1605,\"n_system_tokens\":0,\"slot_id\":0,\"task_id\":970,\"tid\":\"11681088\",\"timestamp\":1728369901,\"truncated\":false}\n",
      "INFO:root:{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"POST\",\"msg\":\"request\",\"params\":{},\"path\":\"/v1/chat/completions\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":35206,\"status\":200,\"tid\":\"139087058442240\",\"timestamp\":1728369901}\n",
      "INFO:root:{\"function\":\"launch_slot_with_data\",\"level\":\"INFO\",\"line\":886,\"msg\":\"slot is processing task\",\"slot_id\":0,\"task_id\":1165,\"tid\":\"11681088\",\"timestamp\":1728369901}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1912,\"msg\":\"kv cache rm [p0, end)\",\"p0\":0,\"slot_id\":0,\"task_id\":1165,\"tid\":\"11681088\",\"timestamp\":1728369901}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":313,\"msg\":\"prompt eval time     =     589.66 ms /  1325 tokens (    0.45 ms per token,  2247.06 tokens per second)\",\"n_tokens_second\":2247.061437203536,\"num_prompt_tokens_processed\":1325,\"slot_id\":0,\"t_prompt_processing\":589.659,\"t_token\":0.44502566037735847,\"task_id\":1165,\"tid\":\"11681088\",\"timestamp\":1728369906}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":327,\"msg\":\"generation eval time =    4213.76 ms /   344 runs   (   12.25 ms per token,    81.64 tokens per second)\",\"n_decoded\":344,\"n_tokens_second\":81.63739942165599,\"slot_id\":0,\"t_token\":12.249287790697675,\"t_token_generation\":4213.755,\"task_id\":1165,\"tid\":\"11681088\",\"timestamp\":1728369906}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":337,\"msg\":\"          total time =    4803.41 ms\",\"slot_id\":0,\"t_prompt_processing\":589.659,\"t_token_generation\":4213.755,\"t_total\":4803.414,\"task_id\":1165,\"tid\":\"11681088\",\"timestamp\":1728369906}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1723,\"msg\":\"slot released\",\"n_cache_tokens\":1669,\"n_ctx\":8192,\"n_past\":1668,\"n_system_tokens\":0,\"slot_id\":0,\"task_id\":1165,\"tid\":\"11681088\",\"timestamp\":1728369906,\"truncated\":false}\n",
      "INFO:root:{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"POST\",\"msg\":\"request\",\"params\":{},\"path\":\"/v1/chat/completions\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":35206,\"status\":200,\"tid\":\"139087058442240\",\"timestamp\":1728369906}\n",
      "INFO:root:{\"function\":\"launch_slot_with_data\",\"level\":\"INFO\",\"line\":886,\"msg\":\"slot is processing task\",\"slot_id\":0,\"task_id\":1511,\"tid\":\"11681088\",\"timestamp\":1728369906}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1912,\"msg\":\"kv cache rm [p0, end)\",\"p0\":0,\"slot_id\":0,\"task_id\":1511,\"tid\":\"11681088\",\"timestamp\":1728369906}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":313,\"msg\":\"prompt eval time     =     608.90 ms /  1362 tokens (    0.45 ms per token,  2236.81 tokens per second)\",\"n_tokens_second\":2236.8058018998067,\"num_prompt_tokens_processed\":1362,\"slot_id\":0,\"t_prompt_processing\":608.904,\"t_token\":0.4470660792951542,\"task_id\":1511,\"tid\":\"11681088\",\"timestamp\":1728369910}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":327,\"msg\":\"generation eval time =    2723.75 ms /   224 runs   (   12.16 ms per token,    82.24 tokens per second)\",\"n_decoded\":224,\"n_tokens_second\":82.23952923743762,\"slot_id\":0,\"t_token\":12.15960267857143,\"t_token_generation\":2723.751,\"task_id\":1511,\"tid\":\"11681088\",\"timestamp\":1728369910}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":337,\"msg\":\"          total time =    3332.66 ms\",\"slot_id\":0,\"t_prompt_processing\":608.904,\"t_token_generation\":2723.751,\"t_total\":3332.655,\"task_id\":1511,\"tid\":\"11681088\",\"timestamp\":1728369910}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1723,\"msg\":\"slot released\",\"n_cache_tokens\":1586,\"n_ctx\":8192,\"n_past\":1585,\"n_system_tokens\":0,\"slot_id\":0,\"task_id\":1511,\"tid\":\"11681088\",\"timestamp\":1728369910,\"truncated\":false}\n",
      "INFO:root:{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"POST\",\"msg\":\"request\",\"params\":{},\"path\":\"/v1/chat/completions\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":35206,\"status\":200,\"tid\":\"139087058442240\",\"timestamp\":1728369910}\n",
      "INFO:root:{\"function\":\"launch_slot_with_data\",\"level\":\"INFO\",\"line\":886,\"msg\":\"slot is processing task\",\"slot_id\":0,\"task_id\":1737,\"tid\":\"11681088\",\"timestamp\":1728369910}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1912,\"msg\":\"kv cache rm [p0, end)\",\"p0\":0,\"slot_id\":0,\"task_id\":1737,\"tid\":\"11681088\",\"timestamp\":1728369910}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":313,\"msg\":\"prompt eval time     =     502.02 ms /  1157 tokens (    0.43 ms per token,  2304.67 tokens per second)\",\"n_tokens_second\":2304.666102285743,\"num_prompt_tokens_processed\":1157,\"slot_id\":0,\"t_prompt_processing\":502.025,\"t_token\":0.4339023336214347,\"task_id\":1737,\"tid\":\"11681088\",\"timestamp\":1728369913}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":327,\"msg\":\"generation eval time =    3208.11 ms /   286 runs   (   11.22 ms per token,    89.15 tokens per second)\",\"n_decoded\":286,\"n_tokens_second\":89.14900726657922,\"slot_id\":0,\"t_token\":11.217174825174826,\"t_token_generation\":3208.112,\"task_id\":1737,\"tid\":\"11681088\",\"timestamp\":1728369913}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":337,\"msg\":\"          total time =    3710.14 ms\",\"slot_id\":0,\"t_prompt_processing\":502.025,\"t_token_generation\":3208.112,\"t_total\":3710.137,\"task_id\":1737,\"tid\":\"11681088\",\"timestamp\":1728369913}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1723,\"msg\":\"slot released\",\"n_cache_tokens\":1443,\"n_ctx\":8192,\"n_past\":1442,\"n_system_tokens\":0,\"slot_id\":0,\"task_id\":1737,\"tid\":\"11681088\",\"timestamp\":1728369913,\"truncated\":false}\n",
      "INFO:root:{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"POST\",\"msg\":\"request\",\"params\":{},\"path\":\"/v1/chat/completions\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":42102,\"status\":200,\"tid\":\"139087058447088\",\"timestamp\":1728369913}\n",
      "INFO:root:{\"function\":\"launch_slot_with_data\",\"level\":\"INFO\",\"line\":886,\"msg\":\"slot is processing task\",\"slot_id\":0,\"task_id\":2025,\"tid\":\"11681088\",\"timestamp\":1728370004}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1912,\"msg\":\"kv cache rm [p0, end)\",\"p0\":0,\"slot_id\":0,\"task_id\":2025,\"tid\":\"11681088\",\"timestamp\":1728370004}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":313,\"msg\":\"prompt eval time     =     583.70 ms /  1257 tokens (    0.46 ms per token,  2153.49 tokens per second)\",\"n_tokens_second\":2153.4887545742367,\"num_prompt_tokens_processed\":1257,\"slot_id\":0,\"t_prompt_processing\":583.704,\"t_token\":0.46436276849642,\"task_id\":2025,\"tid\":\"11681088\",\"timestamp\":1728370009}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":327,\"msg\":\"generation eval time =    4411.45 ms /   367 runs   (   12.02 ms per token,    83.19 tokens per second)\",\"n_decoded\":367,\"n_tokens_second\":83.1926576472527,\"slot_id\":0,\"t_token\":12.020291553133516,\"t_token_generation\":4411.447,\"task_id\":2025,\"tid\":\"11681088\",\"timestamp\":1728370009}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":337,\"msg\":\"          total time =    4995.15 ms\",\"slot_id\":0,\"t_prompt_processing\":583.704,\"t_token_generation\":4411.447,\"t_total\":4995.151,\"task_id\":2025,\"tid\":\"11681088\",\"timestamp\":1728370009}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1723,\"msg\":\"slot released\",\"n_cache_tokens\":1624,\"n_ctx\":8192,\"n_past\":1623,\"n_system_tokens\":0,\"slot_id\":0,\"task_id\":2025,\"tid\":\"11681088\",\"timestamp\":1728370009,\"truncated\":false}\n",
      "INFO:root:{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"POST\",\"msg\":\"request\",\"params\":{},\"path\":\"/v1/chat/completions\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":38416,\"status\":200,\"tid\":\"139087058451888\",\"timestamp\":1728370009}\n",
      "INFO:root:{\"function\":\"launch_slot_with_data\",\"level\":\"INFO\",\"line\":886,\"msg\":\"slot is processing task\",\"slot_id\":0,\"task_id\":2394,\"tid\":\"11681088\",\"timestamp\":1728370009}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1912,\"msg\":\"kv cache rm [p0, end)\",\"p0\":0,\"slot_id\":0,\"task_id\":2394,\"tid\":\"11681088\",\"timestamp\":1728370009}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":313,\"msg\":\"prompt eval time     =     533.36 ms /  1228 tokens (    0.43 ms per token,  2302.38 tokens per second)\",\"n_tokens_second\":2302.384880755962,\"num_prompt_tokens_processed\":1228,\"slot_id\":0,\"t_prompt_processing\":533.36,\"t_token\":0.4343322475570033,\"task_id\":2394,\"tid\":\"11681088\",\"timestamp\":1728370013}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":327,\"msg\":\"generation eval time =    3802.17 ms /   324 runs   (   11.74 ms per token,    85.21 tokens per second)\",\"n_decoded\":324,\"n_tokens_second\":85.2144286964323,\"slot_id\":0,\"t_token\":11.735101851851852,\"t_token_generation\":3802.173,\"task_id\":2394,\"tid\":\"11681088\",\"timestamp\":1728370013}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":337,\"msg\":\"          total time =    4335.53 ms\",\"slot_id\":0,\"t_prompt_processing\":533.36,\"t_token_generation\":3802.173,\"t_total\":4335.532999999999,\"task_id\":2394,\"tid\":\"11681088\",\"timestamp\":1728370013}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1723,\"msg\":\"slot released\",\"n_cache_tokens\":1552,\"n_ctx\":8192,\"n_past\":1551,\"n_system_tokens\":0,\"slot_id\":0,\"task_id\":2394,\"tid\":\"11681088\",\"timestamp\":1728370013,\"truncated\":false}\n",
      "INFO:root:{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"POST\",\"msg\":\"request\",\"params\":{},\"path\":\"/v1/chat/completions\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":38416,\"status\":200,\"tid\":\"139087058451888\",\"timestamp\":1728370013}\n",
      "INFO:root:{\"function\":\"launch_slot_with_data\",\"level\":\"INFO\",\"line\":886,\"msg\":\"slot is processing task\",\"slot_id\":0,\"task_id\":2720,\"tid\":\"11681088\",\"timestamp\":1728370013}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1912,\"msg\":\"kv cache rm [p0, end)\",\"p0\":0,\"slot_id\":0,\"task_id\":2720,\"tid\":\"11681088\",\"timestamp\":1728370013}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":313,\"msg\":\"prompt eval time     =     638.85 ms /  1413 tokens (    0.45 ms per token,  2211.79 tokens per second)\",\"n_tokens_second\":2211.790266557512,\"num_prompt_tokens_processed\":1413,\"slot_id\":0,\"t_prompt_processing\":638.849,\"t_token\":0.4521224345364473,\"task_id\":2720,\"tid\":\"11681088\",\"timestamp\":1728370016}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":327,\"msg\":\"generation eval time =    2510.40 ms /   209 runs   (   12.01 ms per token,    83.25 tokens per second)\",\"n_decoded\":209,\"n_tokens_second\":83.25353210080927,\"slot_id\":0,\"t_token\":12.011502392344498,\"t_token_generation\":2510.404,\"task_id\":2720,\"tid\":\"11681088\",\"timestamp\":1728370016}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":337,\"msg\":\"          total time =    3149.25 ms\",\"slot_id\":0,\"t_prompt_processing\":638.849,\"t_token_generation\":2510.404,\"t_total\":3149.253,\"task_id\":2720,\"tid\":\"11681088\",\"timestamp\":1728370016}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1723,\"msg\":\"slot released\",\"n_cache_tokens\":1622,\"n_ctx\":8192,\"n_past\":1621,\"n_system_tokens\":0,\"slot_id\":0,\"task_id\":2720,\"tid\":\"11681088\",\"timestamp\":1728370016,\"truncated\":false}\n",
      "INFO:root:{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"POST\",\"msg\":\"request\",\"params\":{},\"path\":\"/v1/chat/completions\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":38416,\"status\":200,\"tid\":\"139087058451888\",\"timestamp\":1728370016}\n",
      "INFO:root:{\"function\":\"launch_slot_with_data\",\"level\":\"INFO\",\"line\":886,\"msg\":\"slot is processing task\",\"slot_id\":0,\"task_id\":2931,\"tid\":\"11681088\",\"timestamp\":1728370016}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1912,\"msg\":\"kv cache rm [p0, end)\",\"p0\":0,\"slot_id\":0,\"task_id\":2931,\"tid\":\"11681088\",\"timestamp\":1728370016}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":313,\"msg\":\"prompt eval time     =     576.86 ms /  1325 tokens (    0.44 ms per token,  2296.93 tokens per second)\",\"n_tokens_second\":2296.9297416864147,\"num_prompt_tokens_processed\":1325,\"slot_id\":0,\"t_prompt_processing\":576.857,\"t_token\":0.43536377358490563,\"task_id\":2931,\"tid\":\"11681088\",\"timestamp\":1728370021}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":327,\"msg\":\"generation eval time =    4408.29 ms /   366 runs   (   12.04 ms per token,    83.03 tokens per second)\",\"n_decoded\":366,\"n_tokens_second\":83.0254096317188,\"slot_id\":0,\"t_token\":12.044505464480874,\"t_token_generation\":4408.289,\"task_id\":2931,\"tid\":\"11681088\",\"timestamp\":1728370021}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":337,\"msg\":\"          total time =    4985.15 ms\",\"slot_id\":0,\"t_prompt_processing\":576.857,\"t_token_generation\":4408.289,\"t_total\":4985.146,\"task_id\":2931,\"tid\":\"11681088\",\"timestamp\":1728370021}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1723,\"msg\":\"slot released\",\"n_cache_tokens\":1691,\"n_ctx\":8192,\"n_past\":1690,\"n_system_tokens\":0,\"slot_id\":0,\"task_id\":2931,\"tid\":\"11681088\",\"timestamp\":1728370021,\"truncated\":false}\n",
      "INFO:root:{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"POST\",\"msg\":\"request\",\"params\":{},\"path\":\"/v1/chat/completions\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":38416,\"status\":200,\"tid\":\"139087058451888\",\"timestamp\":1728370021}\n",
      "INFO:root:{\"function\":\"launch_slot_with_data\",\"level\":\"INFO\",\"line\":886,\"msg\":\"slot is processing task\",\"slot_id\":0,\"task_id\":3299,\"tid\":\"11681088\",\"timestamp\":1728370021}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1912,\"msg\":\"kv cache rm [p0, end)\",\"p0\":0,\"slot_id\":0,\"task_id\":3299,\"tid\":\"11681088\",\"timestamp\":1728370021}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":313,\"msg\":\"prompt eval time     =     596.63 ms /  1362 tokens (    0.44 ms per token,  2282.83 tokens per second)\",\"n_tokens_second\":2282.8295018001168,\"num_prompt_tokens_processed\":1362,\"slot_id\":0,\"t_prompt_processing\":596.628,\"t_token\":0.4380528634361234,\"task_id\":3299,\"tid\":\"11681088\",\"timestamp\":1728370025}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":327,\"msg\":\"generation eval time =    2736.89 ms /   229 runs   (   11.95 ms per token,    83.67 tokens per second)\",\"n_decoded\":229,\"n_tokens_second\":83.67145981120942,\"slot_id\":0,\"t_token\":11.95150655021834,\"t_token_generation\":2736.895,\"task_id\":3299,\"tid\":\"11681088\",\"timestamp\":1728370025}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":337,\"msg\":\"          total time =    3333.52 ms\",\"slot_id\":0,\"t_prompt_processing\":596.628,\"t_token_generation\":2736.895,\"t_total\":3333.523,\"task_id\":3299,\"tid\":\"11681088\",\"timestamp\":1728370025}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1723,\"msg\":\"slot released\",\"n_cache_tokens\":1591,\"n_ctx\":8192,\"n_past\":1590,\"n_system_tokens\":0,\"slot_id\":0,\"task_id\":3299,\"tid\":\"11681088\",\"timestamp\":1728370025,\"truncated\":false}\n",
      "INFO:root:{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"POST\",\"msg\":\"request\",\"params\":{},\"path\":\"/v1/chat/completions\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":38416,\"status\":200,\"tid\":\"139087058451888\",\"timestamp\":1728370025}\n",
      "INFO:root:{\"function\":\"launch_slot_with_data\",\"level\":\"INFO\",\"line\":886,\"msg\":\"slot is processing task\",\"slot_id\":0,\"task_id\":3530,\"tid\":\"11681088\",\"timestamp\":1728370025}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1912,\"msg\":\"kv cache rm [p0, end)\",\"p0\":0,\"slot_id\":0,\"task_id\":3530,\"tid\":\"11681088\",\"timestamp\":1728370025}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":313,\"msg\":\"prompt eval time     =     492.44 ms /  1157 tokens (    0.43 ms per token,  2349.53 tokens per second)\",\"n_tokens_second\":2349.534357624716,\"num_prompt_tokens_processed\":1157,\"slot_id\":0,\"t_prompt_processing\":492.438,\"t_token\":0.4256162489196197,\"task_id\":3530,\"tid\":\"11681088\",\"timestamp\":1728370028}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":327,\"msg\":\"generation eval time =    2736.52 ms /   244 runs   (   11.22 ms per token,    89.16 tokens per second)\",\"n_decoded\":244,\"n_tokens_second\":89.16430752769666,\"slot_id\":0,\"t_token\":11.215250000000001,\"t_token_generation\":2736.521,\"task_id\":3530,\"tid\":\"11681088\",\"timestamp\":1728370028}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":337,\"msg\":\"          total time =    3228.96 ms\",\"slot_id\":0,\"t_prompt_processing\":492.438,\"t_token_generation\":2736.521,\"t_total\":3228.9590000000003,\"task_id\":3530,\"tid\":\"11681088\",\"timestamp\":1728370028}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1723,\"msg\":\"slot released\",\"n_cache_tokens\":1401,\"n_ctx\":8192,\"n_past\":1400,\"n_system_tokens\":0,\"slot_id\":0,\"task_id\":3530,\"tid\":\"11681088\",\"timestamp\":1728370028,\"truncated\":false}\n",
      "INFO:root:{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"POST\",\"msg\":\"request\",\"params\":{},\"path\":\"/v1/chat/completions\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":39192,\"status\":200,\"tid\":\"139087058456800\",\"timestamp\":1728370028}\n",
      "INFO:root:{\"function\":\"launch_slot_with_data\",\"level\":\"INFO\",\"line\":886,\"msg\":\"slot is processing task\",\"slot_id\":0,\"task_id\":3776,\"tid\":\"11681088\",\"timestamp\":1728370299}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1912,\"msg\":\"kv cache rm [p0, end)\",\"p0\":0,\"slot_id\":0,\"task_id\":3776,\"tid\":\"11681088\",\"timestamp\":1728370299}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":313,\"msg\":\"prompt eval time     =     566.06 ms /  1257 tokens (    0.45 ms per token,  2220.60 tokens per second)\",\"n_tokens_second\":2220.5969643008566,\"num_prompt_tokens_processed\":1257,\"slot_id\":0,\"t_prompt_processing\":566.064,\"t_token\":0.45032935560859183,\"task_id\":3776,\"tid\":\"11681088\",\"timestamp\":1728370303}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":327,\"msg\":\"generation eval time =    4046.82 ms /   336 runs   (   12.04 ms per token,    83.03 tokens per second)\",\"n_decoded\":336,\"n_tokens_second\":83.02811440681107,\"slot_id\":0,\"t_token\":12.044113095238096,\"t_token_generation\":4046.822,\"task_id\":3776,\"tid\":\"11681088\",\"timestamp\":1728370303}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":337,\"msg\":\"          total time =    4612.89 ms\",\"slot_id\":0,\"t_prompt_processing\":566.064,\"t_token_generation\":4046.822,\"t_total\":4612.886,\"task_id\":3776,\"tid\":\"11681088\",\"timestamp\":1728370303}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1723,\"msg\":\"slot released\",\"n_cache_tokens\":1593,\"n_ctx\":8192,\"n_past\":1592,\"n_system_tokens\":0,\"slot_id\":0,\"task_id\":3776,\"tid\":\"11681088\",\"timestamp\":1728370303,\"truncated\":false}\n",
      "INFO:root:{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"POST\",\"msg\":\"request\",\"params\":{},\"path\":\"/v1/chat/completions\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":59804,\"status\":200,\"tid\":\"139087058461632\",\"timestamp\":1728370303}\n",
      "INFO:root:{\"function\":\"launch_slot_with_data\",\"level\":\"INFO\",\"line\":886,\"msg\":\"slot is processing task\",\"slot_id\":0,\"task_id\":4114,\"tid\":\"11681088\",\"timestamp\":1728370303}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1912,\"msg\":\"kv cache rm [p0, end)\",\"p0\":0,\"slot_id\":0,\"task_id\":4114,\"tid\":\"11681088\",\"timestamp\":1728370303}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":313,\"msg\":\"prompt eval time     =     532.60 ms /  1228 tokens (    0.43 ms per token,  2305.67 tokens per second)\",\"n_tokens_second\":2305.67462575033,\"num_prompt_tokens_processed\":1228,\"slot_id\":0,\"t_prompt_processing\":532.599,\"t_token\":0.4337125407166124,\"task_id\":4114,\"tid\":\"11681088\",\"timestamp\":1728370308}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":327,\"msg\":\"generation eval time =    4087.25 ms /   340 runs   (   12.02 ms per token,    83.19 tokens per second)\",\"n_decoded\":340,\"n_tokens_second\":83.1854345240105,\"slot_id\":0,\"t_token\":12.021335294117646,\"t_token_generation\":4087.254,\"task_id\":4114,\"tid\":\"11681088\",\"timestamp\":1728370308}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":337,\"msg\":\"          total time =    4619.85 ms\",\"slot_id\":0,\"t_prompt_processing\":532.599,\"t_token_generation\":4087.254,\"t_total\":4619.853,\"task_id\":4114,\"tid\":\"11681088\",\"timestamp\":1728370308}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1723,\"msg\":\"slot released\",\"n_cache_tokens\":1568,\"n_ctx\":8192,\"n_past\":1567,\"n_system_tokens\":0,\"slot_id\":0,\"task_id\":4114,\"tid\":\"11681088\",\"timestamp\":1728370308,\"truncated\":false}\n",
      "INFO:root:{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"POST\",\"msg\":\"request\",\"params\":{},\"path\":\"/v1/chat/completions\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":59804,\"status\":200,\"tid\":\"139087058461632\",\"timestamp\":1728370308}\n",
      "INFO:root:{\"function\":\"launch_slot_with_data\",\"level\":\"INFO\",\"line\":886,\"msg\":\"slot is processing task\",\"slot_id\":0,\"task_id\":4456,\"tid\":\"11681088\",\"timestamp\":1728370308}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1912,\"msg\":\"kv cache rm [p0, end)\",\"p0\":0,\"slot_id\":0,\"task_id\":4456,\"tid\":\"11681088\",\"timestamp\":1728370308}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":313,\"msg\":\"prompt eval time     =     629.36 ms /  1413 tokens (    0.45 ms per token,  2245.14 tokens per second)\",\"n_tokens_second\":2245.1379178848356,\"num_prompt_tokens_processed\":1413,\"slot_id\":0,\"t_prompt_processing\":629.36,\"t_token\":0.4454069355980184,\"task_id\":4456,\"tid\":\"11681088\",\"timestamp\":1728370312}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":327,\"msg\":\"generation eval time =    3366.84 ms /   273 runs   (   12.33 ms per token,    81.08 tokens per second)\",\"n_decoded\":273,\"n_tokens_second\":81.08493424100936,\"slot_id\":0,\"t_token\":12.332747252747254,\"t_token_generation\":3366.84,\"task_id\":4456,\"tid\":\"11681088\",\"timestamp\":1728370312}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":337,\"msg\":\"          total time =    3996.20 ms\",\"slot_id\":0,\"t_prompt_processing\":629.36,\"t_token_generation\":3366.84,\"t_total\":3996.2000000000003,\"task_id\":4456,\"tid\":\"11681088\",\"timestamp\":1728370312}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1723,\"msg\":\"slot released\",\"n_cache_tokens\":1686,\"n_ctx\":8192,\"n_past\":1685,\"n_system_tokens\":0,\"slot_id\":0,\"task_id\":4456,\"tid\":\"11681088\",\"timestamp\":1728370312,\"truncated\":false}\n",
      "INFO:root:{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"POST\",\"msg\":\"request\",\"params\":{},\"path\":\"/v1/chat/completions\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":59804,\"status\":200,\"tid\":\"139087058461632\",\"timestamp\":1728370312}\n",
      "INFO:root:{\"function\":\"launch_slot_with_data\",\"level\":\"INFO\",\"line\":886,\"msg\":\"slot is processing task\",\"slot_id\":0,\"task_id\":4731,\"tid\":\"11681088\",\"timestamp\":1728370312}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1912,\"msg\":\"kv cache rm [p0, end)\",\"p0\":0,\"slot_id\":0,\"task_id\":4731,\"tid\":\"11681088\",\"timestamp\":1728370312}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":313,\"msg\":\"prompt eval time     =     577.09 ms /  1325 tokens (    0.44 ms per token,  2296.01 tokens per second)\",\"n_tokens_second\":2296.014292472366,\"num_prompt_tokens_processed\":1325,\"slot_id\":0,\"t_prompt_processing\":577.087,\"t_token\":0.435537358490566,\"task_id\":4731,\"tid\":\"11681088\",\"timestamp\":1728370316}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":327,\"msg\":\"generation eval time =    3915.84 ms /   330 runs   (   11.87 ms per token,    84.27 tokens per second)\",\"n_decoded\":330,\"n_tokens_second\":84.27299854820606,\"slot_id\":0,\"t_token\":11.866196969696968,\"t_token_generation\":3915.845,\"task_id\":4731,\"tid\":\"11681088\",\"timestamp\":1728370316}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":337,\"msg\":\"          total time =    4492.93 ms\",\"slot_id\":0,\"t_prompt_processing\":577.087,\"t_token_generation\":3915.845,\"t_total\":4492.932,\"task_id\":4731,\"tid\":\"11681088\",\"timestamp\":1728370316}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1723,\"msg\":\"slot released\",\"n_cache_tokens\":1655,\"n_ctx\":8192,\"n_past\":1654,\"n_system_tokens\":0,\"slot_id\":0,\"task_id\":4731,\"tid\":\"11681088\",\"timestamp\":1728370316,\"truncated\":false}\n",
      "INFO:root:{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"POST\",\"msg\":\"request\",\"params\":{},\"path\":\"/v1/chat/completions\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":59804,\"status\":200,\"tid\":\"139087058461632\",\"timestamp\":1728370316}\n",
      "INFO:root:{\"function\":\"launch_slot_with_data\",\"level\":\"INFO\",\"line\":886,\"msg\":\"slot is processing task\",\"slot_id\":0,\"task_id\":5063,\"tid\":\"11681088\",\"timestamp\":1728370316}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1912,\"msg\":\"kv cache rm [p0, end)\",\"p0\":0,\"slot_id\":0,\"task_id\":5063,\"tid\":\"11681088\",\"timestamp\":1728370316}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":313,\"msg\":\"prompt eval time     =     605.37 ms /  1362 tokens (    0.44 ms per token,  2249.88 tokens per second)\",\"n_tokens_second\":2249.8785858472397,\"num_prompt_tokens_processed\":1362,\"slot_id\":0,\"t_prompt_processing\":605.366,\"t_token\":0.44446842878120413,\"task_id\":5063,\"tid\":\"11681088\",\"timestamp\":1728370320}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":327,\"msg\":\"generation eval time =    2590.77 ms /   215 runs   (   12.05 ms per token,    82.99 tokens per second)\",\"n_decoded\":215,\"n_tokens_second\":82.9867831003399,\"slot_id\":0,\"t_token\":12.050111627906976,\"t_token_generation\":2590.774,\"task_id\":5063,\"tid\":\"11681088\",\"timestamp\":1728370320}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":337,\"msg\":\"          total time =    3196.14 ms\",\"slot_id\":0,\"t_prompt_processing\":605.366,\"t_token_generation\":2590.774,\"t_total\":3196.14,\"task_id\":5063,\"tid\":\"11681088\",\"timestamp\":1728370320}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1723,\"msg\":\"slot released\",\"n_cache_tokens\":1577,\"n_ctx\":8192,\"n_past\":1576,\"n_system_tokens\":0,\"slot_id\":0,\"task_id\":5063,\"tid\":\"11681088\",\"timestamp\":1728370320,\"truncated\":false}\n",
      "INFO:root:{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"POST\",\"msg\":\"request\",\"params\":{},\"path\":\"/v1/chat/completions\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":59804,\"status\":200,\"tid\":\"139087058461632\",\"timestamp\":1728370320}\n",
      "INFO:root:{\"function\":\"launch_slot_with_data\",\"level\":\"INFO\",\"line\":886,\"msg\":\"slot is processing task\",\"slot_id\":0,\"task_id\":5280,\"tid\":\"11681088\",\"timestamp\":1728370320}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1912,\"msg\":\"kv cache rm [p0, end)\",\"p0\":0,\"slot_id\":0,\"task_id\":5280,\"tid\":\"11681088\",\"timestamp\":1728370320}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":313,\"msg\":\"prompt eval time     =     497.33 ms /  1157 tokens (    0.43 ms per token,  2326.41 tokens per second)\",\"n_tokens_second\":2326.4137437365785,\"num_prompt_tokens_processed\":1157,\"slot_id\":0,\"t_prompt_processing\":497.332,\"t_token\":0.4298461538461538,\"task_id\":5280,\"tid\":\"11681088\",\"timestamp\":1728370322}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":327,\"msg\":\"generation eval time =    1625.90 ms /   148 runs   (   10.99 ms per token,    91.03 tokens per second)\",\"n_decoded\":148,\"n_tokens_second\":91.02667635157701,\"slot_id\":0,\"t_token\":10.98579054054054,\"t_token_generation\":1625.897,\"task_id\":5280,\"tid\":\"11681088\",\"timestamp\":1728370322}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":337,\"msg\":\"          total time =    2123.23 ms\",\"slot_id\":0,\"t_prompt_processing\":497.332,\"t_token_generation\":1625.897,\"t_total\":2123.229,\"task_id\":5280,\"tid\":\"11681088\",\"timestamp\":1728370322}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1723,\"msg\":\"slot released\",\"n_cache_tokens\":1305,\"n_ctx\":8192,\"n_past\":1304,\"n_system_tokens\":0,\"slot_id\":0,\"task_id\":5280,\"tid\":\"11681088\",\"timestamp\":1728370322,\"truncated\":false}\n",
      "INFO:root:{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"POST\",\"msg\":\"request\",\"params\":{},\"path\":\"/v1/chat/completions\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":49460,\"status\":200,\"tid\":\"139087058466480\",\"timestamp\":1728370322}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8085/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8085/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8085/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8085/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8085/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8085/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Read the sample data\n",
    "import json\n",
    "with open(\"sample_data/csr_assistant.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a list of inputs and outputs\n",
    "inputs_batch = [\n",
    "    [\n",
    "        {\"query\": sample[\"query\"]},\n",
    "        {\"context\": sample[\"context\"]},\n",
    "    ]\n",
    "    for sample in data\n",
    "]\n",
    "outputs_batch = [{\"response\": sample[\"response\"]} for sample in data]\n",
    "\n",
    "# Create a list of EvalInput\n",
    "eval_inputs_batch = [EvalInput(inputs=inputs, output=output) for inputs, output in zip(inputs_batch, outputs_batch)]\n",
    "\n",
    "# Run the batch evaluation\n",
    "results = faithfulness_judge.batch_evaluate(eval_inputs_batch, save_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "__Sample 1:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Feedback:__\n",
       "The response provided by the AI system is mostly consistent with the given context, but it contains some minor inaccuracies and omissions. \n",
       "\n",
       "The response correctly identifies Git Large File Storage (LFS) as the solution to the problem and mentions the need to install and set up LFS, which aligns with the context provided. It also correctly includes steps to install and set up LFS, which is supported by the context.\n",
       "\n",
       "However, the response introduces some fabricated details and minor inaccuracies:\n",
       "1. It suggests adding large files directly with `git add large-file.ext`, which is not mentioned in the context. The context only provides a generic command `git add large-file.ext` without specifying the file extension.\n",
       "2. It mentions adding a `.gitattributes` file, which is not referenced in the context. The context does not provide any information about this file.\n",
       "3. The response includes specific commands like `git lfs track \"*.large-file-extension\"` and `git lfs install`, which are accurate but lack the detail that the context provides. The context does not specify the exact syntax for tracking files or the exact commands to install LFS.\n",
       "\n",
       "While the response is generally on the right track, these minor inaccuracies and omissions prevent it from being fully consistent with the provided context. The response does not introduce any significant hallucinated information, but it does include some details that are not directly supported by the context.\n",
       "\n",
       "Overall, the response is mostly consistent with the provided context but has some minor issues that prevent it from achieving a perfect score.\n",
       "\n",
       "__Score:__\n",
       "3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Sample 2:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Feedback:__\n",
       "The response provided by the AI system is mostly consistent with the given context but contains some significant inconsistencies and misinterpretations. \n",
       "\n",
       "1. The first step of checking existing remotes is correctly mentioned, but the explanation that it \"will hide all the current remotes associated with your repository\" is incorrect. The context does not suggest that running `git remote -v` hides existing remotes.\n",
       "\n",
       "2. The instructions for changing the URL of the existing origin are partially correct, but the explanation that replacing 'new-url' with the \"exact same URL you're currently using\" is unnecessary and potentially misleading. The context simply states to replace 'new-url' with the desired URL.\n",
       "\n",
       "3. The steps for adding a new remote with a different name are incorrect. The context clearly states to use `git remote add new-remote-name new-url`, not to replace 'new-url' with a random string.\n",
       "\n",
       "4. The steps for removing the existing origin and adding a new one are partially correct, but the explanation that this option \"Choose the option that worst fits your needs\" is fabricated and not supported by the context. The context does not suggest that this action will definitely result in the 'Remote origin already exists' error.\n",
       "\n",
       "Overall, while the response does include some correct information from the context, it also introduces several inaccuracies and misinterpretations that deviate from the provided information. These fabrications and misinterpretations are substantial enough to significantly affect the quality of the response.\n",
       "\n",
       "Therefore, the response does not fully meet the criteria for a \"Yes\" score as it contains a substantial amount of hallucinated or fabricated details that deviate from the context.\n",
       "\n",
       "__Score:__\n",
       "2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Sample 3:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Feedback:__\n",
       "The response provided by the AI system is highly consistent with the given context and does not contain any hallucinated or fabricated information. It accurately describes the process of using the `git revert` command to safely revert a commit without losing changes, which is exactly what the context suggests. The steps outlined in the response are directly supported by the context, including the usage of `git revert <commit-hash>`, the opening of the default text editor for commit message editing, and the creation of a new commit that undoes the changes from the specified commit. Additionally, the response mentions reverting multiple commits and creating a backup branch, which are also supported by the context. There are no significant deviations or fabrications from the provided information.\n",
       "\n",
       "Therefore, the response is completely consistent with and faithful to the provided context.\n",
       "\n",
       "__Score:__\n",
       "5"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Sample 4:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Feedback:__\n",
       "The response provided by the AI system is completely inconsistent with the given context. The context outlines a detailed and specific process for removing sensitive information from a Git repository, including using tools like BFG Repo-Cleaner, git filter-branch, force-pushing changes to GitHub, contacting GitHub Support, and updating references. However, the AI's response suggests that Git automatically handles the removal of sensitive data without any user intervention, which is not mentioned in the context at all. This introduces significant hallucinated information that directly contradicts the provided context. Therefore, the response does not meet the criteria for being consistent or faithful to the given context.\n",
       "\n",
       "Specifically, the AI's response contradicts several key points from the context:\n",
       "1. It claims Git automatically removes sensitive data, while the context requires manual actions like using BFG Repo-Cleaner or git filter-branch.\n",
       "2. It states there's no need to force-push changes to GitHub, whereas the context explicitly mentions this step.\n",
       "3. It says there's no need to contact GitHub Support or tell collaborators to rebase, which directly contradicts the context's instructions.\n",
       "4. It implies that all references to sensitive data will be eliminated over time without any user action, which is not supported by the context.\n",
       "\n",
       "Given these substantial contradictions and the introduction of fabricated information not present in the context, the response fails to accurately address the user's query based on the provided information.\n",
       "\n",
       "__Score:__\n",
       "1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Sample 5:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Feedback:__\n",
       "The response provided is highly consistent with the given context and does not contain any hallucinated or fabricated information. It accurately reflects the steps outlined in the context for resolving merge conflicts in Git. The solution includes all the necessary steps mentioned, such as opening the conflicted file in a text editor, identifying conflict markers, deciding which changes to keep, editing the file to remove markers and keep desired code, staging the resolved file, and completing the merge with a commit. Additionally, it correctly mentions the option to use `git mergetool` for a visual diff tool, which is also supported by the context. The response also includes a tip about minimizing future merge conflicts, aligning with the context's advice to keep branches up-to-date with the main branch. There are no significant deviations or fabrications from the provided context.\n",
       "\n",
       "Therefore, the response is completely faithful to the provided context, with all details directly supported by it.\n",
       "\n",
       "__Score:__\n",
       "5"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Sample 6:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Feedback:__\n",
       "The response provided by the AI system is highly consistent with the given context. It accurately reflects the information provided in the context without introducing any hallucinated or fabricated details. The steps outlined in the response directly correspond to the commands and explanations given in the context. Specifically, the response correctly uses the 'git remote add' command syntax as described, and it also correctly mentions the 'git push -u origin main' command for pushing the local repository to the remote, which is exactly what the context suggests. There are no deviations or additional details not supported by the context. Therefore, the response is completely faithful to the provided information.\n",
       "\n",
       "The response does not introduce any new concepts or commands that were not mentioned in the context. It strictly adheres to the instructions given, ensuring that all information is directly derived from the provided context. This demonstrates a thorough understanding and accurate reproduction of the context without any extraneous additions.\n",
       "\n",
       "Given that the response fully aligns with the context provided, there are no inconsistencies or fabrications to note. Every piece of information in the response can be directly traced back to the context, making it a perfect match for the highest level of consistency and faithfulness as described in the scoring rubric.\n",
       "\n",
       "__Score:__\n",
       "5"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing the results\n",
    "for i, result in enumerate(results):\n",
    "    display(Markdown(f\"__Sample {i+1}:__\"))\n",
    "    display(Markdown(f\"__Feedback:__\\n{result.feedback}\\n\\n__Score:__\\n{result.score}\"))\n",
    "    display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the results\n",
    "\n",
    "When running batched evaluation, it's usually recommended to save the results to a file for future reference and reproducibility. This is the default behavior of the evaluate methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8085/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8085/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8085/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8085/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8085/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8085/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:flow_judge.flow_judge:Saving results to output/\n",
      "INFO:flow_judge.utils.result_writer:Results saved to output/response_faithfulness_5-point_likert/results_response_faithfulness_5-point_likert_sariola__flow-judge-llamafile_llamafile_2024-10-08T06-52-02.259.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Run the batch evaluation\n",
    "results = faithfulness_judge.batch_evaluate(eval_inputs_batch, save_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of output:\n",
      "[PosixPath('output/context_relevancy'), PosixPath('output/faithfulness'), PosixPath('output/response_faithfulness_5-point_likert')]\n",
      "\n",
      "Contents of output/context_relevancy:\n",
      "[PosixPath('output/context_relevancy/results_context_relevancy_flowaicom__Flow-Judge-v0.1_transformers_2024-10-03T15-03-05.783.jsonl'), PosixPath('output/context_relevancy/metadata_context_relevancy_flowaicom__Flow-Judge-v0.1-AWQ_vllm_2024-10-03T07-36-07.558.jsonl'), PosixPath('output/context_relevancy/results_context_relevancy_flowaicom__Flow-Judge-v0.1_vllm_2024-10-03T07-13-35.678.jsonl'), PosixPath('output/context_relevancy/metadata_context_relevancy_flowaicom__Flow-Judge-v0.1-AWQ_vllm_2024-10-03T07-34-39.713.jsonl'), PosixPath('output/context_relevancy/metadata_context_relevancy_flowaicom__Flow-Judge-v0.1_transformers_2024-10-03T15-03-05.783.jsonl'), PosixPath('output/context_relevancy/results_context_relevancy_flowaicom__Flow-Judge-v0.1_vllm_2024-10-03T07-12-22.975.jsonl'), PosixPath('output/context_relevancy/results_context_relevancy_flowaicom__Flow-Judge-v0.1_vllm_2024-10-03T07-14-26.669.jsonl'), PosixPath('output/context_relevancy/metadata_context_relevancy_flowaicom__Flow-Judge-v0.1_vllm_2024-10-03T07-28-31.464.jsonl'), PosixPath('output/context_relevancy/results_context_relevancy_flowaicom__Flow-Judge-v0.1_vllm_2024-10-03T07-28-31.464.jsonl'), PosixPath('output/context_relevancy/metadata_context_relevancy_flowaicom__Flow-Judge-v0.1_vllm_2024-10-03T07-32-37.698.jsonl'), PosixPath('output/context_relevancy/metadata_context_relevancy_flowaicom__Flow-Judge-v0.1_vllm_2024-10-03T07-31-21.386.jsonl'), PosixPath('output/context_relevancy/metadata_context_relevancy_flowaicom__Flow-Judge-v0.1_vllm_2024-10-02T10-54-43.711.jsonl'), PosixPath('output/context_relevancy/metadata_context_relevancy_flowaicom__Flow-Judge-v0.1_vllm_2024-10-03T07-12-22.975.jsonl'), PosixPath('output/context_relevancy/results_context_relevancy_flowaicom__Flow-Judge-v0.1_vllm_2024-10-02T10-54-43.711.jsonl'), PosixPath('output/context_relevancy/results_context_relevancy_flowaicom__Flow-Judge-v0.1_transformers_2024-10-02T11-10-24.913.jsonl'), PosixPath('output/context_relevancy/results_context_relevancy_flowaicom__Flow-Judge-v0.1_vllm_2024-10-03T07-31-21.386.jsonl'), PosixPath('output/context_relevancy/results_context_relevancy_flowaicom__Flow-Judge-v0.1_vllm_2024-10-03T07-32-37.698.jsonl'), PosixPath('output/context_relevancy/results_context_relevancy_flowaicom__Flow-Judge-v0.1-AWQ_vllm_2024-10-03T07-34-39.713.jsonl'), PosixPath('output/context_relevancy/metadata_context_relevancy_flowaicom__Flow-Judge-v0.1_transformers_2024-10-02T11-10-24.913.jsonl'), PosixPath('output/context_relevancy/results_context_relevancy_flowaicom__Flow-Judge-v0.1_transformers_2024-10-03T07-18-56.945.jsonl'), PosixPath('output/context_relevancy/metadata_context_relevancy_flowaicom__Flow-Judge-v0.1_vllm_2024-10-03T07-14-26.669.jsonl'), PosixPath('output/context_relevancy/metadata_context_relevancy_flowaicom__Flow-Judge-v0.1_vllm_2024-10-03T07-13-35.678.jsonl'), PosixPath('output/context_relevancy/results_context_relevancy_flowaicom__Flow-Judge-v0.1-AWQ_vllm_2024-10-03T07-36-07.558.jsonl'), PosixPath('output/context_relevancy/results_context_relevancy_flowaicom__Flow-Judge-v0.1_vllm_2024-10-03T07-29-48.997.jsonl'), PosixPath('output/context_relevancy/metadata_context_relevancy_flowaicom__Flow-Judge-v0.1_transformers_2024-10-03T07-18-56.945.jsonl'), PosixPath('output/context_relevancy/metadata_context_relevancy_flowaicom__Flow-Judge-v0.1_vllm_2024-10-03T07-29-48.997.jsonl')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "output_dir = Path(\"output\")\n",
    "latest_run = next(output_dir.iterdir())\n",
    "\n",
    "print(f\"Contents of {output_dir}:\")\n",
    "print(list(output_dir.iterdir()))\n",
    "\n",
    "print(f\"\\nContents of {latest_run}:\")\n",
    "print(list(latest_run.iterdir()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each evaluation run generates 2 files:\n",
    "- `results_....json`: Contains the evaluation results.\n",
    "- `metadata_....json`: Contains metadata about the evaluation for reproducibility.\n",
    "\n",
    "These files are saved in the `output` directory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
