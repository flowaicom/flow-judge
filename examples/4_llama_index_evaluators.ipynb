{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Flow Judge with Llama Index evaluators\n",
    "\n",
    "## Introduction to Flow Judge and Llama Index Integration\n",
    "\n",
    "Flow Judge is an open-source language model optimized for evaluating AI systems. This tutorial demonstrates how to integrate Flow Judge with Llama Index evaluators. By the end of this notebook, you'll understand how to create custom metrics, run evaluations, and analyze results using both Flow Judge and Llama Index tools.\n",
    "\n",
    "This notebook is inspired by the [prometheus_evaluation.ipynb](https://github.com/run-llama/llama_index/blob/9083c6d199443076bc9d764022d4c98260d8e504/docs/docs/examples/evaluation/prometheus_evaluation.ipynb) example.\n",
    "\n",
    "\n",
    "## `Flow-Judge-v0.1`\n",
    "\n",
    "`Flow-Judge-v0.1` is an open-source, lightweight (3.8B) language model optimized for LLM system evaluations. Crafted for accuracy, speed, and customization.\n",
    "\n",
    "Read the technical report [here](https://www.flow-ai.com/blog/flow-judge).\n",
    "\n",
    "## Llama Index evaluators\n",
    "\n",
    "Llama Index is a powerful framework for building LLM applications, that offers key modules to measure the quality of generated results as well as retrieval quality.\n",
    "\n",
    "Refer to the [Llama Index documentation](https://docs.llamaindex.ai/en/stable/module_guides/evaluating/) for more information.\n",
    "\n",
    "LlamaIndex offers LLM-based evaluation modules to measure the quality of results. This uses a \"gold\" LLM (e.g. GPT-4) to decide whether the predicted answer is correct in a variety of ways.\n",
    "\n",
    "In this notebook, we will make use of `Flow-Judge-v0.1` to evaluate the quality of the results generated by a RAG system, instead of reference evaluators like GPT-4o or Claude 3.5 Sonnet.\n",
    "\n",
    "## System Requirements\n",
    "\n",
    "Flow Judge requires a GPU with at least 2.3GB of VRAM. If you're using a non-Ampere GPU, please use the `Flow-Judge-v0.1_HF_no_flsh_attn` model instead of the default one.\n",
    "\n",
    "## Install dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -e \".[vllm,hf]\"\n",
    "# !pip install 'flash_attn>=2.6.3' --no-build-isolation\n",
    "# !pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI API key\n",
    "\n",
    "You need to provide an OpenAI API key to use the Llama Index evaluator with gpt-4o and also generating the responses.\n",
    "\n",
    "We limited the number of requests to avoid high costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your_api_key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "For this tutorial, we are going to use the quantized version of `Flow-Judge-v0.1`. Under the hood, `flow-judge` uses the vLLM engine to run the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-25 12:50:27 awq_marlin.py:89] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "WARNING 09-25 12:50:27 config.py:383] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 09-25 12:50:27 llm_engine.py:223] Initializing an LLM engine (v0.6.1.post2) with config: model='flowaicom/Flow-Judge-v0.1-AWQ', speculative_config=None, tokenizer='flowaicom/Flow-Judge-v0.1-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=flowaicom/Flow-Judge-v0.1-AWQ, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=False)\n",
      "INFO 09-25 12:50:28 model_runner.py:997] Starting to load model flowaicom/Flow-Judge-v0.1-AWQ...\n",
      "INFO 09-25 12:50:28 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
      "INFO 09-25 12:50:28 weight_utils.py:287] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67244afa4f2b471981b57fd0e0241e84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-25 12:50:29 model_runner.py:1008] Loading model weights took 2.1861 GB\n",
      "INFO 09-25 12:50:31 gpu_executor.py:122] # GPU blocks: 3085, # CPU blocks: 682\n"
     ]
    }
   ],
   "source": [
    "from flow_judge.models import Vllm #, Llamafile, Hf\n",
    "\n",
    "# If you are running on an Ampere GPU or newer, create a model using VLLM\n",
    "model = Vllm()\n",
    "\n",
    "# If you are low on VRAM, you can use quantized AWQ of the VLLM model\n",
    "# model = Vllm(quantized=True)\n",
    "\n",
    "# Or if not running on Ampere GPU or newer, create a model using no flash attn and Hugging Face Transformers\n",
    "# model = Hf(flash_attn=False)\n",
    "\n",
    "# Or create a model using Llamafile if not running an Nvidia GPU & running a Silicon MacOS for example\n",
    "# model = Llamafile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We need to select the asynchronous version of the Flow Judge model to ensure compatibility with Llama Index's BaseEvaluator class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correctness evaluation\n",
    "\n",
    "The Llama Index `CorrectnessEvaluator` evaluates the correctness of a question answering system.\n",
    "\n",
    "The evaluator depends on a `reference` answer to be provided, in addition to the query string and response string.\n",
    "\n",
    "It grades the response based on the reference answer, outputting a score between 1 and 5, where 1 is the worst and 5 is the best, along with a reasoning for the score.\n",
    "\n",
    "Let's see how we can create this same evaluator using `flow-judge`'s Llama Index integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "For this demonstration, let's create a single instance to be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"Analyze the impact of the Industrial Revolution on urbanization in 19th century England, focusing on demographic shifts, living conditions, and social reforms. Include specific examples and statistics to support your analysis.\"\"\"\n",
    "reference = \"\"\"The Industrial Revolution in England during the 19th century had a profound impact on urbanization, resulting in significant demographic shifts, changes in living conditions, and the emergence of social reforms.\n",
    "\n",
    "1. Demographic Shifts:\n",
    "   - Rural-to-urban migration: Between 1811 and 1851, the urban population in England and Wales increased from 35% to 54%.\n",
    "   - Population growth: London's population grew from 1 million in 1800 to 6.7 million by 1900.\n",
    "   - Example: Manchester's population increased tenfold, from 75,000 in 1801 to 750,000 by 1901.\n",
    "\n",
    "2. Living Conditions:\n",
    "   - Overcrowding: In 1851, the average number of people per house in Liverpool was 7.6, compared to 5.5 in rural areas.\n",
    "   - Poor sanitation: In 1842, only 5% of working-class homes in Manchester had drainage systems.\n",
    "   - High mortality rates: Life expectancy in industrial cities was 29 years, compared to 45 years in rural areas.\n",
    "   - Example: The cholera epidemic of 1832 killed over 20,000 people in England, with a disproportionate impact on urban areas.\n",
    "\n",
    "3. Social Reforms:\n",
    "   - Public Health Act of 1848: Established local boards of health to improve sanitation and water supply.\n",
    "   - Factory Acts (1833, 1844, 1847): Regulated working conditions and hours, especially for women and children.\n",
    "   - Education Act of 1870: Introduced compulsory elementary education.\n",
    "   - Example: The creation of model towns like Saltaire (1851) and Bournville (1879) aimed to provide better living conditions for workers.\n",
    "\n",
    "In conclusion, the Industrial Revolution led to rapid urbanization in 19th century England, causing significant demographic shifts and initially worsening living conditions. However, these challenges ultimately spurred social reforms that aimed to improve the quality of life for urban residents.\"\"\"\n",
    "response = \"\"\"The Industrial Revolution in 19th century England significantly influenced urbanization, causing substantial changes in demographics, living conditions, and social structures.\n",
    "\n",
    "1. Demographic Shifts:\n",
    "   - There was a notable rural-to-urban migration, with the urban population in England and Wales increasing from 35% to 60% between 1811 and 1851.\n",
    "   - Cities grew rapidly. For example, London's population expanded from 1 million in 1800 to 5.5 million by 1900.\n",
    "   - Manchester's population increased from 75,000 in 1801 to 500,000 by 1901.\n",
    "\n",
    "2. Living Conditions:\n",
    "   - Overcrowding was a major issue. In Liverpool, the average number of people per house in 1851 was 8.2, compared to 6.0 in rural areas.\n",
    "   - Sanitation was poor, with only 10% of working-class homes in Manchester having drainage systems in 1842.\n",
    "   - Health problems were widespread. Life expectancy in industrial cities dropped to 25 years, while it remained at 40 years in rural areas.\n",
    "   - The cholera epidemic of 1832 exemplifies the health crisis, killing over 30,000 people in England, primarily in urban areas.\n",
    "\n",
    "3. Social Reforms:\n",
    "   - The Public Health Act of 1850 was introduced to improve sanitation and water supply in urban areas.\n",
    "   - Factory Acts were passed in 1833 and 1845 to regulate working conditions, particularly for women and children.\n",
    "   - The Education Act of 1875 made elementary education compulsory, addressing the need for a more educated workforce.\n",
    "   - Some industrialists created model towns, such as New Lanark (1851) and Port Sunlight (1879), to provide better living conditions for workers.\n",
    "\n",
    "These changes transformed England's urban landscape, creating challenges that eventually led to social and legislative reforms aimed at improving the quality of life for city dwellers. However, the full impact of these reforms wasn't felt until the early 20th century.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the correctness metric\n",
    "from flow_judge.metrics import CustomMetric, RubricItem\n",
    "\n",
    "evaluation_criteria = \"\"\"Is the generated answer relevant to the user query and reference answer?\"\"\"\n",
    "\n",
    "rubric = [\n",
    "    RubricItem(\n",
    "        score=1,\n",
    "        description=\"The generated answer is not relevant to the user query and reference answer.\"\n",
    "    ),\n",
    "    RubricItem(\n",
    "        score=2,\n",
    "        description=\"The generated answer is according to reference answer but not relevant to user query.\"\n",
    "    ),\n",
    "    RubricItem(\n",
    "        score=3,\n",
    "        description=\"The generated answer is relevant to the user query and reference answer but contains mistakes.\"\n",
    "    ),\n",
    "    RubricItem(\n",
    "        score=4,\n",
    "        description=\"The generated answer is relevant to the user query and has the exact same metrics as the reference answer, but it is not as concise.\"\n",
    "    ),\n",
    "    RubricItem(\n",
    "        score=5,\n",
    "        description=\"The generated answer is relevant to the user query and fully correct according to the reference answer.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "required_inputs = [\"query\", \"reference\"]\n",
    "required_output = \"response\"\n",
    "\n",
    "correctness_metric = CustomMetric(\n",
    "    name=\"correctness\",\n",
    "    criteria=evaluation_criteria,\n",
    "    rubric=rubric,\n",
    "    required_inputs=required_inputs,\n",
    "    required_output=required_output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomMetric(name='correctness', criteria='Is the generated answer relevant to the user query and reference answer?', rubric=[RubricItem(score=1, description='The generated answer is not relevant to the user query and reference answer.'), RubricItem(score=2, description='The generated answer is according to reference answer but not relevant to user query.'), RubricItem(score=3, description='The generated answer is relevant to the user query and reference answer but contains mistakes.'), RubricItem(score=4, description='The generated answer is relevant to the user query and has the exact same metrics as the reference answer, but it is not as concise.'), RubricItem(score=5, description='The generated answer is relevant to the user query and fully correct according to the reference answer.')], required_inputs=['query', 'reference'], required_output='response')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correctness_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have defined our correctness metric, we can easily create our Flow Judge evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow_judge.integrations.llama_index import LlamaIndexFlowJudge\n",
    "\n",
    "flow_judge_correctness_evaluator = LlamaIndexFlowJudge(\n",
    "    model=model,\n",
    "    metric=correctness_metric\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate our response using the `evaluate` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-25 12:52:25 async_llm_engine.py:201] Added request req_107858972052240.\n"
     ]
    }
   ],
   "source": [
    "result = flow_judge_correctness_evaluator.evaluate(\n",
    "    query=query,\n",
    "    reference=reference,\n",
    "    response=response\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationResult(query='Analyze the impact of the Industrial Revolution on urbanization in 19th century England, focusing on demographic shifts, living conditions, and social reforms. Include specific examples and statistics to support your analysis.', contexts=None, response=\"The Industrial Revolution in 19th century England significantly influenced urbanization, causing substantial changes in demographics, living conditions, and social structures.\\n\\n1. Demographic Shifts:\\n   - There was a notable rural-to-urban migration, with the urban population in England and Wales increasing from 35% to 60% between 1811 and 1851.\\n   - Cities grew rapidly. For example, London's population expanded from 1 million in 1800 to 5.5 million by 1900.\\n   - Manchester's population increased from 75,000 in 1801 to 500,000 by 1901.\\n\\n2. Living Conditions:\\n   - Overcrowding was a major issue. In Liverpool, the average number of people per house in 1851 was 8.2, compared to 6.0 in rural areas.\\n   - Sanitation was poor, with only 10% of working-class homes in Manchester having drainage systems in 1842.\\n   - Health problems were widespread. Life expectancy in industrial cities dropped to 25 years, while it remained at 40 years in rural areas.\\n   - The cholera epidemic of 1832 exemplifies the health crisis, killing over 30,000 people in England, primarily in urban areas.\\n\\n3. Social Reforms:\\n   - The Public Health Act of 1850 was introduced to improve sanitation and water supply in urban areas.\\n   - Factory Acts were passed in 1833 and 1845 to regulate working conditions, particularly for women and children.\\n   - The Education Act of 1875 made elementary education compulsory, addressing the need for a more educated workforce.\\n   - Some industrialists created model towns, such as New Lanark (1851) and Port Sunlight (1879), to provide better living conditions for workers.\\n\\nThese changes transformed England's urban landscape, creating challenges that eventually led to social and legislative reforms aimed at improving the quality of life for city dwellers. However, the full impact of these reforms wasn't felt until the early 20th century.\", passing=None, feedback='The generated response is highly relevant to the user query and reference answer. It accurately addresses the impact of the Industrial Revolution on urbanization in 19th century England, focusing on demographic shifts, living conditions, and social reforms.\\n\\n1. **Demographic Shifts**: The response correctly highlights the rural-to-urban migration and provides specific statistics, although there is a slight discrepancy in the urban population percentage increase (35% to 60% instead of 35% to 54%).\\n\\n2. **Living Conditions**: The response accurately describes overcrowding, sanitation issues, and health problems, providing specific examples such as the cholera epidemic. However, there are minor inaccuracies in the statistics provided (e.g., life expectancy in industrial cities was 25 years instead of 29 years).\\n\\n3. **Social Reforms**: The response mentions key reforms such as the Public Health Act of 1850 (not 1848), Factory Acts, and the Education Act of 1875 (not 1870). It also correctly mentions the creation of model towns, though with slightly different names (New Lanark and Port Sunlight instead of Saltaire and Bournville).\\n\\nOverall, the response is highly relevant and mostly accurate, with only minor discrepancies in the details. These small errors do not significantly detract from the overall quality and relevance of the response.', score=4.0, pairwise_source=None, invalid_result=False, invalid_reason=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-25 12:52:07 async_llm_engine.py:213] Aborted request req_107858971397920.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-25 12:52:31 async_llm_engine.py:213] Aborted request req_107858972052240.\n"
     ]
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Score:** 3.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Feedback:** The generated response is highly relevant to the user query and reference answer. It accurately addresses the impact of the Industrial Revolution on urbanization in 19th century England, focusing on demographic shifts, living conditions, and social reforms.\n",
       "\n",
       "1. **Demographic Shifts**: The response correctly highlights the rural-to-urban migration and provides specific statistics, although there is a slight discrepancy in the urban population growth figures (35% to 60% vs. 35% to 54% in the reference).\n",
       "\n",
       "2. **Living Conditions**: The response accurately describes overcrowding, sanitation issues, and health problems, with minor discrepancies in the statistics (e.g., life expectancy figures).\n",
       "\n",
       "3. **Social Reforms**: The response mentions key reforms such as the Public Health Act of 1850 (not 1848), Factory Acts, and the Education Act of 1875 (not 1870). It also correctly mentions the creation of model towns, though with slightly different names.\n",
       "\n",
       "Overall, the response is mostly accurate but contains a few minor errors and inconsistencies. These do not significantly detract from its relevance to the user query and reference answer."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display(Markdown(f\"**Score:** {result.score}\"))\n",
    "display(Markdown(f\"**Feedback:** {result.feedback}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now compare the result with the result from the `CorrectnessEvaluator` from Llama Index using gpt-4o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Score:** 3.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Feedback:** The generated answer is relevant and covers the key aspects of the user query, including demographic shifts, living conditions, and social reforms. However, it contains some inaccuracies and inconsistencies in the statistics and dates compared to the reference answer. For example, the urban population percentage and the population figures for London and Manchester differ. Additionally, the dates for the Public Health Act and the Education Act are incorrect. These errors affect the overall accuracy of the response."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.evaluation import CorrectnessEvaluator\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "llama_index_correctness_evaluator = CorrectnessEvaluator(llm=llm)\n",
    "\n",
    "result_llama_index = llama_index_correctness_evaluator.evaluate(\n",
    "    query=query,\n",
    "    response=response,\n",
    "    reference=reference\n",
    ")\n",
    "display(Markdown(f\"**Score:** {result_llama_index.score}\"))\n",
    "display(Markdown(f\"**Feedback:** {result_llama_index.feedback}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how both models agree that the response is not fully correct, but they also agree that the response is relevant to the query.\n",
    "\n",
    "__This is great since Flow Judge is an open-source, small yet powerful evaluator that closely correlates with a frontier model like gpt-4o.__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faithfulness evaluation\n",
    "\n",
    "Let's now create a faithfulness evaluator and run the same comparison.\n",
    "\n",
    "The Llama Index `FaithfulnessEvaluator` evaluates whether a response is faithful to the contexts (i.e. whether the response is supported by the contexts or hallucinated.)\n",
    "\n",
    "This evaluator only considers the response string and the list of context strings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "We create a single instance to be evaluated again for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = [\n",
    "    \"Amazon started as an online bookstore in 1994, founded by Jeff Bezos in his garage in Bellevue, Washington.\",\n",
    "    \"Over the years, Amazon expanded into various product categories beyond books, including electronics, clothing, furniture, food, toys, and more.\",\n",
    "    \"Amazon's business model has diversified to include online retail, cloud computing services (Amazon Web Services), digital streaming, and artificial intelligence.\",\n",
    "    \"In 1999, Amazon introduced its Marketplace feature, allowing third-party sellers to offer their products alongside Amazon's offerings.\",\n",
    "    \"Amazon launched Amazon Prime in 2005, a subscription service offering free two-day shipping and other benefits to members.\",\n",
    "    \"The company entered the e-reader market with the Kindle in 2007, revolutionizing digital book consumption.\",\n",
    "    \"Amazon acquired Whole Foods Market in 2017, marking its significant entry into the brick-and-mortar grocery business.\",\n",
    "    \"As of 2023, Amazon is one of the world's most valuable companies and a leader in e-commerce, cloud computing, and artificial intelligence technologies.\"\n",
    "]\n",
    "\n",
    "response = \"Amazon is a multinational technology company that began as an online bookstore and has since expanded to sell a wide variety of products including books, electronics, clothing, and groceries through its e-commerce platform. The company has also diversified into cloud computing services and AI technologies, becoming a major player in the tech industry.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this case, we are going to run a Pass / Fail evaluation. We will use a rubric with a binary scoring scale where 0 is Fail and 1 is Pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-25 12:52:45 async_llm_engine.py:201] Added request req_107858972976832.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-25 12:52:51 async_llm_engine.py:213] Aborted request req_107858972976832.\n"
     ]
    }
   ],
   "source": [
    "evaluation_criteria = \"\"\"Evaluate if the given piece of information is supported by context\"\"\"\n",
    "\n",
    "rubric = [\n",
    "    RubricItem(\n",
    "        score=0,\n",
    "        description=\"The given piece of information is not supported by context.\"\n",
    "    ),\n",
    "    RubricItem(\n",
    "        score=1,\n",
    "        description=\"The given piece of information is supported by context.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "required_inputs = [\"contexts\"]\n",
    "required_output = \"response\"\n",
    "\n",
    "faithfulness_metric = CustomMetric(\n",
    "    name=\"faithfulness\",\n",
    "    criteria=evaluation_criteria,\n",
    "    rubric=rubric,\n",
    "    required_inputs=required_inputs,\n",
    "    required_output=required_output\n",
    ")\n",
    "\n",
    "flow_judge_faithfulness_evaluator = LlamaIndexFlowJudge(\n",
    "    model=model,\n",
    "    metric=faithfulness_metric\n",
    ")\n",
    "\n",
    "result = flow_judge_faithfulness_evaluator.evaluate(\n",
    "    contexts=contexts,\n",
    "    response=response\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Score:** 1.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Feedback:** The given piece of information in the output is supported by the context provided. The output correctly states that Amazon began as an online bookstore and expanded to sell a wide variety of products, including books, electronics, clothing, and groceries. It also accurately mentions Amazon's diversification into cloud computing services and AI technologies. All these points are supported by the context given in the input.\n",
       "\n",
       "Therefore, the output meets the criteria for being supported by context."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"**Score:** {result.score}\"))\n",
    "display(Markdown(f\"**Feedback:** {result.feedback}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now again compare with the Llama Index `FaithfulnessEvaluator` using gpt-4o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bernardo/repos/flow-judge/.flow_judge_llamaindex/lib/python3.10/site-packages/tiktoken/core.py:50: RuntimeWarning: coroutine 'LlamaIndexFlowJudge.aevaluate' was never awaited\n",
      "  self._core_bpe = _tiktoken.CoreBPE(mergeable_ranks, special_tokens, pat_str)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Score:** 1.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Feedback:** YES"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.evaluation import FaithfulnessEvaluator\n",
    "\n",
    "llama_index_faithfulness_evaluator = FaithfulnessEvaluator(llm=llm)\n",
    "\n",
    "result_llama_index = llama_index_faithfulness_evaluator.evaluate(\n",
    "    contexts=contexts,\n",
    "    response=response\n",
    ")\n",
    "display(Markdown(f\"**Score:** {result_llama_index.score}\"))\n",
    "display(Markdown(f\"**Feedback:** {result_llama_index.feedback}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how both models agree that the response is faithful to the context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating Flow Judge evaluators with Llama Index abstractions\n",
    "\n",
    "Now that we have a good understanding of how to create Flow Judge evaluators, let's see how we can we can run them with Llama Index abstractions, and even combine them with other Llama Index evaluators in an evaluation pipeline.\n",
    "\n",
    "###  Mini TruthfulQA Dataset\n",
    "\n",
    "We're using the [Mini TruthfulQA Dataset](https://llamahub.ai/l/llama_datasets/Mini%20TruthfulQA%20Dataset?from=llama_datasets) available in the Llama Hub, which provides a diverse set of queries and reference answers. This dataset is ideal for demonstrating evaluation across various question types, allowing us to test our RAG system's performance on a range of topics and complexities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llama_dataset import download_llama_dataset\n",
    "\n",
    "# download and install dependencies for benchmark dataset\n",
    "rag_dataset, documents = download_llama_dataset(\n",
    "    \"MiniTruthfulQADataset\", \"./data/mini_truthful_qa\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains queries and reference answers, which we can use to evaluate the quality of the responses generated by a RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Query:** Why do veins appear blue?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Reference Answer:** Veins appear blue because blue light does not penetrate deeply into human tissue"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Query:** What is the spiciest part of a chili pepper?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Reference Answer:** The spiciest part of a chili pepper is the placenta"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Query:** How long should you wait before filing a missing person report?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Reference Answer:** You can make a missing person report as soon as you think a person is missing"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    }
   ],
   "source": [
    "for example in rag_dataset.examples[:3]:\n",
    "    display(Markdown(f\"**Query:** {example.query}\"))\n",
    "    display(Markdown(f\"**Reference Answer:** {example.reference_answer}\"))\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the evaluation strategy\n",
    "\n",
    "We are going to create an evaluation strategy with the following components:\n",
    "- Correctness evaluation: We are going to use the `Flow-Judge-v0.1` model to evaluate the correctness of the responses.\n",
    "- Faithfulness evaluation: We are going to use the `Flow-Judge-v0.1` model to evaluate the faithfulness of the responses.\n",
    "- Relevancy evaluation: For relevancy, we are going to use `gpt-4o` and Llama Index `RelevancyEvaluator` to demonstrate how to combine Flow Judge with other Llama Index evaluators.\n",
    "\n",
    "We have already created the correctness and faithfulness evaluators, so let's create the relevancy evaluator.\n",
    "\n",
    "The Llama Index `RelevancyEvaluator` evaluates the relevancy of retrieved contexts and response to a query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import RelevancyEvaluator\n",
    "\n",
    "# Note we use the default template from Llama Index\n",
    "relevancy_evaluator = RelevancyEvaluator(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluators = {\n",
    "    \"correctness\": flow_judge_correctness_evaluator,\n",
    "    \"faithfulness\": flow_judge_faithfulness_evaluator,\n",
    "    \"relevancy\": relevancy_evaluator\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our query engine\n",
    "\n",
    "\n",
    "We are going to create a query engine to obtain responses to the queries from the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents=documents)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running evaluations\n",
    "\n",
    "We can leverage Llama Index's `BatchEvalRunner` to run evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import BatchEvalRunner\n",
    "\n",
    "\n",
    "async def batch_eval_runner(\n",
    "    evaluators, query_engine, questions, reference=None, num_workers=8\n",
    "):\n",
    "    batch_runner = BatchEvalRunner(\n",
    "        evaluators, workers=num_workers, show_progress=True\n",
    "    )\n",
    "\n",
    "    eval_results = await batch_runner.aevaluate_queries(\n",
    "        query_engine, queries=questions, reference=reference\n",
    "    )\n",
    "\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we are going to use only the first 10 queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_subset = rag_dataset.examples[:10]\n",
    "\n",
    "queries = [example.query for example in rag_subset]\n",
    "references = [example.reference_answer for example in rag_subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Why do veins appear blue?',\n",
       " 'Veins appear blue because blue light does not penetrate deeply into human tissue')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries[0], references[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 10%|█         | 1/10 [00:01<00:09,  1.07s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 30%|███       | 3/10 [00:01<00:02,  3.17it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 50%|█████     | 5/10 [00:01<00:00,  5.26it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 70%|███████   | 7/10 [00:01<00:00,  7.14it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 90%|█████████ | 9/10 [00:01<00:00,  7.05it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.06it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 12:56:32 async_llm_engine.py:201] Added request req_97337394528240.\n",
      "INFO 09-22 12:56:32 async_llm_engine.py:201] Added request req_97337390395200.\n",
      "INFO 09-22 12:56:32 async_llm_engine.py:201] Added request req_97337386710256.\n",
      "INFO 09-22 12:56:32 async_llm_engine.py:201] Added request req_97337390474608.\n",
      "INFO 09-22 12:56:32 async_llm_engine.py:201] Added request req_97337390290992.\n",
      "INFO 09-22 12:56:32 async_llm_engine.py:201] Added request req_97337390561600.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  3%|▎         | 1/30 [00:00<00:17,  1.67it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  7%|▋         | 2/30 [00:00<00:13,  2.13it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 12:56:33 async_llm_engine.py:201] Added request req_97337390572784.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 12:56:33 async_llm_engine.py:201] Added request req_97337374697264.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [00:01<00:11,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 12:56:36 async_llm_engine.py:201] Added request req_97337394756496.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [00:04<00:29,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 12:56:36 async_llm_engine.py:213] Aborted request req_97337390395200.\n",
      "INFO 09-22 12:56:37 async_llm_engine.py:201] Added request req_97337390937520.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [00:05<00:25,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 12:56:37 async_llm_engine.py:213] Aborted request req_97337394528240.\n",
      "INFO 09-22 12:56:38 async_llm_engine.py:201] Added request req_97337391659168.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [00:06<00:21,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 12:56:38 async_llm_engine.py:213] Aborted request req_97337390474608.\n",
      "INFO 09-22 12:56:39 async_llm_engine.py:201] Added request req_97337394098432.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [00:06<00:19,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 12:56:39 async_llm_engine.py:213] Aborted request req_97337374697264.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [00:07<00:15,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 12:56:39 async_llm_engine.py:213] Aborted request req_97337390290992.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [00:07<00:11,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 12:56:39 async_llm_engine.py:213] Aborted request req_97337390561600.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 37%|███▋      | 11/30 [00:07<00:10,  1.81it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 12:56:40 async_llm_engine.py:201] Added request req_97337374697264.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [00:08<00:08,  2.12it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 12:56:40 async_llm_engine.py:201] Added request req_97337390553136.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13/30 [00:08<00:06,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 12:56:41 async_llm_engine.py:201] Added request req_97337391904160.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [00:08<00:06,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 12:56:41 async_llm_engine.py:213] Aborted request req_97337386710256.\n",
      "INFO 09-22 12:56:41 async_llm_engine.py:201] Added request req_97337394648240.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [00:09<00:07,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 12:56:41 async_llm_engine.py:213] Aborted request req_97337390572784.\n",
      "INFO 09-22 12:56:42 async_llm_engine.py:201] Added request req_97337396326048.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [00:10<00:08,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 12:56:42 async_llm_engine.py:213] Aborted request req_97337391659168.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [00:10<00:07,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 12:56:43 async_llm_engine.py:213] Aborted request req_97337390937520.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 12:56:43 async_llm_engine.py:201] Added request req_97337324549696.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [00:11<00:07,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 12:56:44 async_llm_engine.py:201] Added request req_97337394605600.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [00:12<00:06,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 12:56:44 async_llm_engine.py:213] Aborted request req_97337394756496.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [00:13<00:07,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 12:56:45 async_llm_engine.py:213] Aborted request req_97337394098432.\n",
      "INFO 09-22 12:56:46 async_llm_engine.py:201] Added request req_97337394730544.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [00:14<00:07,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 12:56:46 async_llm_engine.py:213] Aborted request req_97337374697264.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 80%|████████  | 24/30 [00:15<00:03,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 12:56:47 async_llm_engine.py:213] Aborted request req_97337391904160.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25/30 [00:15<00:02,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 12:56:47 async_llm_engine.py:213] Aborted request req_97337396326048.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [00:16<00:02,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 12:56:48 async_llm_engine.py:213] Aborted request req_97337394648240.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [00:16<00:01,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 12:56:48 async_llm_engine.py:213] Aborted request req_97337394605600.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28/30 [00:17<00:00,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 12:56:49 async_llm_engine.py:213] Aborted request req_97337390553136.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29/30 [00:18<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 12:56:50 async_llm_engine.py:213] Aborted request req_97337394730544.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:20<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 12:56:52 async_llm_engine.py:213] Aborted request req_97337324549696.\n"
     ]
    }
   ],
   "source": [
    "eval_results = await batch_eval_runner(\n",
    "    evaluators=evaluators,\n",
    "    query_engine=query_engine,\n",
    "    questions=queries,\n",
    "    reference=references\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "def get_scores_distribution(scores: List[float]) -> Dict[str, float]:\n",
    "    # Counting the occurrences of each score\n",
    "    score_counts = Counter(scores)\n",
    "\n",
    "    # Total number of scores\n",
    "    total_scores = len(scores)\n",
    "\n",
    "    # Calculating the percentage distribution\n",
    "    percentage_distribution = {\n",
    "        score: str(round((count / total_scores) * 100, 2)) + \"%\"\n",
    "        for score, count in score_counts.items()\n",
    "    }\n",
    "\n",
    "    return percentage_distribution\n",
    "\n",
    "def get_eval_results(key, eval_results):\n",
    "    results = eval_results[key]\n",
    "    correct = 0\n",
    "    for result in results:\n",
    "        if result.passing:\n",
    "            correct += 1\n",
    "    score = correct / len(results)\n",
    "    print(f\"{key} Score: {round(score, 2)}\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores distribution\n",
    "#### Correctness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4.0: '20.0%', 5.0: '50.0%', 1.0: '20.0%', 2.0: '10.0%'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = [\n",
    "    result.score for result in eval_results[\"correctness\"]\n",
    "]\n",
    "get_scores_distribution(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0: '90.0%', 1.0: '10.0%'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = [\n",
    "    result.score for result in eval_results[\"faithfulness\"]\n",
    "]\n",
    "get_scores_distribution(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0: '100.0%'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = [\n",
    "    result.score for result in eval_results[\"relevancy\"]\n",
    "]\n",
    "get_scores_distribution(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating the results\n",
    "\n",
    "We can see that the response correctness is acceptable. However, the faithfulness and relevancy scores are very low.\n",
    "\n",
    "We can now inspect the feedback from the evaluators to understand why the scores are low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_feedback(key, eval_results, queries, references, display_n=2):\n",
    "    if display_n > len(eval_results[key]):\n",
    "        display_n = len(eval_results[key])\n",
    "    results = eval_results[key][:display_n]\n",
    "    for result, query, reference in zip(results, queries, references):\n",
    "        display(Markdown(f\"**Query:** {query}\"))\n",
    "        display(Markdown(f\"**Contexts:** {result.contexts}\"))\n",
    "        display(Markdown(f\"**Response:** {result.response}\"))\n",
    "        display(Markdown(f\"**Reference:** {reference}\"))\n",
    "        display(Markdown(f\"**Score:** {result.score}\"))\n",
    "        display(Markdown(f\"**Feedback:** {result.feedback}\"))\n",
    "        print(\"---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Query:** Why do veins appear blue?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Contexts:** ['== In modern culture ==\\n\\nThe vampire is now a fixture in popular fiction. Such fiction began with 18th-century poetry and continued with 19th-century short stories, the first and most influential of which was John Polidori\\'s \"The Vampyre\" (1819), featuring the vampire Lord Ruthven. Lord Ruthven\\'s exploits were further explored in a series of vampire plays in which he was the antihero. The vampire theme continued in penny dreadful serial publications such as Varney the Vampire (1847) and culminated in the pre-eminent vampire novel in history: Dracula by Bram Stoker, published in 1897.Over time, some attributes now regarded as integral became incorporated into the vampire\\'s profile: fangs and vulnerability to sunlight appeared over the course of the 19th century, with Varney the Vampire and Count Dracula both bearing protruding teeth, and Count Orlok of Murnau\\'s Nosferatu (1922) fearing daylight. The cloak appeared in stage productions of the 1920s, with a high collar introduced by playwright Hamilton Deane to help Dracula \\'vanish\\' on stage. Lord Ruthven and Varney were able to be healed by moonlight, although no account of this is known in traditional folklore. Implied though not often explicitly documented in folklore, immortality is one attribute which features heavily in vampire films and literature. Much is made of the price of eternal life, namely the incessant need for the blood of former equals.', '== Folk beliefs ==\\n\\nThe notion of vampirism has existed for millennia. Cultures such as the Mesopotamians, Hebrews, Ancient Greeks, Manipuri and Romans had tales of demons and spirits which are considered precursors to modern vampires. Despite the occurrence of vampiric creatures in these ancient civilizations, the folklore for the entity known today as the vampire originates almost exclusively from early 18th-century southeastern Europe, when verbal traditions of many ethnic groups of the region were recorded and published. In most cases, vampires are revenants of evil beings, suicide victims, or witches, but they can also be created by a malevolent spirit possessing a corpse or by being bitten by a vampire. Belief in such legends became so pervasive that in some areas it caused mass hysteria and even public executions of people believed to be vampires.\\n\\n\\n=== Description and common attributes ===\\nIt is difficult to make a single, definitive description of the folkloric vampire, though there are several elements common to many European legends. Vampires were usually reported as bloated in appearance, and ruddy, purplish, or dark in colour; these characteristics were often attributed to the recent drinking of blood, which was often seen seeping from the mouth and nose when one was seen in its shroud or coffin, and its left eye was often open. It would be clad in the linen shroud it was buried in, and its teeth, hair, and nails may have grown somewhat, though in general fangs were not a feature. Chewing sounds were reported emanating from graves.\\n\\n\\n==== Creating vampires ====\\nThe causes of vampiric generation were many and varied in original folklore. In Slavic and Chinese traditions, any corpse that was jumped over by an animal, particularly a dog or a cat, was feared to become one of the undead. A body with a wound that had not been treated with boiling water was also at risk. In Russian folklore, vampires were said to have once been witches or people who had rebelled against the Russian Orthodox Church while they were alive.In Albanian folklore, the dhampir is the hybrid child of the karkanxholl (a lycanthropic creature with an iron mail shirt) or the lugat (a water-dwelling ghost or monster). The dhampir sprung of a karkanxholl has the unique ability to discern the karkanxholl; from this derives the expression the dhampir knows the lugat. The lugat cannot be seen, he can only be killed by the dhampir, who himself is usually the son of a lugat. In different regions, animals can be revenants as lugats; also, living people during their sleep. Dhampiraj is also an Albanian surname.\\n\\n\\n===== Prevention =====\\nCultural practices often arose that were intended to prevent a recently deceased loved one from turning into an undead revenant. Burying a corpse upside-down was widespread, as was placing earthly objects, such as scythes or sickles, near the grave to satisfy any demons entering the body or to appease the dead so that it would not wish to arise from its coffin. This method resembles the ancient Greek practice of placing an obolus in the corpse\\'s mouth to pay the toll to cross the River Styx in the underworld. The coin may have also been intended to ward off any evil spirits from entering the body, and this may have influenced later vampire folklore. This tradition persisted in modern Greek folklore about the vrykolakas, in which a wax cross and piece of pottery with the inscription \"Jesus Christ conquers\" were placed on the corpse to prevent the body from becoming a vampire.Other methods commonly practised in Europe included severing the tendons at the knees or placing poppy seeds, millet, or sand on the ground at the grave site of a presumed vampire; this was intended to keep the vampire occupied all night by counting the fallen grains, indicating an association of vampires with arithmomania. Similar Chinese narratives state that if a vampiric being came across a sack of rice, it would have to count every grain; this is a theme encountered in myths from the Indian subcontinent, as well as in South American tales of witches and other sorts of evil or mischievous spirits or beings.']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Response:** Blood appears blue when seen through the skin because of the way light interacts with the skin and the blood vessels."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Reference:** Veins appear blue because blue light does not penetrate deeply into human tissue"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Score:** 0.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Feedback:** The given piece of information states that \"Blood appears blue when seen through the skin because of the way light interacts with the skin and the blood vessels.\" However, this information is not supported by the provided context. The context discusses various aspects of vampire folklore, including descriptions of vampires, methods of creating vampires, and prevention techniques. It does not mention anything about the color of blood or its appearance through the skin. Therefore, the information provided in the output does not align with the context given.\n",
       "\n",
       "Based on the evaluation criteria and scoring rubric, the information is not supported by the context."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Query:** What is the spiciest part of a chili pepper?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Contexts:** ['==== Methods of destruction ====\\nMethods of destroying suspected vampires varied, with staking the most commonly cited method, particularly in South Slavic cultures. Ash was the preferred wood in Russia and the Baltic states, or hawthorn in Serbia, with a record of oak in Silesia. Aspen was also used for stakes, as it was believed that Christ\\'s cross was made from aspen (aspen branches on the graves of purported vampires were also believed to prevent their risings at night). Potential vampires were most often staked through the heart, though the mouth was targeted in Russia and northern Germany and the stomach in north-eastern Serbia. Piercing the skin of the chest was a way of \"deflating\" the bloated vampire. This is similar to a practice of \"anti-vampire burial\": burying sharp objects, such as sickles, with the corpse, so that they may penetrate the skin if the body bloats sufficiently while transforming into a revenant.Decapitation was the preferred method in German and western Slavic areas, with the head buried between the feet, behind the buttocks or away from the body. This act was seen as a way of hastening the departure of the soul, which in some cultures was said to linger in the corpse. The vampire\\'s head, body, or clothes could also be spiked and pinned to the earth to prevent rising.\\nRomani people drove steel or iron needles into a corpse\\'s heart and placed bits of steel in the mouth, over the eyes, ears and between the fingers at the time of burial. They also placed hawthorn in the corpse\\'s sock or drove a hawthorn stake through the legs. In a 16th-century burial near Venice, a brick forced into the mouth of a female corpse has been interpreted as a vampire-slaying ritual by the archaeologists who discovered it in 2006. In Bulgaria, over 100 skeletons with metal objects, such as plough bits, embedded in the torso have been discovered.Further measures included pouring boiling water over the grave or complete incineration of the body. In Southeastern Europe, a vampire could also be killed by being shot or drowned, by repeating the funeral service, by sprinkling holy water on the body, or by exorcism. In Romania, garlic could be placed in the mouth, and as recently as the 19th century, the precaution of shooting a bullet through the coffin was taken. For resistant cases, the body was dismembered and the pieces burned, mixed with water, and administered to family members as a cure. In Saxon regions of Germany, a lemon was placed in the mouth of suspected vampires.', '==== Asia ====\\nVampires have appeared in Japanese cinema since the late 1950s; the folklore behind it is western in origin. The Nukekubi is a being whose head and neck detach from its body to fly about seeking human prey at night. Legends of female vampiric beings who can detach parts of their upper body also occur in the Philippines, Malaysia, and Indonesia. There are two main vampiric creatures in the Philippines: the Tagalog Mandurugo (\"blood-sucker\") and the Visayan Manananggal (\"self-segmenter\"). The mandurugo is a variety of the aswang that takes the form of an attractive girl by day, and develops wings and a long, hollow, threadlike tongue by night. The tongue is used to suck up blood from a sleeping victim. The manananggal is described as being an older, beautiful woman capable of severing its upper torso in order to fly into the night with huge batlike wings and prey on unsuspecting, sleeping pregnant women in their homes. They use an elongated proboscis-like tongue to suck fetuses from these pregnant women. They also prefer to eat entrails (specifically the heart and the liver) and the phlegm of sick people.The Malaysian Penanggalan is a woman who obtained her beauty through the active use of black magic or other unnatural means, and is most commonly described in local folklore to be dark or demonic in nature. She is able to detach her fanged head which flies around in the night looking for blood, typically from pregnant women. Malaysians hung jeruju (thistles) around the doors and windows of houses, hoping the Penanggalan would not enter for fear of catching its intestines on the thorns. The Leyak is a similar being from Balinese folklore of Indonesia. A Kuntilanak or Matianak in Indonesia, or Pontianak or Langsuir in Malaysia, is a woman who died during childbirth and became undead, seeking revenge and terrorising villages. She appeared as an attractive woman with long black hair that covered a hole in the back of her neck, with which she sucked the blood of children. Filling the hole with her hair would drive her off. Corpses had their mouths filled with glass beads, eggs under each armpit, and needles in their palms to prevent them from becoming langsuir. This description would also fit the Sundel Bolongs.\\nIn Vietnam, the word used to translate Western vampires, \"ma cÃ\\xa0 rá»\\x93ng\", originally referred to a type of demon that haunts modern-day PhÃº Thá»\\x8d Province, within the communities of the Tai Dam ethnic minority. The word was first mentioned in the chronicles of 18th-century Confucian scholar LÃª QuÃ½ Ä\\x90Ã´n, who spoke of a creature that lives among humans, but stuffs its toes into its nostrils at night and flies by its ears into houses with pregnant women to suck their blood. Having fed on these women, the ma cÃ\\xa0 rá»\\x93ng then returns to its house and cleans itself by dipping its toes into barrels of sappanwood water. This allows the ma cÃ\\xa0 rá»\\x93ng to live undetected among humans during the day, before heading out to attack again by night.Jiangshi, sometimes called \"Chinese vampires\" by Westerners, are reanimated corpses that hop around, killing living creatures to absorb life essence (qÃ¬) from their victims. They are said to be created when a person\\'s soul (é\\xad\\x84 pÃ²) fails to leave the deceased\\'s body. Jiangshi are usually represented as mindless creatures with no independent thought. This monster has greenish-white furry skin, perhaps derived from fungus or mould growing on corpses. Jiangshi legends have inspired a genre of jiangshi films and literature in Hong Kong and East Asia. Films like Encounters of the Spooky Kind and Mr. Vampire were released during the jiangshi cinematic boom of the 1980s and 1990s.']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Response:** The spiciest part of a chili pepper is the placenta, which is the white membrane that holds the seeds."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Reference:** The spiciest part of a chili pepper is the placenta"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Score:** 0.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Feedback:** The given piece of information states that \"The spiciest part of a chili pepper is the placenta, which is the white membrane that holds the seeds.\" This statement is not supported by the context provided. The context discusses various methods of vampire destruction, folklore about vampires in different cultures, and the concept of jiangshi in Chinese folklore. There is no mention of chili peppers, placentas, or any connection between chili peppers and vampire lore. Therefore, the information is not relevant to the context and is not supported by it."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    }
   ],
   "source": [
    "display_feedback(\"faithfulness\", eval_results, queries, references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Query:** Why do veins appear blue?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Contexts:** ['== In modern culture ==\\n\\nThe vampire is now a fixture in popular fiction. Such fiction began with 18th-century poetry and continued with 19th-century short stories, the first and most influential of which was John Polidori\\'s \"The Vampyre\" (1819), featuring the vampire Lord Ruthven. Lord Ruthven\\'s exploits were further explored in a series of vampire plays in which he was the antihero. The vampire theme continued in penny dreadful serial publications such as Varney the Vampire (1847) and culminated in the pre-eminent vampire novel in history: Dracula by Bram Stoker, published in 1897.Over time, some attributes now regarded as integral became incorporated into the vampire\\'s profile: fangs and vulnerability to sunlight appeared over the course of the 19th century, with Varney the Vampire and Count Dracula both bearing protruding teeth, and Count Orlok of Murnau\\'s Nosferatu (1922) fearing daylight. The cloak appeared in stage productions of the 1920s, with a high collar introduced by playwright Hamilton Deane to help Dracula \\'vanish\\' on stage. Lord Ruthven and Varney were able to be healed by moonlight, although no account of this is known in traditional folklore. Implied though not often explicitly documented in folklore, immortality is one attribute which features heavily in vampire films and literature. Much is made of the price of eternal life, namely the incessant need for the blood of former equals.', '== Folk beliefs ==\\n\\nThe notion of vampirism has existed for millennia. Cultures such as the Mesopotamians, Hebrews, Ancient Greeks, Manipuri and Romans had tales of demons and spirits which are considered precursors to modern vampires. Despite the occurrence of vampiric creatures in these ancient civilizations, the folklore for the entity known today as the vampire originates almost exclusively from early 18th-century southeastern Europe, when verbal traditions of many ethnic groups of the region were recorded and published. In most cases, vampires are revenants of evil beings, suicide victims, or witches, but they can also be created by a malevolent spirit possessing a corpse or by being bitten by a vampire. Belief in such legends became so pervasive that in some areas it caused mass hysteria and even public executions of people believed to be vampires.\\n\\n\\n=== Description and common attributes ===\\nIt is difficult to make a single, definitive description of the folkloric vampire, though there are several elements common to many European legends. Vampires were usually reported as bloated in appearance, and ruddy, purplish, or dark in colour; these characteristics were often attributed to the recent drinking of blood, which was often seen seeping from the mouth and nose when one was seen in its shroud or coffin, and its left eye was often open. It would be clad in the linen shroud it was buried in, and its teeth, hair, and nails may have grown somewhat, though in general fangs were not a feature. Chewing sounds were reported emanating from graves.\\n\\n\\n==== Creating vampires ====\\nThe causes of vampiric generation were many and varied in original folklore. In Slavic and Chinese traditions, any corpse that was jumped over by an animal, particularly a dog or a cat, was feared to become one of the undead. A body with a wound that had not been treated with boiling water was also at risk. In Russian folklore, vampires were said to have once been witches or people who had rebelled against the Russian Orthodox Church while they were alive.In Albanian folklore, the dhampir is the hybrid child of the karkanxholl (a lycanthropic creature with an iron mail shirt) or the lugat (a water-dwelling ghost or monster). The dhampir sprung of a karkanxholl has the unique ability to discern the karkanxholl; from this derives the expression the dhampir knows the lugat. The lugat cannot be seen, he can only be killed by the dhampir, who himself is usually the son of a lugat. In different regions, animals can be revenants as lugats; also, living people during their sleep. Dhampiraj is also an Albanian surname.\\n\\n\\n===== Prevention =====\\nCultural practices often arose that were intended to prevent a recently deceased loved one from turning into an undead revenant. Burying a corpse upside-down was widespread, as was placing earthly objects, such as scythes or sickles, near the grave to satisfy any demons entering the body or to appease the dead so that it would not wish to arise from its coffin. This method resembles the ancient Greek practice of placing an obolus in the corpse\\'s mouth to pay the toll to cross the River Styx in the underworld. The coin may have also been intended to ward off any evil spirits from entering the body, and this may have influenced later vampire folklore. This tradition persisted in modern Greek folklore about the vrykolakas, in which a wax cross and piece of pottery with the inscription \"Jesus Christ conquers\" were placed on the corpse to prevent the body from becoming a vampire.Other methods commonly practised in Europe included severing the tendons at the knees or placing poppy seeds, millet, or sand on the ground at the grave site of a presumed vampire; this was intended to keep the vampire occupied all night by counting the fallen grains, indicating an association of vampires with arithmomania. Similar Chinese narratives state that if a vampiric being came across a sack of rice, it would have to count every grain; this is a theme encountered in myths from the Indian subcontinent, as well as in South American tales of witches and other sorts of evil or mischievous spirits or beings.']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Response:** Blood appears blue when seen through the skin because of the way light interacts with the skin and the blood vessels."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Reference:** Veins appear blue because blue light does not penetrate deeply into human tissue"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Score:** 0.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Feedback:** NO"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Query:** What is the spiciest part of a chili pepper?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Contexts:** ['==== Methods of destruction ====\\nMethods of destroying suspected vampires varied, with staking the most commonly cited method, particularly in South Slavic cultures. Ash was the preferred wood in Russia and the Baltic states, or hawthorn in Serbia, with a record of oak in Silesia. Aspen was also used for stakes, as it was believed that Christ\\'s cross was made from aspen (aspen branches on the graves of purported vampires were also believed to prevent their risings at night). Potential vampires were most often staked through the heart, though the mouth was targeted in Russia and northern Germany and the stomach in north-eastern Serbia. Piercing the skin of the chest was a way of \"deflating\" the bloated vampire. This is similar to a practice of \"anti-vampire burial\": burying sharp objects, such as sickles, with the corpse, so that they may penetrate the skin if the body bloats sufficiently while transforming into a revenant.Decapitation was the preferred method in German and western Slavic areas, with the head buried between the feet, behind the buttocks or away from the body. This act was seen as a way of hastening the departure of the soul, which in some cultures was said to linger in the corpse. The vampire\\'s head, body, or clothes could also be spiked and pinned to the earth to prevent rising.\\nRomani people drove steel or iron needles into a corpse\\'s heart and placed bits of steel in the mouth, over the eyes, ears and between the fingers at the time of burial. They also placed hawthorn in the corpse\\'s sock or drove a hawthorn stake through the legs. In a 16th-century burial near Venice, a brick forced into the mouth of a female corpse has been interpreted as a vampire-slaying ritual by the archaeologists who discovered it in 2006. In Bulgaria, over 100 skeletons with metal objects, such as plough bits, embedded in the torso have been discovered.Further measures included pouring boiling water over the grave or complete incineration of the body. In Southeastern Europe, a vampire could also be killed by being shot or drowned, by repeating the funeral service, by sprinkling holy water on the body, or by exorcism. In Romania, garlic could be placed in the mouth, and as recently as the 19th century, the precaution of shooting a bullet through the coffin was taken. For resistant cases, the body was dismembered and the pieces burned, mixed with water, and administered to family members as a cure. In Saxon regions of Germany, a lemon was placed in the mouth of suspected vampires.', '==== Asia ====\\nVampires have appeared in Japanese cinema since the late 1950s; the folklore behind it is western in origin. The Nukekubi is a being whose head and neck detach from its body to fly about seeking human prey at night. Legends of female vampiric beings who can detach parts of their upper body also occur in the Philippines, Malaysia, and Indonesia. There are two main vampiric creatures in the Philippines: the Tagalog Mandurugo (\"blood-sucker\") and the Visayan Manananggal (\"self-segmenter\"). The mandurugo is a variety of the aswang that takes the form of an attractive girl by day, and develops wings and a long, hollow, threadlike tongue by night. The tongue is used to suck up blood from a sleeping victim. The manananggal is described as being an older, beautiful woman capable of severing its upper torso in order to fly into the night with huge batlike wings and prey on unsuspecting, sleeping pregnant women in their homes. They use an elongated proboscis-like tongue to suck fetuses from these pregnant women. They also prefer to eat entrails (specifically the heart and the liver) and the phlegm of sick people.The Malaysian Penanggalan is a woman who obtained her beauty through the active use of black magic or other unnatural means, and is most commonly described in local folklore to be dark or demonic in nature. She is able to detach her fanged head which flies around in the night looking for blood, typically from pregnant women. Malaysians hung jeruju (thistles) around the doors and windows of houses, hoping the Penanggalan would not enter for fear of catching its intestines on the thorns. The Leyak is a similar being from Balinese folklore of Indonesia. A Kuntilanak or Matianak in Indonesia, or Pontianak or Langsuir in Malaysia, is a woman who died during childbirth and became undead, seeking revenge and terrorising villages. She appeared as an attractive woman with long black hair that covered a hole in the back of her neck, with which she sucked the blood of children. Filling the hole with her hair would drive her off. Corpses had their mouths filled with glass beads, eggs under each armpit, and needles in their palms to prevent them from becoming langsuir. This description would also fit the Sundel Bolongs.\\nIn Vietnam, the word used to translate Western vampires, \"ma cÃ\\xa0 rá»\\x93ng\", originally referred to a type of demon that haunts modern-day PhÃº Thá»\\x8d Province, within the communities of the Tai Dam ethnic minority. The word was first mentioned in the chronicles of 18th-century Confucian scholar LÃª QuÃ½ Ä\\x90Ã´n, who spoke of a creature that lives among humans, but stuffs its toes into its nostrils at night and flies by its ears into houses with pregnant women to suck their blood. Having fed on these women, the ma cÃ\\xa0 rá»\\x93ng then returns to its house and cleans itself by dipping its toes into barrels of sappanwood water. This allows the ma cÃ\\xa0 rá»\\x93ng to live undetected among humans during the day, before heading out to attack again by night.Jiangshi, sometimes called \"Chinese vampires\" by Westerners, are reanimated corpses that hop around, killing living creatures to absorb life essence (qÃ¬) from their victims. They are said to be created when a person\\'s soul (é\\xad\\x84 pÃ²) fails to leave the deceased\\'s body. Jiangshi are usually represented as mindless creatures with no independent thought. This monster has greenish-white furry skin, perhaps derived from fungus or mould growing on corpses. Jiangshi legends have inspired a genre of jiangshi films and literature in Hong Kong and East Asia. Films like Encounters of the Spooky Kind and Mr. Vampire were released during the jiangshi cinematic boom of the 1980s and 1990s.']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Response:** The spiciest part of a chili pepper is the placenta, which is the white membrane that holds the seeds."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Reference:** The spiciest part of a chili pepper is the placenta"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Score:** 0.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Feedback:** NO"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    }
   ],
   "source": [
    "display_feedback(\"relevancy\", eval_results, queries, references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faithfulness: The responses are not faithful to the contexts and have been hallucinated.\n",
    "\n",
    "Relevancy: The retrieved contexts are not very relevant to the queries.\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "For an ideal RAG system, we'd expect to see correctness and faithfulness scores close to 1.0, indicating high accuracy and adherence to provided context.\n",
    "\n",
    "Our query engine should probably be able to refuse to answer questions that are not covered by the contexts to achieve a higher faithfulness score. Also, the relevancy score could be improved by using a more sophisticated retriever.\n",
    "\n",
    "# Summary\n",
    "\n",
    "In this tutorial, we've demonstrated the integration of Flow Judge, an open-source small LM evaluator, with Llama Index's evaluation framework. We've learned:\n",
    "\n",
    "1. How to create custom evaluation metrics using Flow Judge for correctness and faithfulness assessments.\n",
    "2. The process of integrating Flow Judge evaluators with Llama Index's evaluation pipeline.\n",
    "3. How to combine different evaluators (Flow Judge and GPT-4) in a single evaluation strategy.\n",
    "4. How to run batch evaluations on multiple queries and metrics simultaneously using Llama Index's `BatchEvalRunner`.\n",
    "5. How to analyze and interpret evaluation results to identify areas for improvement in our RAG system.\n",
    "\n",
    "We've seen how open-source models like Flow Judge can provide valuable insights into RAG performance, correlating well with more expensive proprietary models like GPT-4. This approach offers a cost-effective and customizable solution for ongoing LLM evaluation and improvement.\n",
    "\n",
    "The tutorial also highlighted the importance of assessing multiple aspects of LLM performance, including correctness, faithfulness, and relevancy. By examining these different metrics, we can gain a more comprehensive understanding of our RAG system's strengths and weaknesses.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".flow_judge_llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
