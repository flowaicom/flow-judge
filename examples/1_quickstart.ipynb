{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart\n",
    "\n",
    "This tutorial demonstrates how to use the `flow-judge` library to perform language model-based evaluations using Flow-Judge-v0.1 models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running an evaluation\n",
    "\n",
    "Running an evaluation is as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-02 20:13:36 importing.py:10] Triton not installed; certain GPU-related functions will not be available.\n",
      "WARNING 10-02 20:13:36 _custom_ops.py:18] Failed to import from vllm._C with ModuleNotFoundError(\"No module named 'vllm._C'\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to create vLLM model: GPU is not available.                     vLLM requires a GPU to run.                     Check https://docs.vllm.ai/en/latest/getting_started/installation.html                     for installation requirements.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mVLLMError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/projects/flowrite/flow-judge/flow_judge/models/model_factory.py:45\u001b[0m, in \u001b[0;36mModelFactory._create_vllm_model\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFlowJudgeVLLMModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneration_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m VLLMError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/projects/flowrite/flow-judge/flow_judge/models/vllm.py:18\u001b[0m, in \u001b[0;36mFlowJudgeVLLMModel.__init__\u001b[0;34m(self, model, generation_params, **vllm_kwargs)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m VLLMError(\n\u001b[1;32m     19\u001b[0m         status_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     20\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU is not available. \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124m        vLLM requires a GPU to run. \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124m        Check https://docs.vllm.ai/en/latest/getting_started/installation.html \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124m        for installation requirements.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m     )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m LLM(model\u001b[38;5;241m=\u001b[39mmodel, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mvllm_kwargs)\n",
      "\u001b[0;31mVLLMError\u001b[0m: GPU is not available.                     vLLM requires a GPU to run.                     Check https://docs.vllm.ai/en/latest/getting_started/installation.html                     for installation requirements.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Markdown, display\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Create a model using ModelFactory\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mModelFactory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFlow-Judge-v0.1-AWQ\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# ! Replace with \"Flow-Judge-v0.1_HF_no_flsh_attn\" if running on no Ampere GPUs\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Initialize the judge\u001b[39;00m\n\u001b[1;32m     10\u001b[0m faithfulness_judge \u001b[38;5;241m=\u001b[39m FlowJudge(\n\u001b[1;32m     11\u001b[0m     metric\u001b[38;5;241m=\u001b[39mRESPONSE_FAITHFULNESS_5POINT,\n\u001b[1;32m     12\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel\n\u001b[1;32m     13\u001b[0m )\n",
      "File \u001b[0;32m~/projects/flowrite/flow-judge/flow_judge/models/model_factory.py:28\u001b[0m, in \u001b[0;36mModelFactory.create_model\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ModelFactory\u001b[38;5;241m.\u001b[39m_create_transformers_model(model_config)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_config\u001b[38;5;241m.\u001b[39mmodel_type \u001b[38;5;241m==\u001b[39m ModelType\u001b[38;5;241m.\u001b[39mVLLM:\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mModelFactory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_vllm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_config\u001b[38;5;241m.\u001b[39mmodel_type \u001b[38;5;241m==\u001b[39m ModelType\u001b[38;5;241m.\u001b[39mVLLM_ASYNC:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ModelFactory\u001b[38;5;241m.\u001b[39m_create_vllm_async_model(model_config)\n",
      "File \u001b[0;32m~/projects/flowrite/flow-judge/flow_judge/models/model_factory.py:51\u001b[0m, in \u001b[0;36mModelFactory._create_vllm_model\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m FlowJudgeVLLMModel(\n\u001b[1;32m     46\u001b[0m         model\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmodel_id,\n\u001b[1;32m     47\u001b[0m         generation_params\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mgeneration_params,\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig\u001b[38;5;241m.\u001b[39mvllm_kwargs,\n\u001b[1;32m     49\u001b[0m     )\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m VLLMError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to create vLLM model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to create vLLM model: GPU is not available.                     vLLM requires a GPU to run.                     Check https://docs.vllm.ai/en/latest/getting_started/installation.html                     for installation requirements."
     ]
    }
   ],
   "source": [
    "from flow_judge.models.model_factory import ModelFactory\n",
    "from flow_judge.flow_judge import EvalInput, FlowJudge\n",
    "from flow_judge.metrics import RESPONSE_FAITHFULNESS_5POINT\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Create a model using ModelFactory\n",
    "model = ModelFactory.create_model(\"Flow-Judge-v0.1_HF_no_flsh_attn\") # ! Replace with \"Flow-Judge-v0.1_HF_no_flsh_attn\" if running on no Ampere GPUs\n",
    "\n",
    "# Initialize the judge\n",
    "faithfulness_judge = FlowJudge(\n",
    "    metric=RESPONSE_FAITHFULNESS_5POINT,\n",
    "    model=model\n",
    ")\n",
    "\n",
    "# Sample to evaluate\n",
    "query = \"\"\"Please read the technical issue that the user is facing and help me create a detailed solution based on the context provided.\"\"\"\n",
    "context = \"\"\"# Customer Issue:\n",
    "I'm having trouble when uploading a git lfs tracked file to my repo: (base)  bernardo@bernardo-desktop  ~/repos/lm-evaluation-harness  ↱ Flow-Judge-v0.1_evals  git push                                            \n",
    "batch response: This repository is over its data quota. Account responsible for LFS bandwidth should purchase more data packs to restore access.\n",
    "\n",
    "# Documentation:\n",
    "Configuring Git Large File Storage\n",
    "Once Git LFS is installed, you need to associate it with a large file in your repository.\n",
    "\n",
    "Platform navigation\n",
    "Mac\n",
    "Windows\n",
    "Linux\n",
    "If there are existing files in your repository that you'd like to use GitHub with, you need to first remove them from the repository and then add them to Git LFS locally. For more information, see \"Moving a file in your repository to Git Large File Storage.\"\n",
    "\n",
    "If there are referenced Git LFS files that did not upload successfully, you will receive an error message. For more information, see \"Resolving Git Large File Storage upload failures.\"\n",
    "\n",
    "Open Terminal.\n",
    "\n",
    "Change your current working directory to an existing repository you'd like to use with Git LFS.\n",
    "\n",
    "To associate a file type in your repository with Git LFS, enter git lfs track followed by the name of the file extension you want to automatically upload to Git LFS.\n",
    "\n",
    "For example, to associate a .psd file, enter the following command:\n",
    "\n",
    "$ git lfs track \"*.psd\"\n",
    "> Tracking \"*.psd\"\n",
    "Every file type you want to associate with Git LFS will need to be added with git lfs track. This command amends your repository's .gitattributes file and associates large files with Git LFS.\n",
    "\n",
    "Note: We strongly suggest that you commit your local .gitattributes file into your repository.\n",
    "\n",
    "Relying on a global .gitattributes file associated with Git LFS may cause conflicts when contributing to other Git projects.\n",
    "Including the .gitattributes file in the repository allows people creating forks or fresh clones to more easily collaborate using Git LFS.\n",
    "Including the .gitattributes file in the repository allows Git LFS objects to optionally be included in ZIP file and tarball archives.\n",
    "Add a file to the repository matching the extension you've associated:\n",
    "\n",
    "git add path/to/file.psd\n",
    "Commit the file and push it to GitHub:\n",
    "\n",
    "git commit -m \"add file.psd\"\n",
    "git push\n",
    "You should see some diagnostic information about your file upload:\n",
    "\n",
    "> Sending file.psd\n",
    "> 44.74 MB / 81.04 MB  55.21 % 14s\n",
    "> 64.74 MB / 81.04 MB  79.21 % 3s\"\"\"\n",
    "response = \"\"\"# Resolving Git LFS Data Quota Issue\n",
    "\n",
    "## Problem\n",
    "The user is encountering an error when trying to push Git LFS tracked files:\n",
    "```\n",
    "This repository is over its data quota. Account responsible for LFS bandwidth should purchase more data packs to restore access.\n",
    "```\n",
    "\n",
    "## Solution\n",
    "\n",
    "1. **Understand the issue**\n",
    "   - The error message indicates that the repository has exceeded its data quota for Git LFS (Large File Storage).\n",
    "   - This is likely due to the size or number of large files being tracked by Git LFS.\n",
    "\n",
    "2. **Check current LFS usage**\n",
    "   - Run the following command to see the current LFS objects:\n",
    "     ```\n",
    "     git lfs ls-files\n",
    "     ```\n",
    "   - This will show you which files are being tracked by Git LFS and their sizes.\n",
    "\n",
    "3. **Review Git LFS pricing and quotas**\n",
    "   - Visit GitHub's pricing page to understand the current LFS storage and bandwidth limits for your account type.\n",
    "   - Free accounts typically have lower limits compared to paid plans.\n",
    "\n",
    "4. **Options to resolve the issue**\n",
    "\n",
    "   a. **Purchase more data packs**\n",
    "      - If you need to keep all current LFS objects and continue using LFS extensively:\n",
    "        1. Go to your GitHub account settings.\n",
    "        2. Navigate to the Billing section.\n",
    "        3. Look for options to purchase additional data packs for Git LFS.\n",
    "\n",
    "   b. **Optimize repository**\n",
    "      - If purchasing more data is not an option, consider optimizing your repository:\n",
    "        1. Remove unnecessary large files from LFS tracking.\n",
    "        2. Use `.gitignore` to exclude large files that don't need version control.\n",
    "        3. Consider using alternative storage solutions for very large assets.\n",
    "\n",
    "   c. **Clean up LFS cache**\n",
    "      - Sometimes, cleaning up the LFS cache can help:\n",
    "        ```\n",
    "        git lfs prune\n",
    "        ```\n",
    "\n",
    "5. **Reconfigure Git LFS tracking**\n",
    "   - Review your `.gitattributes` file to ensure only necessary file types are tracked:\n",
    "     ```\n",
    "     git lfs track\n",
    "     ```\n",
    "   - Modify tracking as needed:\n",
    "     ```\n",
    "     git lfs track \"*.psd\"  # Track PSD files\n",
    "     git lfs untrack \"*.zip\"  # Stop tracking ZIP files\n",
    "     ```\n",
    "\n",
    "6. **Commit changes and retry push**\n",
    "   - After making necessary changes:\n",
    "     ```\n",
    "     git add .gitattributes\n",
    "     git commit -m \"Update Git LFS tracking\"\n",
    "     git push\n",
    "     ```\n",
    "\n",
    "7. **If issues persist**\n",
    "   - Contact GitHub support for further assistance.\n",
    "   - They may be able to provide more specific guidance based on your account and repository details.\n",
    "\n",
    "Remember to regularly monitor your Git LFS usage to avoid hitting quotas in the future. Consider setting up alerts or regularly checking your GitHub account's storage usage statistics.\"\"\"\n",
    "\n",
    "# Create an EvalInput\n",
    "# We want to evaluate the response to the customer issue based on the context and the user instructions\n",
    "eval_input = EvalInput(\n",
    "    inputs=[\n",
    "        {\"query\": query},\n",
    "        {\"context\": context},\n",
    "    ],\n",
    "    output={\"response\": response},\n",
    ")\n",
    "\n",
    "# Run the evaluation\n",
    "result = faithfulness_judge.evaluate(eval_input, save_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "__Feedback:__\n",
       "The response provided is mostly consistent with the context given, but it introduces some minor fabrications and assumptions that are not explicitly supported by the context.\n",
       "\n",
       "The response correctly identifies the problem of exceeding the Git LFS data quota and provides a general solution outline. It also includes some specific steps that are not directly mentioned in the context, such as running `git lfs ls-files` to check current LFS usage and reviewing Git LFS pricing and quotas. These additions are helpful but not directly derived from the given context.\n",
       "\n",
       "The response does accurately reflect some information from the context, such as the need to purchase more data packs if the user has a free account and wants to keep using LFS extensively. It also mentions optimizing the repository by removing unnecessary large files and using `.gitignore` to exclude large files, which aligns with the context's suggestion to \"remove them from the repository and then add them to Git LFS locally.\"\n",
       "\n",
       "However, the response introduces some assumptions and additional steps that are not explicitly mentioned in the context. For example, it suggests contacting GitHub support if issues persist, which is not mentioned in the provided context. It also includes a recommendation to regularly monitor Git LFS usage and set up alerts, which, while potentially useful, is not stated in the context.\n",
       "\n",
       "Overall, while the response is mostly consistent with the context and provides valuable information, it introduces some minor fabrications and assumptions that are not directly supported by the given context.\n",
       "\n",
       "__Score:__\n",
       "3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the result\n",
    "display(Markdown(f\"__Feedback:__\\n{result.feedback}\\n\\n__Score:__\\n{result.score}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "`flow-judge` support different model configurations. This refers to the library use for running inference with the models. We currently support:\n",
    "- vLLM (default)\n",
    "- Hugging Face\n",
    "\n",
    "You can check the available models and choose the one that best fits your needs. By default, we run inference with a quantized model using the vLLM engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Flow-Judge-v0.1-AWQ',\n",
       " 'Flow-Judge-v0.1',\n",
       " 'Flow-Judge-v0.1_HF',\n",
       " 'Flow-Judge-v0.1_HF_no_flsh_attn',\n",
       " 'Flow-Judge-v0.1-AWQ-Async']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flow_judge.models.model_configs import get_available_configs\n",
    "get_available_configs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "A judge is initialized with a metric and a model.\n",
    "\n",
    "We include some common metrics in the library, such as:\n",
    "- RESPONSE_FAITHFULNESS_3POINT\n",
    "- RESPONSE_FAITHFULNESS_5POINT\n",
    "- RESPONSE_COMPREHENSIVENESS_3POINT\n",
    "- RESPONSE_COMPREHENSIVENESS_5POINT\n",
    "\n",
    "But you can also implement your own metrics and use them with the judge.\n",
    "\n",
    "Note that metrics have required inputs and outputs as you can see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: Response Faithfulness (5-point Likert)\n",
      "Required inputs: query, context\n",
      "Required output: response\n"
     ]
    }
   ],
   "source": [
    "RESPONSE_FAITHFULNESS_5POINT.print_required_keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`flow-judge` checks under the hood if the keys match. This is important to ensure the right prompt is being formatted.\n",
    "\n",
    "When you define a custom metric, you should specify the required keys as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running batched evaluations\n",
    "\n",
    "The `FlowJudge` class also supports batch evaluation. This is useful when you want to evaluate multiple samples at once in Evaluation-Driven Development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:07<00:00,  1.21s/it, est. speed input: 1071.99 toks/s, output: 197.56 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# Read the sample data\n",
    "import json\n",
    "with open(\"sample_data/csr_assistant.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a list of inputs and outputs\n",
    "inputs_batch = [\n",
    "    [\n",
    "        {\"query\": sample[\"query\"]},\n",
    "        {\"context\": sample[\"context\"]},\n",
    "    ]\n",
    "    for sample in data\n",
    "]\n",
    "outputs_batch = [{\"response\": sample[\"response\"]} for sample in data]\n",
    "\n",
    "# Create a list of EvalInput\n",
    "eval_inputs_batch = [EvalInput(inputs=inputs, output=output) for inputs, output in zip(inputs_batch, outputs_batch)]\n",
    "                         \n",
    "# Run the batch evaluation\n",
    "results = faithfulness_judge.batch_evaluate(eval_inputs_batch, save_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "__Sample 1:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Feedback:__\n",
       "tags without any additional surrounding text.\n",
       "- Write the numeric score inside <score> tags, without any additional surrounding text and always after the feedback.\n",
       "\n",
       "Please accurately evaluate the task. Strictly adhere to the evaluation criteria and rubric. \n",
       " <feedback>\n",
       "The response provided is mostly consistent with the given context, but it introduces a few minor inconsistencies and fabrications. \n",
       "\n",
       "1. The response correctly mentions the use of Git Large File Storage (LFS) to resolve the issue, which aligns with the context provided.\n",
       "2. It accurately describes the steps to install and set up Git LFS, which is supported by the context.\n",
       "3. The response introduces a step to track large files using `git lfs track \"*.large-file-extension\"`, which is a plausible extension of the context but not explicitly mentioned. This is a minor fabrication.\n",
       "4. The response mentions adding a .gitattributes file and committing large files, which is consistent with the context but not explicitly stated.\n",
       "5. The response concludes with instructions to push changes, which is a standard Git operation but not specifically mentioned in the context.\n",
       "\n",
       "Overall, the response is mostly consistent with the context, with only minor and inconsequential inconsistencies or fabrications.\n",
       "\n",
       "__Score:__\n",
       "4"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Sample 2:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Feedback:__\n",
       "tags without any additional surrounding text.\n",
       "- Write the numeric score inside <score> tags, without any additional surrounding text and always after the feedback.\n",
       "\n",
       "Please accurately evaluate the task. Strictly adhere to the evaluation criteria and rubric. \n",
       " <feedback>\n",
       "The response provided is mostly consistent with the context but contains some significant inconsistencies and fabrications that deviate from the given information. \n",
       "\n",
       "1. The first step of checking existing remotes is correctly mentioned, but the explanation is misleading. The context does not state that this will \"hide all the current remotes,\" which is a fabrication.\n",
       "2. The second step about changing the URL of the existing origin is partially correct but includes a fabrication. The context does not mention replacing 'new-url' with the \"exact same URL you're currently using,\" which is incorrect.\n",
       "3. The third step about removing a remote with a different name is partially correct but includes a fabrication. The context does not suggest replacing 'new-remote-name' with the name of an existing remote or 'new-url' with any random string.\n",
       "4. The fourth step about removing the existing origin and adding a new one is correct but includes a fabrication. The context does not suggest choosing the option that \"worst fits your needs.\"\n",
       "\n",
       "Overall, while the response does include some correct information from the context, it also introduces several fabrications and misleading statements that significantly deviate from the provided information. Therefore, the response is mostly inconsistent with the context.\n",
       "\n",
       "__Score:__\n",
       "2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Sample 3:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Feedback:__\n",
       "The response provided is mostly consistent with the given context and does not contain significant hallucinated or fabricated information. The response accurately describes the `git revert` command and its usage, which is directly supported by the context. It also mentions the `git reset` command, although it does not elaborate on it as the context does. However, this is a minor omission since the primary focus of the response is on reverting commits using `git revert`.\n",
       "\n",
       "The response includes additional helpful information such as creating a backup branch before performing the operation, which, while not explicitly mentioned in the context, is a good practice that aligns with the advice to ensure safety when making significant changes to Git history.\n",
       "\n",
       "Overall, the response is informative and mostly faithful to the context, with only minor additions that do not contradict the provided information.\n",
       "\n",
       "__Score:__\n",
       "4"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Sample 4:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Feedback:__\n",
       "tags without any additional surrounding text.\n",
       "- Write the numeric score inside <score> tags, without any additional surrounding text and always after the feedback.\n",
       "\n",
       "Please accurately evaluate the task. Strictly adhere to the evaluation criteria and rubric. \n",
       " <feedback>\n",
       "The response provided is significantly inconsistent with the given context. The context outlines a series of specific steps to remove sensitive information from a Git repository, including using tools like BFG Repo-Cleaner or git filter-branch, force-pushing changes, contacting GitHub Support, advising collaborators to rebase, and running specific Git commands to remove old references.\n",
       "\n",
       "However, the generated response contradicts this information by stating that no action is needed and that Git will automatically handle the removal of sensitive data. This claim is not supported by the context and introduces substantial fabricated information. The response also incorrectly suggests that force-pushing, contacting GitHub Support, and other mentioned actions are unnecessary, which directly contradicts the context provided.\n",
       "\n",
       "Given the significant amount of hallucinated and fabricated information, the response fails to accurately reflect the context and provides misleading advice to the user.\n",
       "\n",
       "__Score:__\n",
       "1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Sample 5:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Feedback:__\n",
       "The response provided is mostly consistent with the given context and does not contain significant hallucinated or fabricated information. Here is a detailed breakdown:\n",
       "\n",
       "1. **Consistency with Context**: The response accurately follows the steps outlined in the context for resolving merge conflicts. It correctly mentions opening the conflicted file in a text editor, identifying conflict markers, deciding which changes to keep, editing the file, saving it, staging the resolved file, and committing the changes. These steps are directly supported by the context provided.\n",
       "\n",
       "2. **Additional Information**: The response includes a tip about minimizing merge conflicts in the future by keeping branches up-to-date with the main branch, which is not explicitly mentioned in the context but is a logical and relevant piece of advice. This addition, while not fabricated, is not part of the original context.\n",
       "\n",
       "3. **Minor Inconsistencies**: The response does not contain any significant hallucinated or fabricated information. The additional tip about keeping branches up-to-date is a minor addition that does not detract from the overall consistency with the provided context.\n",
       "\n",
       "Overall, the response is mostly consistent with the context, with only a minor addition that does not contradict the provided information.\n",
       "\n",
       "__Score:__\n",
       "4"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Sample 6:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Feedback:__\n",
       "tags without any additional surrounding text.\n",
       "- Write the numeric score inside <score> tags, without any additional surrounding text and always after the feedback.\n",
       "\n",
       "Please accurately evaluate the task. Strictly adhere to the evaluation criteria and rubric. \n",
       " <feedback>\n",
       "The response provided is highly consistent with the given context. It accurately reflects the information provided in the context about adding a remote repository to a local Git repository. The response correctly uses the 'git remote add' command syntax and provides an example that matches the context exactly. It also correctly mentions the 'git push -u origin main' command to push the local repository to the remote, which is in line with the context.\n",
       "\n",
       "There are no hallucinated or fabricated details in the response. Every piece of information provided is directly supported by the context. The response is clear, concise, and faithfully represents the technical instructions given in the context.\n",
       "\n",
       "Therefore, the response meets the highest standard of consistency and faithfulness to the provided context.\n",
       "\n",
       "__Score:__\n",
       "5"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing the results\n",
    "for i, result in enumerate(results):\n",
    "    display(Markdown(f\"__Sample {i+1}:__\"))\n",
    "    display(Markdown(f\"__Feedback:__\\n{result.feedback}\\n\\n__Score:__\\n{result.score}\"))\n",
    "    display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the results\n",
    "\n",
    "When running batched evaluation, it's usually recommended to save the results to a file for future reference and reproducibility. This is the default behavior of the evaluate methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:flow_judge.flow_judge:Saving results to output/\n",
      "INFO:flow_judge.utils.result_writer:Results saved to output/response_faithfulness_5-point_likert/results_response_faithfulness_5-point_likert_flowaicom__Flow-Judge-v0.1_transformers_2024-09-20T13-50-24.779.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Run the batch evaluation\n",
    "results = faithfulness_judge.batch_evaluate(eval_inputs_batch, save_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of output:\n",
      "[PosixPath('output/response_faithfulness_5-point_likert')]\n",
      "\n",
      "Contents of output/response_faithfulness_5-point_likert:\n",
      "[PosixPath('output/response_faithfulness_5-point_likert/results_response_faithfulness_5-point_likert_flowaicom__Flow-Judge-v0.1_transformers_2024-09-20T13-50-24.779.jsonl'), PosixPath('output/response_faithfulness_5-point_likert/metadata_response_faithfulness_5-point_likert_flowaicom__Flow-Judge-v0.1_transformers_2024-09-20T13-50-24.779.jsonl')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "output_dir = Path(\"output\")\n",
    "latest_run = next(output_dir.iterdir())\n",
    "\n",
    "print(f\"Contents of {output_dir}:\")\n",
    "print(list(output_dir.iterdir()))\n",
    "\n",
    "print(f\"\\nContents of {latest_run}:\")\n",
    "print(list(latest_run.iterdir()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each evaluation run generates 2 files:\n",
    "- `results_....json`: Contains the evaluation results.\n",
    "- `metadata_....json`: Contains metadata about the evaluation for reproducibility.\n",
    "\n",
    "These files are saved in the `output` directory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
