{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart\n",
    "\n",
    "This tutorial demonstrates how to use the `Baseten` model class in async mode to perform language model-based evaluations using Flow-Judge-v0.1 deployed model on Baseten. For detailed instructions on how to use Baseten, visit the [Baseten readme](https://github.com/flowaicom/flow-judge/blob/main/flow_judge/models/adapters/baseten/README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's instantiate the `Baseten` model class in async mode. The async implementation makes use of Baseten's async inference approach. See [here](https://docs.baseten.co/invoke/async).\n",
    "\n",
    "You can imagine this as *fire-and-forget* functionality. Completion requests are made to the deployed model, once data is processed and inference is complete, the output is sent to a predefined webhook. The webhook url is part of the original request. The `Flow-Judge` library then connects with the webhook and *listens* for a response. The library makes use of this approach to allow configurability for concurrent execution.\n",
    "\n",
    "Optionally Flow AI has deployed a webhook proxy that accepts this request signature and feeds-it-forward to the client. This can be found under the URL: \"https://proxy.flow-ai.dev\"\n",
    "\n",
    "### Pre-requisite\n",
    "\n",
    "1. Sign-up to [Baseten](https://www.baseten.co/)\n",
    "2. Generate a Baseten API Key from [here](https://app.baseten.co/settings/api_keys)\n",
    "3. Generate a Webhook secret from [here](https://app.baseten.co/settings/secrets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Requirements\n",
    "\n",
    "Set your `Baseten API key`, `Webhook secret` and `GPU` option in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"BASETEN_WEBHOOK_SECRET\"] = \"your_baseten_webhook_secret\"\n",
    "os.environ[\"BASETEN_API_KEY\"] = \"your_baseten_api_key\"\n",
    "\n",
    "# You can optionally switch the GPU to H100.\n",
    "# This will deploy the FlowJudge model on H100 40GB\n",
    "# A10G deployment is Flow-Judge-v0.1-AWQ\n",
    "# H100 deployment is Flow-Judge-v0.1-FP8\n",
    "# !! Manually changing the hardware on Baseten's UI may cause compatibility issues !!\n",
    "os.environ[\"BASETEN_GPU\"] = \"A10G\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the Baseten model\n",
    "\n",
    "Set the following required options for async execution mode of the Baseten model class: \n",
    "1. `exec_async=True`\n",
    "2. `webhook_proxy_url=https://proxy.flow-ai.dev` (or [run the proxy locally](https://github.com/flowaicom/flow-judge/blob/main/flow_judge/models/adapters/baseten/README.md))\n",
    "\n",
    "Optionally you can set the `async_batch_size` option to a value > 0 (defaults to `128`). This is the number of concurrent requests sent to the deployed model. It is associated with the concurrency goals you want to achieve and can be actively configured in Baseten's UI. For more information, see [here](https://docs.baseten.co/performance/concurrency). Our current deployment configuration allows a concurrency target of `128` and max replica of `1` for the deployed model as the default on Baseten. This means if you have max replica set to 1 on Baseten, it can accept concurrent requests of `128`. The batch size you set for the Baseten model class should be equivalent to the number of `concurrency_target * number_of_replicas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow_judge import Baseten, AsyncFlowJudge\n",
    "from flow_judge.metrics import RESPONSE_FAITHFULNESS_5POINT\n",
    "\n",
    "# Async model execution\n",
    "model = Baseten(\n",
    "    webhook_proxy_url=\"https://proxy.flow-ai.dev\",\n",
    "    exec_async=True,\n",
    ")\n",
    "\n",
    "# Instantiate the Async Judge with the model and a metric\n",
    "# The library includes multiple default metrics and you can implement your own.\n",
    "faithfulness_judge = AsyncFlowJudge(\n",
    "    metric=RESPONSE_FAITHFULNESS_5POINT,\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Evaluations\n",
    "\n",
    "Let's test batched evaluations with our example csr data on the faithfulness 5 point likert.\n",
    "\n",
    "We use the `async_batch_evaluate` method from the AsyncFlowJudge class. Underneath this uses batched processing utilizing the batch_size set with the `async_batch_size` argument of the Baseten model class. If there are failures, for example with networking, the batch will process and errors will be propagated as log outputs. The output would include the successful responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the sample data\n",
    "import json\n",
    "from flow_judge import EvalInput\n",
    "with open(\"sample_data/csr_assistant.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a list of inputs and outputs\n",
    "inputs_batch = [\n",
    "    [\n",
    "        {\"query\": sample[\"query\"]},\n",
    "        {\"context\": sample[\"context\"]},\n",
    "    ]\n",
    "    for sample in data\n",
    "]\n",
    "outputs_batch = [{\"response\": sample[\"response\"]} for sample in data]\n",
    "\n",
    "# Create a list of EvalInput\n",
    "eval_inputs_batch = [EvalInput(inputs=inputs, output=output) for inputs, output in zip(inputs_batch, outputs_batch)]\n",
    "\n",
    "# Run the batch evaluation\n",
    "results = await faithfulness_judge.async_batch_evaluate(eval_inputs_batch, save_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Visualizing the results\n",
    "for i, result in enumerate(results):\n",
    "    display(Markdown(f\"__Sample {i+1}:__\"))\n",
    "    display(Markdown(f\"__Feedback:__\\n{result.feedback}\\n\\n__Score:__\\n{result.score}\"))\n",
    "    display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly you can run a single evaluation task using the `async_evaluate` method on the `AsyncFlowJudge` class. Under the hood, this will process a single async request and attach listeners to the webhook for the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await faithfulness_judge.async_evaluate(eval_inputs_batch[0], save_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the result\n",
    "display(Markdown(f\"__Feedback:__\\n{result.feedback}\\n\\n__Score:__\\n{result.score}\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flow-judge-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
