model_name: "Flow-Judge-v0.1"
# Note: prefix_caching is not supported by vLLM for this model, please do not include enable_prefix_caching as part of the vllm config in config.yaml
environment_variables:
  {
    "CUDA_HOME": "/usr/local/cuda",
    "PATH": "/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin",
    "LD_LIBRARY_PATH": "/usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64",
    "VLLM_ATTENTION_BACKEND": "FLASH_ATTN",
  }
external_package_dirs: []
python_version: py311
base_image:
  image: baseten/truss-server-base:3.11-gpu-v0.9.0
model_metadata:
  example_model_input:
    {
      "messages":
        [
          {
            "role": "system",
            "content": "You are ChatGPT, an AI assistant. Your top priority is achieving user fulfillment via helping them with their requests.",
          },
          {
            "role": "user",
            "content": "Write a limerick about python exceptions",
          },
        ],
    }
  repo_id: flowaicom/Flow-Judge-v0.1-AWQ
  openai_compatible: true
  vllm_config:
    max_model_len: 8192
    tensor_parallel_size: 1
requirements:
  - vllm>=0.6.2
  - vllm-flash-attn
resources:
  accelerator: A10G
  use_gpu: true
runtime:
  predict_concurrency: 128
secrets:
  hf_access_token: hf_xyz
