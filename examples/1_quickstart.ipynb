{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart\n",
    "\n",
    "This tutorial demonstrates how to use the `flow-judge` library to perform language model-based evaluations using Flow-Judge-v0.1 models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running an evaluation\n",
    "\n",
    "Running an evaluation is as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b081e099b1147e4a333526adf3dfd71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from flow_judge.models import Llamafile\n",
    "\n",
    "from flow_judge.flow_judge import EvalInput, FlowJudge\n",
    "from flow_judge.metrics import RESPONSE_FAITHFULNESS_5POINT\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Create a model using ModelFactory\n",
    "# model = ModelFactory.create_model(\"Flow-Judge-v0.1-AWQ\") # ! Replace with \"Flow-Judge-v0.1_HF_no_flsh_attn\" if running on no Ampere GPUs\n",
    "model = Llamafile.load()\n",
    "\n",
    "# Initialize the judge\n",
    "faithfulness_judge = FlowJudge(\n",
    "    metric=RESPONSE_FAITHFULNESS_5POINT,\n",
    "    model=model\n",
    ")\n",
    "\n",
    "# Sample to evaluate\n",
    "query = \"\"\"Please read the technical issue that the user is facing and help me create a detailed solution based on the context provided.\"\"\"\n",
    "context = \"\"\"# Customer Issue:\n",
    "I'm having trouble when uploading a git lfs tracked file to my repo: (base)  bernardo@bernardo-desktop  ~/repos/lm-evaluation-harness  ↱ Flow-Judge-v0.1_evals  git push\n",
    "batch response: This repository is over its data quota. Account responsible for LFS bandwidth should purchase more data packs to restore access.\n",
    "\n",
    "# Documentation:\n",
    "Configuring Git Large File Storage\n",
    "Once Git LFS is installed, you need to associate it with a large file in your repository.\n",
    "\n",
    "Platform navigation\n",
    "Mac\n",
    "Windows\n",
    "Linux\n",
    "If there are existing files in your repository that you'd like to use GitHub with, you need to first remove them from the repository and then add them to Git LFS locally. For more information, see \"Moving a file in your repository to Git Large File Storage.\"\n",
    "\n",
    "If there are referenced Git LFS files that did not upload successfully, you will receive an error message. For more information, see \"Resolving Git Large File Storage upload failures.\"\n",
    "\n",
    "Open Terminal.\n",
    "\n",
    "Change your current working directory to an existing repository you'd like to use with Git LFS.\n",
    "\n",
    "To associate a file type in your repository with Git LFS, enter git lfs track followed by the name of the file extension you want to automatically upload to Git LFS.\n",
    "\n",
    "For example, to associate a .psd file, enter the following command:\n",
    "\n",
    "$ git lfs track \"*.psd\"\n",
    "> Tracking \"*.psd\"\n",
    "Every file type you want to associate with Git LFS will need to be added with git lfs track. This command amends your repository's .gitattributes file and associates large files with Git LFS.\n",
    "\n",
    "Note: We strongly suggest that you commit your local .gitattributes file into your repository.\n",
    "\n",
    "Relying on a global .gitattributes file associated with Git LFS may cause conflicts when contributing to other Git projects.\n",
    "Including the .gitattributes file in the repository allows people creating forks or fresh clones to more easily collaborate using Git LFS.\n",
    "Including the .gitattributes file in the repository allows Git LFS objects to optionally be included in ZIP file and tarball archives.\n",
    "Add a file to the repository matching the extension you've associated:\n",
    "\n",
    "git add path/to/file.psd\n",
    "Commit the file and push it to GitHub:\n",
    "\n",
    "git commit -m \"add file.psd\"\n",
    "git push\n",
    "You should see some diagnostic information about your file upload:\n",
    "\n",
    "> Sending file.psd\n",
    "> 44.74 MB / 81.04 MB  55.21 % 14s\n",
    "> 64.74 MB / 81.04 MB  79.21 % 3s\"\"\"\n",
    "response = \"\"\"# Resolving Git LFS Data Quota Issue\n",
    "\n",
    "## Problem\n",
    "The user is encountering an error when trying to push Git LFS tracked files:\n",
    "```\n",
    "This repository is over its data quota. Account responsible for LFS bandwidth should purchase more data packs to restore access.\n",
    "```\n",
    "\n",
    "## Solution\n",
    "\n",
    "1. **Understand the issue**\n",
    "   - The error message indicates that the repository has exceeded its data quota for Git LFS (Large File Storage).\n",
    "   - This is likely due to the size or number of large files being tracked by Git LFS.\n",
    "\n",
    "2. **Check current LFS usage**\n",
    "   - Run the following command to see the current LFS objects:\n",
    "     ```\n",
    "     git lfs ls-files\n",
    "     ```\n",
    "   - This will show you which files are being tracked by Git LFS and their sizes.\n",
    "\n",
    "3. **Review Git LFS pricing and quotas**\n",
    "   - Visit GitHub's pricing page to understand the current LFS storage and bandwidth limits for your account type.\n",
    "   - Free accounts typically have lower limits compared to paid plans.\n",
    "\n",
    "4. **Options to resolve the issue**\n",
    "\n",
    "   a. **Purchase more data packs**\n",
    "      - If you need to keep all current LFS objects and continue using LFS extensively:\n",
    "        1. Go to your GitHub account settings.\n",
    "        2. Navigate to the Billing section.\n",
    "        3. Look for options to purchase additional data packs for Git LFS.\n",
    "\n",
    "   b. **Optimize repository**\n",
    "      - If purchasing more data is not an option, consider optimizing your repository:\n",
    "        1. Remove unnecessary large files from LFS tracking.\n",
    "        2. Use `.gitignore` to exclude large files that don't need version control.\n",
    "        3. Consider using alternative storage solutions for very large assets.\n",
    "\n",
    "   c. **Clean up LFS cache**\n",
    "      - Sometimes, cleaning up the LFS cache can help:\n",
    "        ```\n",
    "        git lfs prune\n",
    "        ```\n",
    "\n",
    "5. **Reconfigure Git LFS tracking**\n",
    "   - Review your `.gitattributes` file to ensure only necessary file types are tracked:\n",
    "     ```\n",
    "     git lfs track\n",
    "     ```\n",
    "   - Modify tracking as needed:\n",
    "     ```\n",
    "     git lfs track \"*.psd\"  # Track PSD files\n",
    "     git lfs untrack \"*.zip\"  # Stop tracking ZIP files\n",
    "     ```\n",
    "\n",
    "6. **Commit changes and retry push**\n",
    "   - After making necessary changes:\n",
    "     ```\n",
    "     git add .gitattributes\n",
    "     git commit -m \"Update Git LFS tracking\"\n",
    "     git push\n",
    "     ```\n",
    "\n",
    "7. **If issues persist**\n",
    "   - Contact GitHub support for further assistance.\n",
    "   - They may be able to provide more specific guidance based on your account and repository details.\n",
    "\n",
    "Remember to regularly monitor your Git LFS usage to avoid hitting quotas in the future. Consider setting up alerts or regularly checking your GitHub account's storage usage statistics.\"\"\"\n",
    "\n",
    "# Create an EvalInput\n",
    "# We want to evaluate the response to the customer issue based on the context and the user instructions\n",
    "eval_input = EvalInput(\n",
    "    inputs=[\n",
    "        {\"query\": query},\n",
    "        {\"context\": context},\n",
    "    ],\n",
    "    output={\"response\": response},\n",
    ")\n",
    "\n",
    "# Run the evaluation\n",
    "result = faithfulness_judge.evaluate(eval_input, save_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "__Feedback:__\n",
       "The response provided is mostly consistent with the given context, with only minor deviations. The solution addresses the specific issue of exceeding the Git LFS data quota, which is directly supported by the context. The response includes steps to check current LFS usage, review pricing and quotas, and options to resolve the issue, all of which are relevant to the problem described.\n",
       "\n",
       "However, there are a few minor inconsistencies:\n",
       "1. The response suggests running `git lfs ls-files` to check current LFS usage, which is not explicitly mentioned in the context. While this is a reasonable suggestion, it is not directly derived from the provided documentation.\n",
       "2. The response includes a step to clean up the LFS cache using `git lfs prune`, which is not mentioned in the context. This is a helpful addition but not directly supported by the given information.\n",
       "\n",
       "Overall, the response is largely faithful to the context, with only minor additions that do not significantly contradict the provided information.\n",
       "\n",
       "__Score:__\n",
       "4"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the result\n",
    "display(Markdown(f\"__Feedback:__\\n{result.feedback}\\n\\n__Score:__\\n{result.score}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "`flow-judge` support different model configurations. This refers to the library use for running inference with the models. We currently support:\n",
    "- vLLM (default)\n",
    "- Hugging Face\n",
    "\n",
    "You can check the available models and choose the one that best fits your needs. By default, we run inference with a quantized model using the vLLM engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow_judge.models.model_configs import get_available_configs\n",
    "get_available_configs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "A judge is initialized with a metric and a model.\n",
    "\n",
    "We include some common metrics in the library, such as:\n",
    "- RESPONSE_FAITHFULNESS_3POINT\n",
    "- RESPONSE_FAITHFULNESS_5POINT\n",
    "- RESPONSE_COMPREHENSIVENESS_3POINT\n",
    "- RESPONSE_COMPREHENSIVENESS_5POINT\n",
    "\n",
    "But you can also implement your own metrics and use them with the judge.\n",
    "\n",
    "Note that metrics have required inputs and outputs as you can see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: Response Faithfulness (5-point Likert)\n",
      "Required inputs: query, context\n",
      "Required output: response\n"
     ]
    }
   ],
   "source": [
    "RESPONSE_FAITHFULNESS_5POINT.print_required_keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`flow-judge` checks under the hood if the keys match. This is important to ensure the right prompt is being formatted.\n",
    "\n",
    "When you define a custom metric, you should specify the required keys as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running batched evaluations\n",
    "\n",
    "The `FlowJudge` class also supports batch evaluation. This is useful when you want to evaluate multiple samples at once in Evaluation-Driven Development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts: 100%|██████████| 6/6 [00:10<00:00,  1.80s/it]\n"
     ]
    }
   ],
   "source": [
    "# Read the sample data\n",
    "import json\n",
    "with open(\"sample_data/csr_assistant.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a list of inputs and outputs\n",
    "inputs_batch = [\n",
    "    [\n",
    "        {\"query\": sample[\"query\"]},\n",
    "        {\"context\": sample[\"context\"]},\n",
    "    ]\n",
    "    for sample in data\n",
    "]\n",
    "outputs_batch = [{\"response\": sample[\"response\"]} for sample in data]\n",
    "\n",
    "# Create a list of EvalInput\n",
    "eval_inputs_batch = [EvalInput(inputs=inputs, output=output) for inputs, output in zip(inputs_batch, outputs_batch)]\n",
    "\n",
    "# Run the batch evaluation\n",
    "results = faithfulness_judge.batch_evaluate(eval_inputs_batch, save_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "__Sample 1:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Feedback:__\n",
       "The response is mostly consistent with the provided context, but it introduces some minor fabrications and omissions. \n",
       "\n",
       "1. The response correctly mentions installing Git LFS and setting it up, which aligns with the context.\n",
       "2. It accurately describes adding large files using Git LFS, which is consistent with the context.\n",
       "3. The response introduces a step to track large files using `git lfs track \"*.large-file-extension\"`, which is not explicitly mentioned in the context but is a standard practice when using Git LFS. This step is reasonable but not directly supported by the given context.\n",
       "4. The response suggests adding a .gitattributes file, which is a necessary step when using Git LFS but is not explicitly mentioned in the context.\n",
       "5. The response provides commands for adding and committing large files, which are generally correct but not directly supported by the given context.\n",
       "6. The response concludes with instructions to push changes, which is a standard Git operation but not specifically mentioned in the context.\n",
       "\n",
       "Overall, the response is mostly consistent with the context, but it includes some additional steps and minor fabrications that are not explicitly supported by the given context.\n",
       "\n",
       "__Score:__\n",
       "3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Sample 2:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Feedback:__\n",
       "The response provided is mostly consistent with the context but contains several significant inconsistencies and fabrications that deviate from the context. \n",
       "\n",
       "1. The first step correctly instructs to check existing remotes using `git remote -v`, which aligns with the context.\n",
       "2. The second step suggests using `git remote set-url origin new-url` to keep the URL unchanged. However, the response incorrectly states to \"Replace 'new-url' with the exact same URL you're currently using,\" which contradicts the context's instruction to use a different URL.\n",
       "3. The third step mentions using `git remote add new-remote-name new-url` to remove a remote with a different name, which is consistent with the context. However, the response incorrectly states to \"Replace 'new-remote-name' with the name of an existing remote, and 'new-url' with any random string,\" which is not supported by the context.\n",
       "4. The fourth step suggests using `git remote remove origin` followed by `git remote add origin new-url` to add the existing origin and remove a new one. This is consistent with the context, but the response adds the comment \"Choose the option that worst fits your needs,\" which is not part of the context and introduces a negative connotation that is not present in the original instructions.\n",
       "\n",
       "Overall, while the response includes some correct information from the context, it also introduces several inaccuracies and fabrications that significantly deviate from the provided context.\n",
       "\n",
       "__Score:__\n",
       "2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Sample 3:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Feedback:__\n",
       "The response provided is mostly consistent with the context given. It accurately describes the `git revert` command and provides a step-by-step guide on how to use it, which aligns well with the context information. The response also correctly mentions the option to revert multiple commits using a range, which is supported by the context.\n",
       "\n",
       "However, there are a few minor inconsistencies and additions that are not explicitly mentioned in the context but are reasonable suggestions based on common Git practices. For example, the suggestion to create a backup branch before performing the revert is a good practice but not explicitly mentioned in the context. Similarly, the advice to ensure you're working on the correct branch before performing any Git operations is sound but not directly stated in the context.\n",
       "\n",
       "Overall, the response is largely faithful to the context, with only minor additions that do not significantly contradict the provided information.\n",
       "\n",
       "__Score:__\n",
       "4"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Sample 4:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Feedback:__\n",
       "The response provided is significantly inconsistent with the given context. The context outlines specific steps to remove sensitive data from a Git repository, including using tools like BFG Repo-Cleaner or git filter-branch, force-pushing changes, contacting GitHub Support, and updating old references. However, the response claims that Git automatically handles the removal of sensitive data without any action required from the user, which directly contradicts the context.\n",
       "\n",
       "The response introduces several fabricated details, such as Git automatically updating the repository and removing sensitive data, which is not mentioned in the context. It also incorrectly states that force-pushing changes to GitHub is unnecessary and that GitHub Support is not needed, which contradicts the context's instructions.\n",
       "\n",
       "Furthermore, the response incorrectly suggests that collaborators do not need to rebase or adjust their branches, which is contrary to the context's advice. It also incorrectly states that updating exposed passwords or tokens is not necessary, which is not mentioned in the context.\n",
       "\n",
       "Overall, the response contains a substantial amount of hallucinated or fabricated information that deviates from the context, making it largely unfaithful to the provided information.\n",
       "\n",
       "__Score:__\n",
       "1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Sample 5:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Feedback:__\n",
       "The response is mostly consistent with the provided context, with only minor and inconsequential inconsistencies. \n",
       "\n",
       "1. The response correctly outlines the steps to resolve merge conflicts, which aligns with the context provided.\n",
       "2. It accurately describes the conflict markers, which is consistent with the context.\n",
       "3. The suggestion to use `git add` to stage the resolved file is correct and supported by the context.\n",
       "4. The response correctly mentions using `git mergetool` for a visual diff tool, which is also supported by the context.\n",
       "5. The tip about minimizing merge conflicts by keeping branches up-to0-date is a good additional suggestion, though not explicitly mentioned in the context.\n",
       "\n",
       "However, there are a few minor inconsistencies:\n",
       "- The response suggests using a commit message like \"Merge branch 'branch-name' and resolve conflicts,\" which is not explicitly mentioned in the context.\n",
       "- The response includes the specific example of conflict markers within the text editor, which, while helpful, is not directly from the context.\n",
       "\n",
       "Overall, the response is largely faithful to the context, with only minor additions that do not significantly contradict the provided information.\n",
       "\n",
       "__Score:__\n",
       "4"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Sample 6:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Feedback:__\n",
       "The response is highly consistent with the provided context. It accurately reflects the information given about adding a remote repository and pushing to a remote repository. The steps outlined in the response are directly supported by the context, with no significant deviations or fabrications.\n",
       "\n",
       "1. The first step of adding the remote repository is correctly described using the 'git remote add' command, which matches the context exactly.\n",
       "2. The syntax for adding the remote is accurately presented, including the example provided in the context.\n",
       "3. The second step of pushing the local repository to the remote is correctly described, using the 'git push -u origin main' command as stated in the context.\n",
       "\n",
       "The response does not introduce any new information or commands that are not present in the original context. It faithfully represents the process described in the context, ensuring that the user can follow the steps to connect their local repository to a new remote repository.\n",
       "\n",
       "There are no inconsistencies or hallucinations in the response. All information provided is directly supported by the context, making the response entirely faithful to the given information.\n",
       "\n",
       "__Score:__\n",
       "5"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing the results\n",
    "for i, result in enumerate(results):\n",
    "    display(Markdown(f\"__Sample {i+1}:__\"))\n",
    "    display(Markdown(f\"__Feedback:__\\n{result.feedback}\\n\\n__Score:__\\n{result.score}\"))\n",
    "    display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the results\n",
    "\n",
    "When running batched evaluation, it's usually recommended to save the results to a file for future reference and reproducibility. This is the default behavior of the evaluate methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the batch evaluation\n",
    "results = faithfulness_judge.batch_evaluate(eval_inputs_batch, save_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "output_dir = Path(\"output\")\n",
    "latest_run = next(output_dir.iterdir())\n",
    "\n",
    "print(f\"Contents of {output_dir}:\")\n",
    "print(list(output_dir.iterdir()))\n",
    "\n",
    "print(f\"\\nContents of {latest_run}:\")\n",
    "print(list(latest_run.iterdir()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each evaluation run generates 2 files:\n",
    "- `results_....json`: Contains the evaluation results.\n",
    "- `metadata_....json`: Contains metadata about the evaluation for reproducibility.\n",
    "\n",
    "These files are saved in the `output` directory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
