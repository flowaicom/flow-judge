{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Evaluation Criteria\n",
    "\n",
    "In this example, we will see how to create a custom metric.\n",
    "\n",
    "We will create a metric that evaluates whether a user query is decomposed into sub-queries, covering all the angles of the original query to retrieve all the necessary information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the metric\n",
    "\n",
    "`flow-judge` makes it easy to create custom metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow_judge.metrics import CustomMetric, RubricItem\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Define the criteria\n",
    "evaluation_criteria = \"\"\"Do the generated sub-queries provide sufficient breadth to cover all aspects of the main query?\"\"\"\n",
    "\n",
    "# Define the rubric using RubricItem's\n",
    "rubric = [\n",
    "    RubricItem(\n",
    "        score=1,\n",
    "        description=\"The sub-queries lack breadth and fail to address multiple important aspects of the main query. They are either too narrow, focusing on only one or two dimensions of the question, or they diverge significantly from the main query's intent. Using these sub-queries alone would result in a severely limited exploration of the topic.\"),\n",
    "    RubricItem(\n",
    "        score=2,\n",
    "        description=\"The sub-queries cover some aspects of the main query but lack comprehensive breadth. While they touch on several dimensions of the question, there are still noticeable gaps in coverage. Some important facets of the main query are either underrepresented or missing entirely. Answering these sub-queries would provide a partial, but not complete, exploration of the main topic.\"),\n",
    "    RubricItem(\n",
    "        score=3,\n",
    "        description=\"The sub-queries demonstrate excellent breadth, effectively covering all major aspects of the main query. They break down the main question into a diverse set of dimensions, ensuring a comprehensive exploration of the topic. Each significant facet of the main query is represented in the sub-queries, allowing for a thorough and well-rounded investigation of the subject matter.\"),\n",
    "]\n",
    "\n",
    "# We need to define the required inputs and output for the metric\n",
    "required_inputs = [\"query\"]\n",
    "required_output = \"sub_queries\"\n",
    "\n",
    "# Create the metric\n",
    "sub_query_coverage = CustomMetric(\n",
    "    name=\"sub-query-coverage\",\n",
    "    criteria=evaluation_criteria,\n",
    "    rubric=rubric,\n",
    "    required_inputs=required_inputs,\n",
    "    required_output=required_output\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we quickly created a custom metric that will instruct the model to evaluate according to the criteria and rubric we set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomMetric(name='sub-query-coverage', criteria='Do the generated sub-queries provide sufficient breadth to cover all aspects of the main query?', rubric=[RubricItem(score=1, description=\"The sub-queries lack breadth and fail to address multiple important aspects of the main query. They are either too narrow, focusing on only one or two dimensions of the question, or they diverge significantly from the main query's intent. Using these sub-queries alone would result in a severely limited exploration of the topic.\"), RubricItem(score=2, description='The sub-queries cover some aspects of the main query but lack comprehensive breadth. While they touch on several dimensions of the question, there are still noticeable gaps in coverage. Some important facets of the main query are either underrepresented or missing entirely. Answering these sub-queries would provide a partial, but not complete, exploration of the main topic.'), RubricItem(score=3, description='The sub-queries demonstrate excellent breadth, effectively covering all major aspects of the main query. They break down the main question into a diverse set of dimensions, ensuring a comprehensive exploration of the topic. Each significant facet of the main query is represented in the sub-queries, allowing for a thorough and well-rounded investigation of the subject matter.')], required_inputs=['query'], required_output='sub_queries')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_query_coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running evaluations with the custom metric\n",
    "\n",
    "Now, we just need to initialize the judge with our custom metric and run the evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-08 08:52:43 awq_marlin.py:90] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "WARNING 10-08 08:52:43 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 10-08 08:52:43 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='flowaicom/Flow-Judge-v0.1-AWQ', speculative_config=None, tokenizer='flowaicom/Flow-Judge-v0.1-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=flowaicom/Flow-Judge-v0.1-AWQ, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 10-08 08:52:43 model_runner.py:1014] Starting to load model flowaicom/Flow-Judge-v0.1-AWQ...\n",
      "INFO 10-08 08:52:44 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
      "INFO 10-08 08:52:44 weight_utils.py:287] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3fd5ee0b58c467294660be2b0a5cf60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-08 08:52:46 model_runner.py:1025] Loading model weights took 2.1717 GB\n",
      "INFO 10-08 08:52:48 gpu_executor.py:122] # GPU blocks: 3085, # CPU blocks: 682\n"
     ]
    }
   ],
   "source": [
    "from flow_judge.flow_judge import EvalInput, FlowJudge\n",
    "from flow_judge.models import Vllm #, Llamafile, Hf\n",
    "# from flow_judge import Baseten\n",
    "\n",
    "# If you are running on an Ampere GPU or newer, create a model using VLLM\n",
    "model = Vllm()\n",
    "\n",
    "# Or if not running on Ampere GPU or newer, create a model using no flash attn and Hugging Face Transformers\n",
    "# model = Hf(flash_attn=False)\n",
    "\n",
    "# Or create a model using Llamafile if not running an Nvidia GPU & running a Silicon MacOS for example\n",
    "# model = Llamafile()\n",
    "\n",
    "# Or create a model using Baseten if you don't want to run locally.\n",
    "# As a pre-requisite step:\n",
    "#  - Sign up to Baseten\n",
    "#  - Generate an api key https://app.baseten.co/settings/api_keys\n",
    "#  - Set the api key as an environment variable & initialize:\n",
    "# import os\n",
    "# os.environ[\"BASETEN_API_KEY\"] = \"your_api_key\"\n",
    "# model = Baseten()\n",
    "\n",
    "# Initialize the judge\n",
    "judge = FlowJudge(metric=sub_query_coverage, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.70s/it, est. speed input: 147.77 toks/s, output: 45.67 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# Sample to evaluate\n",
    "\n",
    "query = \"I placed an order for a custom-built gaming PC (order #AC-789012) two weeks ago, but the estimated delivery date has changed twice since then. Originally it was supposed to arrive yesterday, then it got pushed to next week, and now the tracking page shows 'Status: Processing' with no delivery estimate at all. I've tried calling customer service, but after waiting on hold for an hour, I was told to check the website. Can you please look into this and explain what's causing the delays, when I can realistically expect my order to arrive, and whether I'm eligible for any kind of compensation or expedited shipping given these repeated delays? I'm especially concerned because I need this computer for an upcoming gaming tournament I'm participating in next month.\"\n",
    "sub_queries = \"What is the current shipping status of my order? How can I build a PC?\"\n",
    "\n",
    "# bad decomposition\n",
    "eval_input = EvalInput(\n",
    "    inputs=[\n",
    "        {\"query\": query}\n",
    "    ],\n",
    "    output={\"sub_queries\": sub_queries}\n",
    ")\n",
    "\n",
    "# Run the evaluation\n",
    "result = judge.evaluate(eval_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "__Feedback:__\n",
       "The generated sub-queries fail to provide sufficient breadth to cover all aspects of the main query. The main query involves multiple issues: repeated delays in delivery, lack of delivery estimate, customer service experience, and potential compensation. However, the sub-queries only address two very narrow aspects: the current shipping status and how to build a PC.\n",
       "\n",
       "The first sub-query, \"What is the current shipping status of my order?\" is relevant but limited. It only addresses the current status of the order, not the repeated delays or the lack of delivery estimate.\n",
       "\n",
       "The second sub-query, \"How can I build a PC?\" is entirely unrelated to the main query. It diverges significantly from the main query's intent and fails to address any of the customer's concerns about the delayed order.\n",
       "\n",
       "Using these sub-queries alone would result in a severely limited exploration of the topic. They do not cover the repeated delays, lack of delivery estimate, customer service experience, potential compensation, or the urgency due to the upcoming gaming tournament. There are noticeable gaps in coverage, with several important facets of the main query being underrepresented or missing entirely.\n",
       "\n",
       "Therefore, the sub-queries do not meet the criteria for sufficient breadth as outlined in the scoring rubric.\n",
       "\n",
       "__Score:__\n",
       "1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the result\n",
    "display(Markdown(f\"__Feedback:__\\n{result.feedback}\\n\\n__Score:__\\n{result.score}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.59s/it, est. speed input: 179.46 toks/s, output: 44.64 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# Good decomposition\n",
    "\n",
    "sub_queries = \"\"\"1. What is the current status of order #AC-789012, and why has it changed from the original estimated delivery date?\n",
    "2. What specific factors are causing the delays in processing and shipping this custom-built gaming PC?\n",
    "3. Based on the current situation, when can the customer realistically expect the order to arrive?\n",
    "4. Given the repeated delays and changes in estimated delivery, what compensation options (if any) are available to the customer?\n",
    "5. Is expedited shipping an option at this point, and if so, how would it affect the delivery timeline?\n",
    "6. How can the urgency of this order be communicated and prioritized, considering the customer's upcoming gaming tournament next month?\n",
    "7. What steps has customer service already taken to address this issue, and what additional actions can be taken to resolve it?\n",
    "8. How can the customer receive more frequent and accurate updates about their order status going forward?\"\"\"\n",
    "\n",
    "eval_input = EvalInput(\n",
    "    inputs=[\n",
    "        {\"query\": query}\n",
    "    ],\n",
    "    output={\"sub_queries\": sub_queries}\n",
    ")\n",
    "\n",
    "# Run the evaluation\n",
    "result = judge.evaluate(eval_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "__Feedback:__\n",
       "The generated sub-queries demonstrate excellent breadth, effectively covering all major aspects of the main query. Each significant facet of the main query is represented in the sub-queries, ensuring a comprehensive exploration of the topic.\n",
       "\n",
       "The sub-queries address the following key points from the main query:\n",
       "\n",
       "1. Current status and reasons for delivery date changes\n",
       "2. Specific factors causing delays\n",
       "3. Realistic expected arrival time\n",
       "4. Compensation options due to repeated delays\n",
       "5. Availability of expedited shipping\n",
       "6. Communication of order urgency\n",
       "7. Actions taken by customer service and additional steps\n",
       "8. Improved communication for future updates\n",
       "\n",
       "These sub-queries break down the main question into diverse dimensions, ensuring a thorough investigation of the subject matter. They cover the technical aspects of the order, the customer service experience, potential solutions, and future preventative measures. This comprehensive approach allows for a well-rounded exploration of the topic, addressing all major concerns raised in the main query.\n",
       "\n",
       "The sub-queries are well-structured and cover all important aspects without straying from the main query's intent. They provide a solid foundation for a thorough investigation and resolution of the customer's issues.\n",
       "\n",
       "__Score:__\n",
       "3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the result\n",
    "display(Markdown(f\"__Feedback:__\\n{result.feedback}\\n\\n__Score:__\\n{result.score}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.31s/it, est. speed input: 144.97 toks/s, output: 43.77 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "__Feedback:__\n",
       "The generated sub-queries demonstrate a good attempt to cover various aspects of the main query, but they fall short of providing comprehensive breadth. \n",
       "\n",
       "The sub-queries address several key points from the main query:\n",
       "1. The current status of the order (sub-query 1)\n",
       "2. The reason for multiple delivery date changes (sub-query 2)\n",
       "3. The expected arrival time of the order (sub-query 3)\n",
       "4. The possibility of expediting the order (sub-query 8)\n",
       "\n",
       "However, there are noticeable gaps in coverage:\n",
       "- The sub-queries do not address the customer's eligibility for compensation, which was explicitly mentioned in the main query.\n",
       "- The sub-queries do not address the specific concern about the repeated delays and their impact on the customer's upcoming gaming tournament.\n",
       "- The sub-queries include some irrelevant questions (e.g., store hours on weekends, the name of the person responsible for the company) that do not directly relate to the main query's concerns.\n",
       "\n",
       "While the sub-queries touch on several important facets of the main query, they do not fully capture all the dimensions of the customer's concerns. Answering these sub-queries would provide a partial exploration of the topic, but would not fully address all aspects of the main query.\n",
       "\n",
       "__Score:__\n",
       "2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Okish decomposition\n",
    "\n",
    "sub_queries = \"\"\"1. What is the current status of order #AC-7892?\n",
    "Why has the delivery date changed multiple times?\n",
    "When will the custom-built gaming PC actually arrive?\n",
    "What are your store hours on weekends?\n",
    "How can I get better customer service support?\n",
    "8. Can the order be expedited?\n",
    "9. What is the name of the person that is responsible for the company?\n",
    "\"\"\"\n",
    "\n",
    "eval_input = EvalInput(\n",
    "    inputs=[\n",
    "        {\"query\": query}\n",
    "    ],\n",
    "    output={\"sub_queries\": sub_queries}\n",
    ")\n",
    "\n",
    "# Run the evaluation\n",
    "result = judge.evaluate(eval_input)\n",
    "# Display the result\n",
    "display(Markdown(f\"__Feedback:__\\n{result.feedback}\\n\\n__Score:__\\n{result.score}\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
