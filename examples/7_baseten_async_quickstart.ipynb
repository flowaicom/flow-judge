{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart\n",
    "\n",
    "This tutorial demonstrates how to use the `Baseten` model class in async mode to perform language model-based evaluations using Flow-Judge-v0.1 deployed model on Baseten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's instantiate the `Baseten` model class in async mode. The async implementation makes use Baseten's async inference approach. See [here](https://docs.baseten.co/invoke/async).\n",
    "\n",
    "You can imagine this as *fire-and-forget* functionality. Completion requests are made to the deployed model, once data is processed and inference is complete, the output is sent to a predefined webhook. The webhook url is part of the original request. The `Flow-Judge` library then connects with the webhook and *listens* for a response. The library makes use of this approach to allow configurability for concurrent execution.\n",
    "\n",
    "Optionally Flow AI has deployed a webhook proxy that accepts this request signature and feeds-it-forward to the client. This can be found under the URL: \"https://proxy.flowrite.com\"\n",
    "\n",
    "### Pre-requisite\n",
    "\n",
    "1. Sign-up to [Baseten](https://www.baseten.co/)\n",
    "2. Generate a Baseten API Key from [here](https://app.baseten.co/settings/api_keys)\n",
    "3. Generate a Webhook secret from [here](https://app.baseten.co/settings/secrets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Requirements\n",
    "\n",
    "Set your `Baseten API key` and `Webhook secret` in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"BASETEN_WEBHOOK_SECRET\"] = \"your_baseten_webhook_secret\"\n",
    "os.environ[\"BASETEN_API_KEY\"] = \"your_baseten_api_key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the Baseten model\n",
    "\n",
    "Set the following required options for async execution mode of the Baseten model class: \n",
    "1. `exec_async=True`\n",
    "2. `webhook_proxy_url=https://proxy.flowrite.com` (or an alternative)\n",
    "\n",
    "Optionally you can set the `async_batch_size` option to a value > 0 (defaults to `128`). This is the number of concurrent requests sent to the deployed model. It is associated with the concurrency goals you want to achieve and can be actively configured in Baseten's UI. For more information, see [here](https://docs.baseten.co/performance/concurrency). Our current deployment configuration allows a concurrency target of `128` for the deployed model as the default on Baseten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow_judge import Baseten, AsyncFlowJudge\n",
    "from flow_judge.metrics import RESPONSE_FAITHFULNESS_5POINT\n",
    "\n",
    "# Async model execution\n",
    "model = Baseten(\n",
    "    webhook_proxy_url=\"https://proxy.flowrite.com\",\n",
    "    exec_async=True,\n",
    "    async_batch_size=6\n",
    ")\n",
    "\n",
    "# Instantiate the Async Judge with the model and a metric\n",
    "# The library includes multiple default metrics and you can implement your own.\n",
    "faithfulness_judge = AsyncFlowJudge(\n",
    "    metric=RESPONSE_FAITHFULNESS_5POINT,\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Evaluations\n",
    "\n",
    "Let's test batched evaluations with our example csr data on the faithfulness 5 point likert.\n",
    "\n",
    "We use the `async_batch_evaluate` method from the FlowJudge class. Underneath this uses batched processing using the batch_size set with the `async_batch_size` argument of the Baseten model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the sample data\n",
    "import json\n",
    "from flow_judge import EvalInput\n",
    "with open(\"sample_data/csr_assistant.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a list of inputs and outputs\n",
    "inputs_batch = [\n",
    "    [\n",
    "        {\"query\": sample[\"query\"]},\n",
    "        {\"context\": sample[\"context\"]},\n",
    "    ]\n",
    "    for sample in data\n",
    "]\n",
    "outputs_batch = [{\"response\": sample[\"response\"]} for sample in data]\n",
    "\n",
    "# Create a list of EvalInput\n",
    "eval_inputs_batch = [EvalInput(inputs=inputs, output=output) for inputs, output in zip(inputs_batch, outputs_batch)]\n",
    "\n",
    "# Run the batch evaluation\n",
    "results = await faithfulness_judge.async_batch_evaluate(eval_inputs_batch, save_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Visualizing the results\n",
    "for i, result in enumerate(results):\n",
    "    display(Markdown(f\"__Sample {i+1}:__\"))\n",
    "    display(Markdown(f\"__Feedback:__\\n{result.feedback}\\n\\n__Score:__\\n{result.score}\"))\n",
    "    display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly you can run a single evaluation task given an input using the `async_evaluate` method on the Flow Judge class. Under the hood this will process a single async request and attach listeners to the webhook for the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await faithfulness_judge.async_evaluate(eval_inputs_batch[0], save_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the result\n",
    "display(Markdown(f\"__Feedback:__\\n{result.feedback}\\n\\n__Score:__\\n{result.score}\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flow-judge-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
